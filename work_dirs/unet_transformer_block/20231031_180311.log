2023-10-31 18:03:11,216 - mmseg - INFO - Multi-processing start method is `None`
2023-10-31 18:03:11,216 - mmseg - INFO - OpenCV num_threads is `112
2023-10-31 18:03:11,216 - mmseg - INFO - OMP num threads is 1
2023-10-31 18:03:11,372 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3: Quadro RTX 8000
CUDA_HOME: /usr/local/cuda-10.1
NVCC: Cuda compilation tools, release 10.1, V10.1.10
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.10.0+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.1+cu102
OpenCV: 4.7.0
MMCV: 1.7.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMSegmentation: 0.29.1+
------------------------------------------------------------

2023-10-31 18:03:11,372 - mmseg - INFO - Distributed training: True
2023-10-31 18:03:11,734 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
backbone_norm_cfg = dict(type='LN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained=None,
    backbone=dict(
        type='UnetBackbone',
        in_channels=3,
        transformer_block=True,
        channel_list=[64, 128, 256, 512]),
    decode_head=dict(
        type='UnetHead',
        num_classes=4,
        channels=64,
        threshold=0.2,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        loss_decode=[
            dict(
                type='FocalLoss',
                use_sigmoid=True,
                gamma=2.0,
                alpha=0.25,
                loss_weight=2.0),
            dict(type='DiceLoss', loss_name='loss_dice', loss_weight=2.0)
        ]))
train_cfg = dict()
test_cfg = dict(mode='whole')
dataset_type = 'MyDataset'
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(600, 600)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(512, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=False),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data_root = './datasets/'
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=2,
    train=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='train/images',
        ann_dir='train/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(600, 600)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TensorboardLoggerHook'),
        dict(type='TextLoggerHook', by_epoch=False)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = False
optimizer = dict(type='Adam', lr=0.0001, betas=(0.9, 0.999))
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=1e-05, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=5000)
checkpoint_config = dict(by_epoch=False, save_optimizer=False, interval=5000)
evaluation = dict(interval=500, metric=['mIoU', 'mFscore', 'mDice'])
work_dir = './work_dirs/unet_transformer_block'
gpu_ids = range(0, 4)
auto_resume = False

2023-10-31 18:03:11,734 - mmseg - INFO - Set random seed to 0, deterministic: False
2023-10-31 18:03:12,016 - mmseg - INFO - initialize UnetHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

backbone.inc.conv.conv.0.weight - torch.Size([64, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.0.weight - torch.Size([128, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.3.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.0.weight - torch.Size([256, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.3.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.0.weight - torch.Size([512, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.3.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.4.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.0.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.3.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.4.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.linear.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.linear.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.linear.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.linear.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_seg.weight - torch.Size([4, 64, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.up1.conv.conv.0.weight - torch.Size([256, 1024, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.3.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.0.weight - torch.Size([128, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.3.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.0.weight - torch.Size([64, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.0.weight - torch.Size([64, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2023-10-31 18:03:12,023 - mmseg - INFO - EncoderDecoder(
  (backbone): UnetBackbone(
    (inc): InConv(
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (down1): Down(
      (down_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (conv): Sequential(
            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (4): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down2): Down(
      (down_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (conv): Sequential(
            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (4): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down3): Down(
      (down_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (conv): Sequential(
            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (4): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down4): Down(
      (down_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (conv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (4): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (aspp4): TransformerBlock(
      (linear): Linear(in_features=512, out_features=512, bias=True)
      (tr): Sequential(
        (0): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (1): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (2): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (3): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
      )
    )
    (aspp5): TransformerBlock(
      (linear): Linear(in_features=512, out_features=512, bias=True)
      (tr): Sequential(
        (0): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (1): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (2): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (3): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
      )
    )
  )
  (decode_head): UnetHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): ModuleList(
      (0): FocalLoss()
      (1): DiceLoss()
    )
    (conv_seg): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (up1): Up(
      (up): Upsample(scale_factor=2.0, mode=bilinear)
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up2): Up(
      (up): Upsample(scale_factor=2.0, mode=bilinear)
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up3): Up(
      (up): Upsample(scale_factor=2.0, mode=bilinear)
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up4): Up(
      (up): Upsample(scale_factor=2.0, mode=bilinear)
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2023-10-31 18:03:12,030 - mmseg - INFO - Loaded 308 images
2023-10-31 18:03:17,816 - mmseg - INFO - Loaded 78 images
2023-10-31 18:03:17,843 - mmseg - INFO - Start running, host: zhangzifan@s2, work_dir: /data2/zhangzifan/code_dir/2023-10-25-01/work_dirs/unet_transformer_block
2023-10-31 18:03:17,844 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-10-31 18:03:17,844 - mmseg - INFO - workflow: [('train', 1)], max: 5000 iters
2023-10-31 18:03:17,844 - mmseg - INFO - Checkpoints will be saved to /data2/zhangzifan/code_dir/2023-10-25-01/work_dirs/unet_transformer_block by HardDiskBackend.
2023-10-31 18:04:46,482 - mmseg - INFO - Iter [50/5000]	lr: 9.921e-05, eta: 2:22:33, time: 1.728, data_time: 0.507, memory: 25234, decode.loss_focal: 0.1765, decode.loss_dice: 1.7112, decode.acc_seg: 96.0350, loss: 1.8877
2023-10-31 18:06:10,145 - mmseg - INFO - Iter [100/5000]	lr: 9.839e-05, eta: 2:18:52, time: 1.673, data_time: 0.448, memory: 25234, decode.loss_focal: 0.1344, decode.loss_dice: 1.6586, decode.acc_seg: 99.6358, loss: 1.7931
2023-10-31 18:07:30,023 - mmseg - INFO - Iter [150/5000]	lr: 9.758e-05, eta: 2:14:41, time: 1.598, data_time: 0.380, memory: 25234, decode.loss_focal: 0.0999, decode.loss_dice: 1.6083, decode.acc_seg: 99.6445, loss: 1.7082
2023-10-31 18:08:53,594 - mmseg - INFO - Iter [200/5000]	lr: 9.677e-05, eta: 2:13:24, time: 1.671, data_time: 0.440, memory: 25234, decode.loss_focal: 0.0752, decode.loss_dice: 1.5666, decode.acc_seg: 99.6381, loss: 1.6418
2023-10-31 18:10:13,503 - mmseg - INFO - Iter [250/5000]	lr: 9.596e-05, eta: 2:10:54, time: 1.598, data_time: 0.372, memory: 25234, decode.loss_focal: 0.0577, decode.loss_dice: 1.5285, decode.acc_seg: 99.6179, loss: 1.5861
2023-10-31 18:11:37,405 - mmseg - INFO - Iter [300/5000]	lr: 9.514e-05, eta: 2:09:51, time: 1.678, data_time: 0.448, memory: 25234, decode.loss_focal: 0.0457, decode.loss_dice: 1.4804, decode.acc_seg: 99.5298, loss: 1.5261
2023-10-31 18:12:57,405 - mmseg - INFO - Iter [350/5000]	lr: 9.433e-05, eta: 2:07:50, time: 1.600, data_time: 0.381, memory: 25234, decode.loss_focal: 0.0379, decode.loss_dice: 1.4160, decode.acc_seg: 99.5325, loss: 1.4539
2023-10-31 18:14:21,271 - mmseg - INFO - Iter [400/5000]	lr: 9.351e-05, eta: 2:06:43, time: 1.677, data_time: 0.453, memory: 25234, decode.loss_focal: 0.0316, decode.loss_dice: 1.3489, decode.acc_seg: 99.5518, loss: 1.3805
2023-10-31 18:15:41,183 - mmseg - INFO - Iter [450/5000]	lr: 9.269e-05, eta: 2:04:53, time: 1.598, data_time: 0.382, memory: 25234, decode.loss_focal: 0.0256, decode.loss_dice: 1.2868, decode.acc_seg: 99.5983, loss: 1.3124
2023-10-31 18:17:04,814 - mmseg - INFO - Iter [500/5000]	lr: 9.187e-05, eta: 2:03:42, time: 1.673, data_time: 0.457, memory: 25234, decode.loss_focal: 0.0205, decode.loss_dice: 1.2303, decode.acc_seg: 99.6518, loss: 1.2508
2023-10-31 18:18:07,773 - mmseg - INFO - per class results:
2023-10-31 18:18:07,775 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.82 | 70.55 | 72.48 | 70.44 | 99.69 | 99.87  |   99.94   | 99.79  |  99.8 |
|  scratch   | 80.63 | 89.74 | 74.11 | 78.59 | 63.01 | 78.42  |   82.63   | 75.34  | 67.63 |
|   stain    | 71.71 | 98.66 | 75.29 | 84.48 | 66.14 | 59.41  |   57.67   | 77.43  | 39.11 |
| edgeDamage | 78.79 | 91.58 | 74.43 | 80.19 |  76.3 | 75.23  |   70.54   |  84.2  | 62.85 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 18:18:07,775 - mmseg - INFO - Summary:
2023-10-31 18:18:07,775 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.82 | 82.74 | 87.63 | 74.08 | 78.43 | 76.28 |  78.23  |   77.69    |  84.19  | 67.35 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 18:18:07,782 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9982, mIoU: 0.8274, mVOE: 0.8763, mASD: 0.7408, mMSSD: 0.7843, mAcc: 0.7628, mFscore: 0.7823, mPrecision: 0.7769, mRecall: 0.8419, mDice: 0.6735, IoU.background: 0.9982, IoU.scratch: 0.8063, IoU.stain: 0.7171, IoU.edgeDamage: 0.7879, VOE.background: 0.7055, VOE.scratch: 0.8974, VOE.stain: 0.9866, VOE.edgeDamage: 0.9158, ASD.background: 0.7248, ASD.scratch: 0.7411, ASD.stain: 0.7529, ASD.edgeDamage: 0.7443, MSSD.background: 0.7044, MSSD.scratch: 0.7859, MSSD.stain: 0.8448, MSSD.edgeDamage: 0.8019, Acc.background: 0.9969, Acc.scratch: 0.6301, Acc.stain: 0.6614, Acc.edgeDamage: 0.7630, Fscore.background: 0.9987, Fscore.scratch: 0.7842, Fscore.stain: 0.5941, Fscore.edgeDamage: 0.7523, Precision.background: 0.9994, Precision.scratch: 0.8263, Precision.stain: 0.5767, Precision.edgeDamage: 0.7054, Recall.background: 0.9979, Recall.scratch: 0.7534, Recall.stain: 0.7743, Recall.edgeDamage: 0.8420, Dice.background: 0.9980, Dice.scratch: 0.6763, Dice.stain: 0.3911, Dice.edgeDamage: 0.6285
2023-10-31 18:19:30,891 - mmseg - INFO - Iter [550/5000]	lr: 9.106e-05, eta: 2:10:54, time: 2.921, data_time: 1.721, memory: 25234, decode.loss_focal: 0.0162, decode.loss_dice: 1.1869, decode.acc_seg: 99.6677, loss: 1.2031
2023-10-31 18:20:50,532 - mmseg - INFO - Iter [600/5000]	lr: 9.024e-05, eta: 2:08:23, time: 1.593, data_time: 0.383, memory: 25234, decode.loss_focal: 0.0137, decode.loss_dice: 1.1404, decode.acc_seg: 99.7105, loss: 1.1541
2023-10-31 18:22:13,692 - mmseg - INFO - Iter [650/5000]	lr: 8.941e-05, eta: 2:06:26, time: 1.663, data_time: 0.456, memory: 25234, decode.loss_focal: 0.0115, decode.loss_dice: 1.1168, decode.acc_seg: 99.7217, loss: 1.1282
2023-10-31 18:23:33,109 - mmseg - INFO - Iter [700/5000]	lr: 8.859e-05, eta: 2:04:11, time: 1.588, data_time: 0.383, memory: 25234, decode.loss_focal: 0.0098, decode.loss_dice: 1.0909, decode.acc_seg: 99.7467, loss: 1.1007
2023-10-31 18:24:56,274 - mmseg - INFO - Iter [750/5000]	lr: 8.777e-05, eta: 2:02:25, time: 1.663, data_time: 0.451, memory: 25234, decode.loss_focal: 0.0088, decode.loss_dice: 1.0694, decode.acc_seg: 99.7562, loss: 1.0782
2023-10-31 18:26:15,547 - mmseg - INFO - Iter [800/5000]	lr: 8.695e-05, eta: 2:00:21, time: 1.585, data_time: 0.381, memory: 25234, decode.loss_focal: 0.0079, decode.loss_dice: 1.0561, decode.acc_seg: 99.7640, loss: 1.0640
2023-10-31 18:27:39,009 - mmseg - INFO - Iter [850/5000]	lr: 8.612e-05, eta: 1:58:42, time: 1.669, data_time: 0.456, memory: 25234, decode.loss_focal: 0.0072, decode.loss_dice: 1.0523, decode.acc_seg: 99.7631, loss: 1.0595
2023-10-31 18:28:58,861 - mmseg - INFO - Iter [900/5000]	lr: 8.530e-05, eta: 1:56:49, time: 1.597, data_time: 0.380, memory: 25234, decode.loss_focal: 0.0067, decode.loss_dice: 1.0407, decode.acc_seg: 99.7718, loss: 1.0474
2023-10-31 18:30:22,293 - mmseg - INFO - Iter [950/5000]	lr: 8.447e-05, eta: 1:55:15, time: 1.669, data_time: 0.459, memory: 25234, decode.loss_focal: 0.0063, decode.loss_dice: 1.0529, decode.acc_seg: 99.7468, loss: 1.0592
2023-10-31 18:31:45,822 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 18:31:45,823 - mmseg - INFO - Iter [1000/5000]	lr: 8.364e-05, eta: 1:53:42, time: 1.671, data_time: 0.457, memory: 25234, decode.loss_focal: 0.0056, decode.loss_dice: 1.0297, decode.acc_seg: 99.7792, loss: 1.0353
2023-10-31 18:32:43,418 - mmseg - INFO - per class results:
2023-10-31 18:32:43,419 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.92 | 70.45 | 72.47 |  70.4 | 99.88 | 99.94  |   99.96   | 99.92  | 99.91 |
|  scratch   | 82.96 | 87.41 |  73.7 | 76.56 | 72.13 | 82.06  |   82.73   | 81.42  | 73.09 |
|   stain    | 81.29 | 89.08 |  73.9 | 77.55 | 70.94 | 79.49  |   78.45   | 80.63  | 69.23 |
| edgeDamage | 76.92 | 93.45 | 74.73 |  81.7 | 85.78 | 71.65  |   66.01   | 90.52  | 57.48 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 18:32:43,419 - mmseg - INFO - Summary:
2023-10-31 18:32:43,420 - mmseg - INFO - 
+-------+-------+------+------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU | mVOE | mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+------+------+-------+-------+---------+------------+---------+-------+
| 99.92 | 85.27 | 85.1 | 73.7 | 76.55 | 82.18 |  83.28  |   81.79    |  88.12  | 74.93 |
+-------+-------+------+------+-------+-------+---------+------------+---------+-------+
2023-10-31 18:32:43,425 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 18:32:43,426 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9992, mIoU: 0.8527, mVOE: 0.8510, mASD: 0.7370, mMSSD: 0.7655, mAcc: 0.8218, mFscore: 0.8328, mPrecision: 0.8179, mRecall: 0.8812, mDice: 0.7493, IoU.background: 0.9992, IoU.scratch: 0.8296, IoU.stain: 0.8129, IoU.edgeDamage: 0.7692, VOE.background: 0.7045, VOE.scratch: 0.8741, VOE.stain: 0.8908, VOE.edgeDamage: 0.9345, ASD.background: 0.7247, ASD.scratch: 0.7370, ASD.stain: 0.7390, ASD.edgeDamage: 0.7473, MSSD.background: 0.7040, MSSD.scratch: 0.7656, MSSD.stain: 0.7755, MSSD.edgeDamage: 0.8170, Acc.background: 0.9988, Acc.scratch: 0.7213, Acc.stain: 0.7094, Acc.edgeDamage: 0.8578, Fscore.background: 0.9994, Fscore.scratch: 0.8206, Fscore.stain: 0.7949, Fscore.edgeDamage: 0.7165, Precision.background: 0.9996, Precision.scratch: 0.8273, Precision.stain: 0.7845, Precision.edgeDamage: 0.6601, Recall.background: 0.9992, Recall.scratch: 0.8142, Recall.stain: 0.8063, Recall.edgeDamage: 0.9052, Dice.background: 0.9991, Dice.scratch: 0.7309, Dice.stain: 0.6923, Dice.edgeDamage: 0.5748
2023-10-31 18:34:02,388 - mmseg - INFO - Iter [1050/5000]	lr: 8.281e-05, eta: 1:55:30, time: 2.731, data_time: 1.534, memory: 25234, decode.loss_focal: 0.0064, decode.loss_dice: 1.0266, decode.acc_seg: 99.7386, loss: 1.0330
2023-10-31 18:35:25,427 - mmseg - INFO - Iter [1100/5000]	lr: 8.198e-05, eta: 1:53:46, time: 1.661, data_time: 0.456, memory: 25234, decode.loss_focal: 0.0069, decode.loss_dice: 1.0451, decode.acc_seg: 99.7090, loss: 1.0520
2023-10-31 18:36:44,770 - mmseg - INFO - Iter [1150/5000]	lr: 8.115e-05, eta: 1:51:51, time: 1.587, data_time: 0.383, memory: 25234, decode.loss_focal: 0.0064, decode.loss_dice: 1.0472, decode.acc_seg: 99.7057, loss: 1.0536
2023-10-31 18:38:07,847 - mmseg - INFO - Iter [1200/5000]	lr: 8.032e-05, eta: 1:50:11, time: 1.661, data_time: 0.457, memory: 25234, decode.loss_focal: 0.0058, decode.loss_dice: 0.9244, decode.acc_seg: 99.7373, loss: 0.9302
2023-10-31 18:39:27,144 - mmseg - INFO - Iter [1250/5000]	lr: 7.949e-05, eta: 1:48:21, time: 1.586, data_time: 0.384, memory: 25234, decode.loss_focal: 0.0054, decode.loss_dice: 0.8969, decode.acc_seg: 99.7426, loss: 0.9023
2023-10-31 18:40:50,327 - mmseg - INFO - Iter [1300/5000]	lr: 7.865e-05, eta: 1:46:44, time: 1.664, data_time: 0.460, memory: 25234, decode.loss_focal: 0.0050, decode.loss_dice: 0.8512, decode.acc_seg: 99.7503, loss: 0.8563
2023-10-31 18:42:09,832 - mmseg - INFO - Iter [1350/5000]	lr: 7.782e-05, eta: 1:44:58, time: 1.590, data_time: 0.388, memory: 25234, decode.loss_focal: 0.0050, decode.loss_dice: 0.8983, decode.acc_seg: 99.7362, loss: 0.9033
2023-10-31 18:43:32,723 - mmseg - INFO - Iter [1400/5000]	lr: 7.698e-05, eta: 1:43:23, time: 1.658, data_time: 0.454, memory: 25234, decode.loss_focal: 0.0046, decode.loss_dice: 0.8689, decode.acc_seg: 99.7442, loss: 0.8735
2023-10-31 18:44:55,517 - mmseg - INFO - Iter [1450/5000]	lr: 7.614e-05, eta: 1:41:49, time: 1.656, data_time: 0.452, memory: 25234, decode.loss_focal: 0.0045, decode.loss_dice: 0.8249, decode.acc_seg: 99.7566, loss: 0.8293
2023-10-31 18:46:14,880 - mmseg - INFO - Iter [1500/5000]	lr: 7.530e-05, eta: 1:40:07, time: 1.587, data_time: 0.378, memory: 25234, decode.loss_focal: 0.0044, decode.loss_dice: 0.8177, decode.acc_seg: 99.7542, loss: 0.8221
2023-10-31 18:47:14,869 - mmseg - INFO - per class results:
2023-10-31 18:47:14,871 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 | 72.47 | 70.39 | 99.97 | 99.96  |   99.93   | 99.98  | 99.93 |
|  scratch   |  81.7 | 88.67 | 74.09 | 78.48 | 63.53 | 80.15  |   87.15   | 75.68  | 70.22 |
|   stain    | 80.38 | 89.99 | 74.31 |  79.6 | 58.46 |  78.0  |   89.58   |  72.3  | 67.01 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 18:47:14,871 - mmseg - INFO - Summary:
2023-10-31 18:47:14,872 - mmseg - INFO - 
+-------+------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc | mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 83.1 | 87.27 | 73.62 | 76.16 | 63.82 |  86.04  |   92.22    |  75.88  | 67.62 |
+-------+------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 18:47:14,878 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8310, mVOE: 0.8727, mASD: 0.7362, mMSSD: 0.7616, mAcc: 0.6382, mFscore: 0.8604, mPrecision: 0.9222, mRecall: 0.7588, mDice: 0.6762, IoU.background: 0.9994, IoU.scratch: 0.8170, IoU.stain: 0.8038, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8867, VOE.stain: 0.8999, VOE.edgeDamage: 1.0000, ASD.background: 0.7247, ASD.scratch: 0.7409, ASD.stain: 0.7431, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7848, MSSD.stain: 0.7960, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.6353, Acc.stain: 0.5846, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8015, Fscore.stain: 0.7800, Fscore.edgeDamage: nan, Precision.background: 0.9993, Precision.scratch: 0.8715, Precision.stain: 0.8958, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.7568, Recall.stain: 0.7230, Recall.edgeDamage: 0.5556, Dice.background: 0.9993, Dice.scratch: 0.7022, Dice.stain: 0.6701, Dice.edgeDamage: 0.3333
2023-10-31 18:48:37,494 - mmseg - INFO - Iter [1550/5000]	lr: 7.446e-05, eta: 1:40:48, time: 2.852, data_time: 1.652, memory: 25234, decode.loss_focal: 0.0041, decode.loss_dice: 0.7879, decode.acc_seg: 99.7703, loss: 0.7920
2023-10-31 18:49:56,812 - mmseg - INFO - Iter [1600/5000]	lr: 7.362e-05, eta: 1:39:02, time: 1.586, data_time: 0.380, memory: 25234, decode.loss_focal: 0.0040, decode.loss_dice: 0.7647, decode.acc_seg: 99.7804, loss: 0.7687
2023-10-31 18:51:19,692 - mmseg - INFO - Iter [1650/5000]	lr: 7.278e-05, eta: 1:37:26, time: 1.658, data_time: 0.454, memory: 25234, decode.loss_focal: 0.0041, decode.loss_dice: 0.7722, decode.acc_seg: 99.7746, loss: 0.7763
2023-10-31 18:52:38,970 - mmseg - INFO - Iter [1700/5000]	lr: 7.194e-05, eta: 1:35:43, time: 1.586, data_time: 0.380, memory: 25234, decode.loss_focal: 0.0040, decode.loss_dice: 0.7589, decode.acc_seg: 99.7751, loss: 0.7629
2023-10-31 18:54:01,711 - mmseg - INFO - Iter [1750/5000]	lr: 7.109e-05, eta: 1:34:08, time: 1.655, data_time: 0.453, memory: 25234, decode.loss_focal: 0.0039, decode.loss_dice: 0.7517, decode.acc_seg: 99.7791, loss: 0.7556
2023-10-31 18:55:20,867 - mmseg - INFO - Iter [1800/5000]	lr: 7.025e-05, eta: 1:32:27, time: 1.583, data_time: 0.386, memory: 25234, decode.loss_focal: 0.0039, decode.loss_dice: 0.7475, decode.acc_seg: 99.7783, loss: 0.7514
2023-10-31 18:56:43,737 - mmseg - INFO - Iter [1850/5000]	lr: 6.940e-05, eta: 1:30:54, time: 1.657, data_time: 0.449, memory: 25234, decode.loss_focal: 0.0038, decode.loss_dice: 0.7331, decode.acc_seg: 99.7855, loss: 0.7368
2023-10-31 18:58:06,536 - mmseg - INFO - Iter [1900/5000]	lr: 6.855e-05, eta: 1:29:22, time: 1.656, data_time: 0.452, memory: 25234, decode.loss_focal: 0.0039, decode.loss_dice: 0.7583, decode.acc_seg: 99.7727, loss: 0.7621
2023-10-31 18:59:25,650 - mmseg - INFO - Iter [1950/5000]	lr: 6.770e-05, eta: 1:27:44, time: 1.582, data_time: 0.382, memory: 25234, decode.loss_focal: 0.0038, decode.loss_dice: 0.7658, decode.acc_seg: 99.7696, loss: 0.7696
2023-10-31 19:00:48,454 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 19:00:48,454 - mmseg - INFO - Iter [2000/5000]	lr: 6.685e-05, eta: 1:26:12, time: 1.656, data_time: 0.452, memory: 25234, decode.loss_focal: 0.0037, decode.loss_dice: 0.7617, decode.acc_seg: 99.7774, loss: 0.7654
2023-10-31 19:01:47,078 - mmseg - INFO - per class results:
2023-10-31 19:01:47,079 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 | 72.47 | 70.39 | 99.97 | 99.96  |   99.93   | 99.98  | 99.93 |
|  scratch   | 82.47 |  87.9 | 73.96 | 77.85 | 66.32 | 81.33  |    86.7   | 77.55  |  72.0 |
|   stain    | 79.01 | 91.36 | 74.54 | 80.74 | 53.33 | 75.62  |   96.08   | 68.89  | 63.43 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 19:01:47,079 - mmseg - INFO - Summary:
2023-10-31 19:01:47,080 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 82.95 | 87.42 | 73.66 | 76.33 | 63.24 |  85.64  |   94.24    |  75.49  | 67.17 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 19:01:47,085 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 19:01:47,086 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8295, mVOE: 0.8742, mASD: 0.7366, mMSSD: 0.7633, mAcc: 0.6324, mFscore: 0.8564, mPrecision: 0.9424, mRecall: 0.7549, mDice: 0.6717, IoU.background: 0.9994, IoU.scratch: 0.8247, IoU.stain: 0.7901, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8790, VOE.stain: 0.9136, VOE.edgeDamage: 1.0000, ASD.background: 0.7247, ASD.scratch: 0.7396, ASD.stain: 0.7454, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7785, MSSD.stain: 0.8074, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.6632, Acc.stain: 0.5333, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8133, Fscore.stain: 0.7562, Fscore.edgeDamage: nan, Precision.background: 0.9993, Precision.scratch: 0.8670, Precision.stain: 0.9608, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.7755, Recall.stain: 0.6889, Recall.edgeDamage: 0.5556, Dice.background: 0.9993, Dice.scratch: 0.7200, Dice.stain: 0.6343, Dice.edgeDamage: 0.3333
2023-10-31 19:03:06,042 - mmseg - INFO - Iter [2050/5000]	lr: 6.599e-05, eta: 1:26:00, time: 2.752, data_time: 1.558, memory: 25234, decode.loss_focal: 0.0037, decode.loss_dice: 0.7491, decode.acc_seg: 99.7784, loss: 0.7528
2023-10-31 19:04:28,848 - mmseg - INFO - Iter [2100/5000]	lr: 6.514e-05, eta: 1:24:26, time: 1.656, data_time: 0.457, memory: 25234, decode.loss_focal: 0.0034, decode.loss_dice: 0.7200, decode.acc_seg: 99.7959, loss: 0.7235
2023-10-31 19:05:47,936 - mmseg - INFO - Iter [2150/5000]	lr: 6.428e-05, eta: 1:22:48, time: 1.582, data_time: 0.382, memory: 25234, decode.loss_focal: 0.0035, decode.loss_dice: 0.7209, decode.acc_seg: 99.7864, loss: 0.7244
2023-10-31 19:07:10,671 - mmseg - INFO - Iter [2200/5000]	lr: 6.343e-05, eta: 1:21:15, time: 1.655, data_time: 0.451, memory: 25234, decode.loss_focal: 0.0033, decode.loss_dice: 0.7069, decode.acc_seg: 99.8010, loss: 0.7102
2023-10-31 19:08:29,801 - mmseg - INFO - Iter [2250/5000]	lr: 6.257e-05, eta: 1:19:38, time: 1.583, data_time: 0.383, memory: 25234, decode.loss_focal: 0.0034, decode.loss_dice: 0.7146, decode.acc_seg: 99.7913, loss: 0.7180
2023-10-31 19:09:52,930 - mmseg - INFO - Iter [2300/5000]	lr: 6.171e-05, eta: 1:18:07, time: 1.663, data_time: 0.460, memory: 25234, decode.loss_focal: 0.0033, decode.loss_dice: 0.7028, decode.acc_seg: 99.7981, loss: 0.7060
2023-10-31 19:11:15,710 - mmseg - INFO - Iter [2350/5000]	lr: 6.084e-05, eta: 1:16:35, time: 1.656, data_time: 0.455, memory: 25234, decode.loss_focal: 0.0032, decode.loss_dice: 0.7089, decode.acc_seg: 99.8016, loss: 0.7121
2023-10-31 19:12:34,544 - mmseg - INFO - Iter [2400/5000]	lr: 5.998e-05, eta: 1:15:00, time: 1.577, data_time: 0.380, memory: 25234, decode.loss_focal: 0.0030, decode.loss_dice: 0.6941, decode.acc_seg: 99.8101, loss: 0.6971
2023-10-31 19:13:57,379 - mmseg - INFO - Iter [2450/5000]	lr: 5.911e-05, eta: 1:13:30, time: 1.657, data_time: 0.455, memory: 25234, decode.loss_focal: 0.0033, decode.loss_dice: 0.7097, decode.acc_seg: 99.7913, loss: 0.7130
2023-10-31 19:15:16,519 - mmseg - INFO - Iter [2500/5000]	lr: 5.825e-05, eta: 1:11:56, time: 1.583, data_time: 0.384, memory: 25234, decode.loss_focal: 0.0033, decode.loss_dice: 0.7202, decode.acc_seg: 99.7949, loss: 0.7235
2023-10-31 19:16:13,738 - mmseg - INFO - per class results:
2023-10-31 19:16:13,740 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 | 72.47 | 70.39 | 99.96 | 99.96  |   99.94   | 99.98  | 99.94 |
|  scratch   | 83.96 | 86.41 | 73.69 | 76.52 | 72.35 | 83.51  |   85.77   | 81.57  | 75.27 |
|   stain    | 81.07 |  89.3 | 74.28 | 79.47 | 59.07 | 79.14  |   93.27   | 72.71  | 68.71 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 19:16:13,740 - mmseg - INFO - Summary:
2023-10-31 19:16:13,740 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 83.84 | 86.53 | 73.48 | 75.46 | 66.18 |  87.54  |    93.0    |  77.45  | 69.31 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 19:16:13,745 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8384, mVOE: 0.8653, mASD: 0.7348, mMSSD: 0.7546, mAcc: 0.6618, mFscore: 0.8754, mPrecision: 0.9300, mRecall: 0.7745, mDice: 0.6931, IoU.background: 0.9994, IoU.scratch: 0.8396, IoU.stain: 0.8107, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8641, VOE.stain: 0.8930, VOE.edgeDamage: 1.0000, ASD.background: 0.7247, ASD.scratch: 0.7369, ASD.stain: 0.7428, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7652, MSSD.stain: 0.7947, MSSD.edgeDamage: nan, Acc.background: 0.9996, Acc.scratch: 0.7235, Acc.stain: 0.5907, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8351, Fscore.stain: 0.7914, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8577, Precision.stain: 0.9327, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.8157, Recall.stain: 0.7271, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7527, Dice.stain: 0.6871, Dice.edgeDamage: 0.3333
2023-10-31 19:17:36,435 - mmseg - INFO - Iter [2550/5000]	lr: 5.738e-05, eta: 1:11:21, time: 2.798, data_time: 1.607, memory: 25234, decode.loss_focal: 0.0032, decode.loss_dice: 0.7184, decode.acc_seg: 99.7958, loss: 0.7216
2023-10-31 19:18:55,589 - mmseg - INFO - Iter [2600/5000]	lr: 5.651e-05, eta: 1:09:46, time: 1.583, data_time: 0.384, memory: 25234, decode.loss_focal: 0.0031, decode.loss_dice: 0.6999, decode.acc_seg: 99.8026, loss: 0.7031
2023-10-31 19:20:18,510 - mmseg - INFO - Iter [2650/5000]	lr: 5.563e-05, eta: 1:08:15, time: 1.658, data_time: 0.456, memory: 25234, decode.loss_focal: 0.0030, decode.loss_dice: 0.6942, decode.acc_seg: 99.8074, loss: 0.6972
2023-10-31 19:21:37,728 - mmseg - INFO - Iter [2700/5000]	lr: 5.476e-05, eta: 1:06:41, time: 1.584, data_time: 0.388, memory: 25234, decode.loss_focal: 0.0030, decode.loss_dice: 0.6930, decode.acc_seg: 99.8084, loss: 0.6960
2023-10-31 19:23:00,419 - mmseg - INFO - Iter [2750/5000]	lr: 5.388e-05, eta: 1:05:11, time: 1.654, data_time: 0.454, memory: 25234, decode.loss_focal: 0.0031, decode.loss_dice: 0.7158, decode.acc_seg: 99.7932, loss: 0.7189
2023-10-31 19:24:23,176 - mmseg - INFO - Iter [2800/5000]	lr: 5.301e-05, eta: 1:03:40, time: 1.655, data_time: 0.451, memory: 25234, decode.loss_focal: 0.0034, decode.loss_dice: 0.7317, decode.acc_seg: 99.7745, loss: 0.7350
2023-10-31 19:25:42,478 - mmseg - INFO - Iter [2850/5000]	lr: 5.213e-05, eta: 1:02:08, time: 1.586, data_time: 0.393, memory: 25234, decode.loss_focal: 0.0031, decode.loss_dice: 0.7144, decode.acc_seg: 99.7938, loss: 0.7175
2023-10-31 19:27:05,177 - mmseg - INFO - Iter [2900/5000]	lr: 5.124e-05, eta: 1:00:38, time: 1.654, data_time: 0.456, memory: 25234, decode.loss_focal: 0.0033, decode.loss_dice: 0.7367, decode.acc_seg: 99.7808, loss: 0.7400
2023-10-31 19:28:24,317 - mmseg - INFO - Iter [2950/5000]	lr: 5.036e-05, eta: 0:59:06, time: 1.583, data_time: 0.383, memory: 25234, decode.loss_focal: 0.0034, decode.loss_dice: 0.7349, decode.acc_seg: 99.7767, loss: 0.7383
2023-10-31 19:29:47,028 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 19:29:47,029 - mmseg - INFO - Iter [3000/5000]	lr: 4.947e-05, eta: 0:57:37, time: 1.654, data_time: 0.457, memory: 25234, decode.loss_focal: 0.0031, decode.loss_dice: 0.7187, decode.acc_seg: 99.7944, loss: 0.7218
2023-10-31 19:30:44,186 - mmseg - INFO - per class results:
2023-10-31 19:30:44,187 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 | 72.47 | 70.39 | 99.96 | 99.96  |   99.94   | 99.97  | 99.94 |
|  scratch   | 83.88 | 86.49 | 73.66 | 76.35 |  73.1 | 83.39  |   84.85   | 82.07  | 75.08 |
|   stain    | 83.37 |  87.0 | 73.94 | 77.76 | 66.74 | 82.66  |   90.17   | 77.83  | 73.99 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 19:30:44,187 - mmseg - INFO - Summary:
2023-10-31 19:30:44,187 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 84.39 | 85.98 | 73.36 | 74.83 | 68.28 |  88.67  |   91.65    |  78.86  | 70.59 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 19:30:44,193 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 19:30:44,194 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8439, mVOE: 0.8598, mASD: 0.7336, mMSSD: 0.7483, mAcc: 0.6828, mFscore: 0.8867, mPrecision: 0.9165, mRecall: 0.7886, mDice: 0.7059, IoU.background: 0.9994, IoU.scratch: 0.8388, IoU.stain: 0.8337, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8649, VOE.stain: 0.8700, VOE.edgeDamage: 1.0000, ASD.background: 0.7247, ASD.scratch: 0.7366, ASD.stain: 0.7394, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7635, MSSD.stain: 0.7776, MSSD.edgeDamage: nan, Acc.background: 0.9996, Acc.scratch: 0.7310, Acc.stain: 0.6674, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8339, Fscore.stain: 0.8266, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8485, Precision.stain: 0.9017, Precision.edgeDamage: nan, Recall.background: 0.9997, Recall.scratch: 0.8207, Recall.stain: 0.7783, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7508, Dice.stain: 0.7399, Dice.edgeDamage: 0.3333
2023-10-31 19:32:02,554 - mmseg - INFO - Iter [3050/5000]	lr: 4.858e-05, eta: 0:56:42, time: 2.711, data_time: 1.523, memory: 25234, decode.loss_focal: 0.0029, decode.loss_dice: 0.6992, decode.acc_seg: 99.8077, loss: 0.7021
2023-10-31 19:33:25,164 - mmseg - INFO - Iter [3100/5000]	lr: 4.769e-05, eta: 0:55:12, time: 1.652, data_time: 0.457, memory: 25234, decode.loss_focal: 0.0028, decode.loss_dice: 0.6952, decode.acc_seg: 99.8082, loss: 0.6981
2023-10-31 19:34:44,271 - mmseg - INFO - Iter [3150/5000]	lr: 4.680e-05, eta: 0:53:40, time: 1.582, data_time: 0.383, memory: 25234, decode.loss_focal: 0.0030, decode.loss_dice: 0.6985, decode.acc_seg: 99.7994, loss: 0.7015
2023-10-31 19:36:07,335 - mmseg - INFO - Iter [3200/5000]	lr: 4.590e-05, eta: 0:52:11, time: 1.661, data_time: 0.455, memory: 25234, decode.loss_focal: 0.0028, decode.loss_dice: 0.6946, decode.acc_seg: 99.8138, loss: 0.6974
2023-10-31 19:37:30,670 - mmseg - INFO - Iter [3250/5000]	lr: 4.500e-05, eta: 0:50:42, time: 1.667, data_time: 0.461, memory: 25234, decode.loss_focal: 0.0029, decode.loss_dice: 0.6827, decode.acc_seg: 99.8084, loss: 0.6855
2023-10-31 19:38:49,672 - mmseg - INFO - Iter [3300/5000]	lr: 4.410e-05, eta: 0:49:11, time: 1.580, data_time: 0.384, memory: 25234, decode.loss_focal: 0.0028, decode.loss_dice: 0.6846, decode.acc_seg: 99.8122, loss: 0.6874
2023-10-31 19:40:12,672 - mmseg - INFO - Iter [3350/5000]	lr: 4.320e-05, eta: 0:47:42, time: 1.660, data_time: 0.465, memory: 25234, decode.loss_focal: 0.0028, decode.loss_dice: 0.6810, decode.acc_seg: 99.8106, loss: 0.6838
2023-10-31 19:41:31,743 - mmseg - INFO - Iter [3400/5000]	lr: 4.229e-05, eta: 0:46:12, time: 1.581, data_time: 0.383, memory: 25234, decode.loss_focal: 0.0028, decode.loss_dice: 0.6772, decode.acc_seg: 99.8121, loss: 0.6800
2023-10-31 19:42:54,377 - mmseg - INFO - Iter [3450/5000]	lr: 4.138e-05, eta: 0:44:44, time: 1.653, data_time: 0.452, memory: 25234, decode.loss_focal: 0.0027, decode.loss_dice: 0.6749, decode.acc_seg: 99.8182, loss: 0.6776
2023-10-31 19:44:13,601 - mmseg - INFO - Iter [3500/5000]	lr: 4.047e-05, eta: 0:43:14, time: 1.584, data_time: 0.385, memory: 25234, decode.loss_focal: 0.0028, decode.loss_dice: 0.6808, decode.acc_seg: 99.8142, loss: 0.6835
2023-10-31 19:45:12,024 - mmseg - INFO - per class results:
2023-10-31 19:45:12,025 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 | 72.47 | 70.39 | 99.96 | 99.96  |   99.94   | 99.97  | 99.94 |
|  scratch   | 84.42 | 85.95 |  73.6 | 76.05 | 74.44 | 84.14  |   85.43   | 82.96  | 76.21 |
|   stain    | 83.84 | 86.53 | 73.91 | 77.59 | 67.51 | 83.34  |   91.14   | 78.34  | 75.01 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 19:45:12,026 - mmseg - INFO - Summary:
2023-10-31 19:45:12,026 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 84.64 | 85.73 | 73.33 | 74.68 | 68.81 |  89.15  |   92.17    |  79.21  | 71.12 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 19:45:12,032 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8464, mVOE: 0.8573, mASD: 0.7333, mMSSD: 0.7468, mAcc: 0.6881, mFscore: 0.8915, mPrecision: 0.9217, mRecall: 0.7921, mDice: 0.7112, IoU.background: 0.9994, IoU.scratch: 0.8442, IoU.stain: 0.8384, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8595, VOE.stain: 0.8653, VOE.edgeDamage: 1.0000, ASD.background: 0.7247, ASD.scratch: 0.7360, ASD.stain: 0.7391, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7605, MSSD.stain: 0.7759, MSSD.edgeDamage: nan, Acc.background: 0.9996, Acc.scratch: 0.7444, Acc.stain: 0.6751, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8414, Fscore.stain: 0.8334, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8543, Precision.stain: 0.9114, Precision.edgeDamage: nan, Recall.background: 0.9997, Recall.scratch: 0.8296, Recall.stain: 0.7834, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7621, Dice.stain: 0.7501, Dice.edgeDamage: 0.3333
2023-10-31 19:46:34,378 - mmseg - INFO - Iter [3550/5000]	lr: 3.956e-05, eta: 0:42:10, time: 2.816, data_time: 1.622, memory: 25234, decode.loss_focal: 0.0025, decode.loss_dice: 0.6695, decode.acc_seg: 99.8291, loss: 0.6719
2023-10-31 19:47:53,272 - mmseg - INFO - Iter [3600/5000]	lr: 3.864e-05, eta: 0:40:39, time: 1.578, data_time: 0.378, memory: 25234, decode.loss_focal: 0.0026, decode.loss_dice: 0.6738, decode.acc_seg: 99.8211, loss: 0.6764
2023-10-31 19:49:15,788 - mmseg - INFO - Iter [3650/5000]	lr: 3.772e-05, eta: 0:39:10, time: 1.650, data_time: 0.452, memory: 25234, decode.loss_focal: 0.0026, decode.loss_dice: 0.6675, decode.acc_seg: 99.8197, loss: 0.6702
2023-10-31 19:50:38,346 - mmseg - INFO - Iter [3700/5000]	lr: 3.679e-05, eta: 0:37:42, time: 1.651, data_time: 0.450, memory: 25234, decode.loss_focal: 0.0026, decode.loss_dice: 0.6665, decode.acc_seg: 99.8214, loss: 0.6691
2023-10-31 19:51:57,230 - mmseg - INFO - Iter [3750/5000]	lr: 3.586e-05, eta: 0:36:12, time: 1.578, data_time: 0.382, memory: 25234, decode.loss_focal: 0.0026, decode.loss_dice: 0.6610, decode.acc_seg: 99.8225, loss: 0.6636
2023-10-31 19:53:20,011 - mmseg - INFO - Iter [3800/5000]	lr: 3.493e-05, eta: 0:34:44, time: 1.656, data_time: 0.459, memory: 25234, decode.loss_focal: 0.0025, decode.loss_dice: 0.6606, decode.acc_seg: 99.8272, loss: 0.6632
2023-10-31 19:54:39,027 - mmseg - INFO - Iter [3850/5000]	lr: 3.400e-05, eta: 0:33:14, time: 1.580, data_time: 0.382, memory: 25234, decode.loss_focal: 0.0026, decode.loss_dice: 0.6637, decode.acc_seg: 99.8239, loss: 0.6663
2023-10-31 19:56:01,609 - mmseg - INFO - Iter [3900/5000]	lr: 3.306e-05, eta: 0:31:47, time: 1.652, data_time: 0.451, memory: 25234, decode.loss_focal: 0.0025, decode.loss_dice: 0.6661, decode.acc_seg: 99.8274, loss: 0.6686
2023-10-31 19:57:20,475 - mmseg - INFO - Iter [3950/5000]	lr: 3.211e-05, eta: 0:30:18, time: 1.577, data_time: 0.378, memory: 25234, decode.loss_focal: 0.0025, decode.loss_dice: 0.6588, decode.acc_seg: 99.8280, loss: 0.6613
2023-10-31 19:58:43,280 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 19:58:43,280 - mmseg - INFO - Iter [4000/5000]	lr: 3.116e-05, eta: 0:28:50, time: 1.656, data_time: 0.454, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6630, decode.acc_seg: 99.8296, loss: 0.6654
2023-10-31 19:59:41,617 - mmseg - INFO - per class results:
2023-10-31 19:59:41,619 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.95 | 70.42 | 72.47 | 70.39 | 99.97 | 99.96  |   99.94   | 99.98  | 99.94 |
|  scratch   | 84.41 | 85.96 | 73.68 | 76.45 | 72.64 | 84.13  |   86.97   | 81.76  | 76.19 |
|   stain    | 83.31 | 87.06 | 73.99 | 78.02 |  65.6 | 82.57  |   91.85   | 77.07  | 73.85 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 19:59:41,619 - mmseg - INFO - Summary:
2023-10-31 19:59:41,619 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.95 | 84.51 | 85.86 | 73.38 | 74.95 | 67.89 |  88.89  |   92.92    |  78.59  | 70.83 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 19:59:41,630 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 19:59:41,630 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9995, mIoU: 0.8451, mVOE: 0.8586, mASD: 0.7338, mMSSD: 0.7495, mAcc: 0.6789, mFscore: 0.8889, mPrecision: 0.9292, mRecall: 0.7859, mDice: 0.7083, IoU.background: 0.9995, IoU.scratch: 0.8441, IoU.stain: 0.8331, IoU.edgeDamage: 0.7037, VOE.background: 0.7042, VOE.scratch: 0.8596, VOE.stain: 0.8706, VOE.edgeDamage: 1.0000, ASD.background: 0.7247, ASD.scratch: 0.7368, ASD.stain: 0.7399, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7645, MSSD.stain: 0.7802, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.7264, Acc.stain: 0.6560, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8413, Fscore.stain: 0.8257, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8697, Precision.stain: 0.9185, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.8176, Recall.stain: 0.7707, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7619, Dice.stain: 0.7385, Dice.edgeDamage: 0.3333
2023-10-31 20:01:00,201 - mmseg - INFO - Iter [4050/5000]	lr: 3.021e-05, eta: 0:27:36, time: 2.738, data_time: 1.542, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6606, decode.acc_seg: 99.8291, loss: 0.6630
2023-10-31 20:02:22,531 - mmseg - INFO - Iter [4100/5000]	lr: 2.925e-05, eta: 0:26:07, time: 1.647, data_time: 0.450, memory: 25234, decode.loss_focal: 0.0025, decode.loss_dice: 0.6624, decode.acc_seg: 99.8278, loss: 0.6648
2023-10-31 20:03:44,877 - mmseg - INFO - Iter [4150/5000]	lr: 2.829e-05, eta: 0:24:39, time: 1.647, data_time: 0.447, memory: 25234, decode.loss_focal: 0.0026, decode.loss_dice: 0.6594, decode.acc_seg: 99.8195, loss: 0.6620
2023-10-31 20:05:03,667 - mmseg - INFO - Iter [4200/5000]	lr: 2.732e-05, eta: 0:23:11, time: 1.576, data_time: 0.369, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6567, decode.acc_seg: 99.8297, loss: 0.6592
2023-10-31 20:06:26,106 - mmseg - INFO - Iter [4250/5000]	lr: 2.634e-05, eta: 0:21:43, time: 1.649, data_time: 0.450, memory: 25234, decode.loss_focal: 0.0023, decode.loss_dice: 0.6557, decode.acc_seg: 99.8357, loss: 0.6580
2023-10-31 20:07:45,001 - mmseg - INFO - Iter [4300/5000]	lr: 2.536e-05, eta: 0:20:15, time: 1.578, data_time: 0.376, memory: 25234, decode.loss_focal: 0.0025, decode.loss_dice: 0.6573, decode.acc_seg: 99.8264, loss: 0.6599
2023-10-31 20:09:07,580 - mmseg - INFO - Iter [4350/5000]	lr: 2.437e-05, eta: 0:18:47, time: 1.652, data_time: 0.451, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6546, decode.acc_seg: 99.8293, loss: 0.6571
2023-10-31 20:10:26,288 - mmseg - INFO - Iter [4400/5000]	lr: 2.337e-05, eta: 0:17:19, time: 1.574, data_time: 0.374, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6508, decode.acc_seg: 99.8339, loss: 0.6531
2023-10-31 20:11:48,810 - mmseg - INFO - Iter [4450/5000]	lr: 2.237e-05, eta: 0:15:52, time: 1.650, data_time: 0.449, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6611, decode.acc_seg: 99.8300, loss: 0.6635
2023-10-31 20:13:07,400 - mmseg - INFO - Iter [4500/5000]	lr: 2.135e-05, eta: 0:14:25, time: 1.572, data_time: 0.373, memory: 25234, decode.loss_focal: 0.0025, decode.loss_dice: 0.6531, decode.acc_seg: 99.8269, loss: 0.6556
2023-10-31 20:14:06,205 - mmseg - INFO - per class results:
2023-10-31 20:14:06,206 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 | 72.47 | 70.39 | 99.96 | 99.96  |   99.95   | 99.97  | 99.94 |
|  scratch   | 84.71 | 85.66 | 73.52 | 75.62 | 76.36 | 84.55  |   84.86   | 84.24  | 76.82 |
|   stain    | 83.62 | 86.75 | 73.95 | 77.78 | 66.65 | 83.03  |   91.55   | 77.77  | 74.54 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 20:14:06,206 - mmseg - INFO - Summary:
2023-10-31 20:14:06,207 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 84.66 | 85.71 | 73.31 |  74.6 | 69.08 |  89.18  |   92.12    |  79.38  | 71.16 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 20:14:06,212 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8466, mVOE: 0.8571, mASD: 0.7331, mMSSD: 0.7460, mAcc: 0.6908, mFscore: 0.8918, mPrecision: 0.9212, mRecall: 0.7938, mDice: 0.7116, IoU.background: 0.9994, IoU.scratch: 0.8471, IoU.stain: 0.8362, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8566, VOE.stain: 0.8675, VOE.edgeDamage: 1.0000, ASD.background: 0.7247, ASD.scratch: 0.7352, ASD.stain: 0.7395, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7562, MSSD.stain: 0.7778, MSSD.edgeDamage: nan, Acc.background: 0.9996, Acc.scratch: 0.7636, Acc.stain: 0.6665, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8455, Fscore.stain: 0.8303, Fscore.edgeDamage: nan, Precision.background: 0.9995, Precision.scratch: 0.8486, Precision.stain: 0.9155, Precision.edgeDamage: nan, Recall.background: 0.9997, Recall.scratch: 0.8424, Recall.stain: 0.7777, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7682, Dice.stain: 0.7454, Dice.edgeDamage: 0.3333
2023-10-31 20:15:27,993 - mmseg - INFO - Iter [4550/5000]	lr: 2.033e-05, eta: 0:13:04, time: 2.812, data_time: 1.617, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6539, decode.acc_seg: 99.8285, loss: 0.6563
2023-10-31 20:16:50,372 - mmseg - INFO - Iter [4600/5000]	lr: 1.929e-05, eta: 0:11:36, time: 1.648, data_time: 0.451, memory: 25234, decode.loss_focal: 0.0023, decode.loss_dice: 0.6520, decode.acc_seg: 99.8342, loss: 0.6543
2023-10-31 20:18:09,446 - mmseg - INFO - Iter [4650/5000]	lr: 1.824e-05, eta: 0:10:08, time: 1.581, data_time: 0.384, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6470, decode.acc_seg: 99.8329, loss: 0.6494
2023-10-31 20:19:32,152 - mmseg - INFO - Iter [4700/5000]	lr: 1.718e-05, eta: 0:08:41, time: 1.654, data_time: 0.448, memory: 25234, decode.loss_focal: 0.0023, decode.loss_dice: 0.6449, decode.acc_seg: 99.8357, loss: 0.6472
2023-10-31 20:20:51,177 - mmseg - INFO - Iter [4750/5000]	lr: 1.609e-05, eta: 0:07:14, time: 1.580, data_time: 0.378, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6459, decode.acc_seg: 99.8324, loss: 0.6484
2023-10-31 20:22:13,804 - mmseg - INFO - Iter [4800/5000]	lr: 1.499e-05, eta: 0:05:47, time: 1.652, data_time: 0.454, memory: 25234, decode.loss_focal: 0.0023, decode.loss_dice: 0.6455, decode.acc_seg: 99.8349, loss: 0.6479
2023-10-31 20:23:32,916 - mmseg - INFO - Iter [4850/5000]	lr: 1.386e-05, eta: 0:04:20, time: 1.582, data_time: 0.379, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6508, decode.acc_seg: 99.8316, loss: 0.6532
2023-10-31 20:24:55,932 - mmseg - INFO - Iter [4900/5000]	lr: 1.269e-05, eta: 0:02:53, time: 1.660, data_time: 0.453, memory: 25234, decode.loss_focal: 0.0023, decode.loss_dice: 0.6553, decode.acc_seg: 99.8320, loss: 0.6577
2023-10-31 20:26:14,804 - mmseg - INFO - Iter [4950/5000]	lr: 1.145e-05, eta: 0:01:26, time: 1.577, data_time: 0.372, memory: 25234, decode.loss_focal: 0.0024, decode.loss_dice: 0.6489, decode.acc_seg: 99.8295, loss: 0.6513
2023-10-31 20:27:37,504 - mmseg - INFO - Saving checkpoint at 5000 iterations
2023-10-31 20:27:38,084 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 20:27:38,085 - mmseg - INFO - Iter [5000/5000]	lr: 1.004e-05, eta: 0:00:00, time: 1.666, data_time: 0.454, memory: 25234, decode.loss_focal: 0.0023, decode.loss_dice: 0.6464, decode.acc_seg: 99.8342, loss: 0.6488
2023-10-31 20:28:37,367 - mmseg - INFO - per class results:
2023-10-31 20:28:37,369 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.95 | 70.43 | 72.47 | 70.39 | 99.96 | 99.96  |   99.95   | 99.97  | 99.94 |
|  scratch   | 84.62 | 85.75 | 73.56 | 75.85 | 75.32 | 84.42  |   85.35   | 83.55  | 76.63 |
|   stain    | 83.53 | 86.84 | 73.93 | 77.68 | 67.12 | 82.89  |   90.32   | 78.08  | 74.34 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 20:28:37,369 - mmseg - INFO - Summary:
2023-10-31 20:28:37,369 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.95 | 84.62 | 85.75 | 73.32 | 74.64 | 68.93 |  89.09  |   91.87    |  79.29  | 71.06 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 20:28:37,375 - mmseg - INFO - Exp name: unet_transformer_block.py
2023-10-31 20:28:37,376 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9995, mIoU: 0.8462, mVOE: 0.8575, mASD: 0.7332, mMSSD: 0.7464, mAcc: 0.6893, mFscore: 0.8909, mPrecision: 0.9187, mRecall: 0.7929, mDice: 0.7106, IoU.background: 0.9995, IoU.scratch: 0.8462, IoU.stain: 0.8353, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8575, VOE.stain: 0.8684, VOE.edgeDamage: 1.0000, ASD.background: 0.7247, ASD.scratch: 0.7356, ASD.stain: 0.7393, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7585, MSSD.stain: 0.7768, MSSD.edgeDamage: nan, Acc.background: 0.9996, Acc.scratch: 0.7532, Acc.stain: 0.6712, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8442, Fscore.stain: 0.8289, Fscore.edgeDamage: nan, Precision.background: 0.9995, Precision.scratch: 0.8535, Precision.stain: 0.9032, Precision.edgeDamage: nan, Recall.background: 0.9997, Recall.scratch: 0.8355, Recall.stain: 0.7808, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7663, Dice.stain: 0.7434, Dice.edgeDamage: 0.3333
