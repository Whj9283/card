2023-10-31 20:28:49,792 - mmseg - INFO - Multi-processing start method is `None`
2023-10-31 20:28:49,793 - mmseg - INFO - OpenCV num_threads is `112
2023-10-31 20:28:49,793 - mmseg - INFO - OMP num threads is 1
2023-10-31 20:28:49,962 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3: Quadro RTX 8000
CUDA_HOME: /usr/local/cuda-10.1
NVCC: Cuda compilation tools, release 10.1, V10.1.10
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.10.0+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.1+cu102
OpenCV: 4.7.0
MMCV: 1.7.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMSegmentation: 0.29.1+
------------------------------------------------------------

2023-10-31 20:28:49,962 - mmseg - INFO - Distributed training: True
2023-10-31 20:28:50,339 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
backbone_norm_cfg = dict(type='LN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained=None,
    backbone=dict(
        type='UnetBackbone',
        in_channels=3,
        context_layer='kernelselect',
        transformer_block=True,
        channel_list=[64, 128, 256, 512]),
    decode_head=dict(
        type='UnetHead',
        num_classes=4,
        channels=64,
        threshold=0.2,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        loss_decode=[
            dict(
                type='FocalLoss',
                use_sigmoid=True,
                gamma=2.0,
                alpha=0.25,
                loss_weight=2.0),
            dict(type='DiceLoss', loss_name='loss_dice', loss_weight=2.0)
        ]))
train_cfg = dict()
test_cfg = dict(mode='whole')
dataset_type = 'MyDataset'
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(600, 600)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(512, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=False),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data_root = './datasets/'
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=2,
    train=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='train/images',
        ann_dir='train/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(600, 600)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TensorboardLoggerHook'),
        dict(type='TextLoggerHook', by_epoch=False)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = False
optimizer = dict(type='Adam', lr=0.0001, betas=(0.9, 0.999))
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=1e-05, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=5000)
checkpoint_config = dict(by_epoch=False, save_optimizer=False, interval=5000)
evaluation = dict(interval=500, metric=['mIoU', 'mFscore', 'mDice'])
work_dir = './work_dirs/unet_all'
gpu_ids = range(0, 4)
auto_resume = False

2023-10-31 20:28:50,339 - mmseg - INFO - Set random seed to 0, deterministic: False
2023-10-31 20:28:50,750 - mmseg - INFO - initialize UnetHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

backbone.inc.conv.conv.0.weight - torch.Size([64, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.0.weight - torch.Size([128, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.3.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.0.weight - torch.Size([256, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.3.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.0.weight - torch.Size([512, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.3.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.4.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.0.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.3.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.4.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv.4.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.0.0.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.0.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.1.0.weight - torch.Size([64, 64, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.1.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.2.0.weight - torch.Size([64, 64, 7, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.2.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.2.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.2.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.3.0.weight - torch.Size([64, 64, 9, 9]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.3.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.3.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.convs.3.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fc.weight - torch.Size([32, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fc.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fcs.0.weight - torch.Size([64, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fcs.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fcs.1.weight - torch.Size([64, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fcs.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fcs.2.weight - torch.Size([64, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fcs.2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fcs.3.weight - torch.Size([64, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer1_1.fcs.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.0.0.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.0.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.1.0.weight - torch.Size([128, 128, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.1.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.2.0.weight - torch.Size([128, 128, 7, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.2.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.3.0.weight - torch.Size([128, 128, 9, 9]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.3.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.3.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.convs.3.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fc.weight - torch.Size([32, 128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fc.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fcs.0.weight - torch.Size([128, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fcs.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fcs.1.weight - torch.Size([128, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fcs.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fcs.2.weight - torch.Size([128, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fcs.2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fcs.3.weight - torch.Size([128, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer2_1.fcs.3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.0.0.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.1.0.weight - torch.Size([256, 256, 5, 5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.2.0.weight - torch.Size([256, 256, 7, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.3.0.weight - torch.Size([256, 256, 9, 9]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.convs.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fc.weight - torch.Size([32, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fc.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fcs.0.weight - torch.Size([256, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fcs.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fcs.1.weight - torch.Size([256, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fcs.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fcs.2.weight - torch.Size([256, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fcs.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fcs.3.weight - torch.Size([256, 32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.context_layer3_1.fcs.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.linear.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.linear.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.0.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.1.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.2.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp4.tr.3.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.linear.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.linear.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.0.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.1.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.2.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.q.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.k.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.v.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.ma.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.ma.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.ma.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.ma.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.fc1.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.aspp5.tr.3.fc2.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_seg.weight - torch.Size([4, 64, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.up1.conv.conv.0.weight - torch.Size([256, 1024, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.3.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.0.weight - torch.Size([128, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.3.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.0.weight - torch.Size([64, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.0.weight - torch.Size([64, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2023-10-31 20:28:50,757 - mmseg - INFO - EncoderDecoder(
  (backbone): UnetBackbone(
    (inc): InConv(
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (down1): Down(
      (down_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (conv): Sequential(
            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (4): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down2): Down(
      (down_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (conv): Sequential(
            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (4): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down3): Down(
      (down_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (conv): Sequential(
            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (4): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down4): Down(
      (down_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (conv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (4): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (context_layer1_1): KernelSelectAttention(
      (convs): ModuleList(
        (0): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): Sequential(
          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (2): Sequential(
          (0): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (3): Sequential(
          (0): Conv2d(64, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (fc): Linear(in_features=64, out_features=32, bias=True)
      (fcs): ModuleList(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): Linear(in_features=32, out_features=64, bias=True)
        (2): Linear(in_features=32, out_features=64, bias=True)
        (3): Linear(in_features=32, out_features=64, bias=True)
      )
      (softmax): Softmax(dim=0)
    )
    (context_layer2_1): KernelSelectAttention(
      (convs): ModuleList(
        (0): Sequential(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): Sequential(
          (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (2): Sequential(
          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (3): Sequential(
          (0): Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))
          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (fc): Linear(in_features=128, out_features=32, bias=True)
      (fcs): ModuleList(
        (0): Linear(in_features=32, out_features=128, bias=True)
        (1): Linear(in_features=32, out_features=128, bias=True)
        (2): Linear(in_features=32, out_features=128, bias=True)
        (3): Linear(in_features=32, out_features=128, bias=True)
      )
      (softmax): Softmax(dim=0)
    )
    (context_layer3_1): KernelSelectAttention(
      (convs): ModuleList(
        (0): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): Sequential(
          (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (2): Sequential(
          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (3): Sequential(
          (0): Conv2d(256, 256, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
      (fc): Linear(in_features=256, out_features=32, bias=True)
      (fcs): ModuleList(
        (0): Linear(in_features=32, out_features=256, bias=True)
        (1): Linear(in_features=32, out_features=256, bias=True)
        (2): Linear(in_features=32, out_features=256, bias=True)
        (3): Linear(in_features=32, out_features=256, bias=True)
      )
      (softmax): Softmax(dim=0)
    )
    (aspp4): TransformerBlock(
      (linear): Linear(in_features=512, out_features=512, bias=True)
      (tr): Sequential(
        (0): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (1): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (2): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (3): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
      )
    )
    (aspp5): TransformerBlock(
      (linear): Linear(in_features=512, out_features=512, bias=True)
      (tr): Sequential(
        (0): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (1): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (2): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
        (3): TransformerLayer(
          (q): Linear(in_features=512, out_features=512, bias=False)
          (k): Linear(in_features=512, out_features=512, bias=False)
          (v): Linear(in_features=512, out_features=512, bias=False)
          (ma): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (fc1): Linear(in_features=512, out_features=512, bias=False)
          (fc2): Linear(in_features=512, out_features=512, bias=False)
        )
      )
    )
  )
  (decode_head): UnetHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): ModuleList(
      (0): FocalLoss()
      (1): DiceLoss()
    )
    (conv_seg): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (up1): Up(
      (up): Upsample(scale_factor=2.0, mode=bilinear)
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up2): Up(
      (up): Upsample(scale_factor=2.0, mode=bilinear)
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up3): Up(
      (up): Upsample(scale_factor=2.0, mode=bilinear)
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up4): Up(
      (up): Upsample(scale_factor=2.0, mode=bilinear)
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2023-10-31 20:28:50,764 - mmseg - INFO - Loaded 308 images
2023-10-31 20:28:56,556 - mmseg - INFO - Loaded 78 images
2023-10-31 20:28:56,576 - mmseg - INFO - Start running, host: zhangzifan@s2, work_dir: /data2/zhangzifan/code_dir/2023-10-25-01/work_dirs/unet_all
2023-10-31 20:28:56,576 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-10-31 20:28:56,576 - mmseg - INFO - workflow: [('train', 1)], max: 5000 iters
2023-10-31 20:28:56,577 - mmseg - INFO - Checkpoints will be saved to /data2/zhangzifan/code_dir/2023-10-25-01/work_dirs/unet_all by HardDiskBackend.
2023-10-31 20:32:26,313 - mmseg - INFO - Iter [50/5000]	lr: 9.921e-05, eta: 5:42:21, time: 4.150, data_time: 0.514, memory: 36260, decode.loss_focal: 0.1781, decode.loss_dice: 1.7239, decode.acc_seg: 90.8162, loss: 1.9021
2023-10-31 20:35:51,957 - mmseg - INFO - Iter [100/5000]	lr: 9.839e-05, eta: 5:37:23, time: 4.113, data_time: 0.449, memory: 36260, decode.loss_focal: 0.1325, decode.loss_dice: 1.6704, decode.acc_seg: 99.5745, loss: 1.8028
2023-10-31 20:39:13,511 - mmseg - INFO - Iter [150/5000]	lr: 9.758e-05, eta: 5:31:14, time: 4.031, data_time: 0.374, memory: 36260, decode.loss_focal: 0.0996, decode.loss_dice: 1.6109, decode.acc_seg: 99.4346, loss: 1.7104
2023-10-31 20:42:38,911 - mmseg - INFO - Iter [200/5000]	lr: 9.677e-05, eta: 5:28:02, time: 4.108, data_time: 0.444, memory: 36260, decode.loss_focal: 0.0753, decode.loss_dice: 1.5513, decode.acc_seg: 99.4341, loss: 1.6267
2023-10-31 20:46:00,350 - mmseg - INFO - Iter [250/5000]	lr: 9.596e-05, eta: 5:23:28, time: 4.029, data_time: 0.371, memory: 36260, decode.loss_focal: 0.0579, decode.loss_dice: 1.4936, decode.acc_seg: 99.4769, loss: 1.5514
2023-10-31 20:49:26,297 - mmseg - INFO - Iter [300/5000]	lr: 9.514e-05, eta: 5:20:30, time: 4.119, data_time: 0.447, memory: 36260, decode.loss_focal: 0.0453, decode.loss_dice: 1.4313, decode.acc_seg: 99.5572, loss: 1.4766
2023-10-31 20:52:48,096 - mmseg - INFO - Iter [350/5000]	lr: 9.433e-05, eta: 5:16:28, time: 4.036, data_time: 0.370, memory: 36260, decode.loss_focal: 0.0365, decode.loss_dice: 1.3686, decode.acc_seg: 99.6180, loss: 1.4051
2023-10-31 20:56:13,421 - mmseg - INFO - Iter [400/5000]	lr: 9.351e-05, eta: 5:13:17, time: 4.107, data_time: 0.445, memory: 36260, decode.loss_focal: 0.0295, decode.loss_dice: 1.3033, decode.acc_seg: 99.6643, loss: 1.3328
2023-10-31 20:59:34,995 - mmseg - INFO - Iter [450/5000]	lr: 9.269e-05, eta: 5:09:25, time: 4.032, data_time: 0.375, memory: 36260, decode.loss_focal: 0.0246, decode.loss_dice: 1.2461, decode.acc_seg: 99.6776, loss: 1.2707
2023-10-31 21:03:00,368 - mmseg - INFO - Iter [500/5000]	lr: 9.187e-05, eta: 5:06:13, time: 4.107, data_time: 0.452, memory: 36260, decode.loss_focal: 0.0201, decode.loss_dice: 1.1956, decode.acc_seg: 99.7380, loss: 1.2157
2023-10-31 21:04:03,086 - mmseg - INFO - per class results:
2023-10-31 21:04:03,088 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.91 | 70.46 |  72.8 |  70.4 | 99.87 | 99.94  |   99.96   | 99.91  |  99.9 |
|  scratch   |  81.2 | 89.17 | 74.39 | 78.34 | 75.72 | 79.35  |    76.1   | 83.82  | 69.02 |
|   stain    | 78.62 | 91.75 | 74.77 | 80.26 | 75.41 | 74.91  |   70.33   | 83.61  | 62.37 |
| edgeDamage | 86.49 | 83.89 |  73.7 | 74.87 | 79.73 | 86.87  |   87.26   | 86.49  |  80.3 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 21:04:03,088 - mmseg - INFO - Summary:
2023-10-31 21:04:03,088 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.91 | 86.55 | 83.82 | 73.92 | 75.97 | 82.68 |  85.27  |   83.41    |  88.46  |  77.9 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 21:04:03,093 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9991, mIoU: 0.8655, mVOE: 0.8382, mASD: 0.7392, mMSSD: 0.7597, mAcc: 0.8268, mFscore: 0.8527, mPrecision: 0.8341, mRecall: 0.8846, mDice: 0.7790, IoU.background: 0.9991, IoU.scratch: 0.8120, IoU.stain: 0.7862, IoU.edgeDamage: 0.8649, VOE.background: 0.7046, VOE.scratch: 0.8917, VOE.stain: 0.9175, VOE.edgeDamage: 0.8389, ASD.background: 0.7280, ASD.scratch: 0.7439, ASD.stain: 0.7477, ASD.edgeDamage: 0.7370, MSSD.background: 0.7040, MSSD.scratch: 0.7834, MSSD.stain: 0.8026, MSSD.edgeDamage: 0.7487, Acc.background: 0.9987, Acc.scratch: 0.7572, Acc.stain: 0.7541, Acc.edgeDamage: 0.7973, Fscore.background: 0.9994, Fscore.scratch: 0.7935, Fscore.stain: 0.7491, Fscore.edgeDamage: 0.8687, Precision.background: 0.9996, Precision.scratch: 0.7610, Precision.stain: 0.7033, Precision.edgeDamage: 0.8726, Recall.background: 0.9991, Recall.scratch: 0.8382, Recall.stain: 0.8361, Recall.edgeDamage: 0.8649, Dice.background: 0.9990, Dice.scratch: 0.6902, Dice.stain: 0.6237, Dice.edgeDamage: 0.8030
2023-10-31 21:07:27,787 - mmseg - INFO - Iter [550/5000]	lr: 9.106e-05, eta: 5:11:21, time: 5.348, data_time: 1.701, memory: 36260, decode.loss_focal: 0.0166, decode.loss_dice: 1.1431, decode.acc_seg: 99.7678, loss: 1.1597
2023-10-31 21:10:49,727 - mmseg - INFO - Iter [600/5000]	lr: 9.024e-05, eta: 5:06:53, time: 4.039, data_time: 0.371, memory: 36260, decode.loss_focal: 0.0141, decode.loss_dice: 1.1085, decode.acc_seg: 99.7594, loss: 1.1226
2023-10-31 21:14:14,841 - mmseg - INFO - Iter [650/5000]	lr: 8.941e-05, eta: 5:02:56, time: 4.102, data_time: 0.444, memory: 36260, decode.loss_focal: 0.0118, decode.loss_dice: 1.0771, decode.acc_seg: 99.7916, loss: 1.0889
2023-10-31 21:17:36,526 - mmseg - INFO - Iter [700/5000]	lr: 8.859e-05, eta: 4:58:42, time: 4.034, data_time: 0.378, memory: 36260, decode.loss_focal: 0.0101, decode.loss_dice: 1.0494, decode.acc_seg: 99.8068, loss: 1.0595
2023-10-31 21:21:01,409 - mmseg - INFO - Iter [750/5000]	lr: 8.777e-05, eta: 4:54:54, time: 4.098, data_time: 0.445, memory: 36260, decode.loss_focal: 0.0089, decode.loss_dice: 1.0313, decode.acc_seg: 99.8115, loss: 1.0402
2023-10-31 21:24:22,654 - mmseg - INFO - Iter [800/5000]	lr: 8.695e-05, eta: 4:50:50, time: 4.025, data_time: 0.377, memory: 36260, decode.loss_focal: 0.0079, decode.loss_dice: 1.0144, decode.acc_seg: 99.8191, loss: 1.0223
2023-10-31 21:27:47,825 - mmseg - INFO - Iter [850/5000]	lr: 8.612e-05, eta: 4:47:09, time: 4.103, data_time: 0.448, memory: 36260, decode.loss_focal: 0.0071, decode.loss_dice: 1.0004, decode.acc_seg: 99.8312, loss: 1.0074
2023-10-31 21:31:09,132 - mmseg - INFO - Iter [900/5000]	lr: 8.530e-05, eta: 4:43:13, time: 4.026, data_time: 0.375, memory: 36260, decode.loss_focal: 0.0065, decode.loss_dice: 0.9942, decode.acc_seg: 99.8270, loss: 1.0007
2023-10-31 21:34:33,983 - mmseg - INFO - Iter [950/5000]	lr: 8.447e-05, eta: 4:39:36, time: 4.097, data_time: 0.448, memory: 36260, decode.loss_focal: 0.0067, decode.loss_dice: 0.9895, decode.acc_seg: 99.8200, loss: 0.9963
2023-10-31 21:37:58,783 - mmseg - INFO - Exp name: unet_all.py
2023-10-31 21:37:58,784 - mmseg - INFO - Iter [1000/5000]	lr: 8.364e-05, eta: 4:35:59, time: 4.096, data_time: 0.452, memory: 36260, decode.loss_focal: 0.0072, decode.loss_dice: 1.0051, decode.acc_seg: 99.7753, loss: 1.0123
2023-10-31 21:38:58,636 - mmseg - INFO - per class results:
2023-10-31 21:38:58,637 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.93 | 70.44 |  72.8 | 70.39 | 99.96 | 99.95  |   99.93   | 99.97  | 99.93 |
|  scratch   | 81.37 |  89.0 |  74.3 | 77.89 | 66.16 | 79.62  |   82.28   | 77.44  | 69.43 |
|   stain    | 76.48 | 93.89 |  75.1 | 81.88 | 48.23 | 70.75  |   87.91   | 65.48  | 56.12 |
| edgeDamage | 72.02 | 98.35 | 75.58 | 84.31 | 37.29 | 60.25  |   77.14   | 58.19  | 40.38 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 21:38:58,637 - mmseg - INFO - Summary:
2023-10-31 21:38:58,637 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.93 | 82.45 | 87.92 | 74.45 | 78.62 | 62.91 |  77.64  |   86.82    |  75.27  | 66.47 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 21:38:58,641 - mmseg - INFO - Exp name: unet_all.py
2023-10-31 21:38:58,642 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9993, mIoU: 0.8245, mVOE: 0.8792, mASD: 0.7445, mMSSD: 0.7862, mAcc: 0.6291, mFscore: 0.7764, mPrecision: 0.8682, mRecall: 0.7527, mDice: 0.6647, IoU.background: 0.9993, IoU.scratch: 0.8137, IoU.stain: 0.7648, IoU.edgeDamage: 0.7202, VOE.background: 0.7044, VOE.scratch: 0.8900, VOE.stain: 0.9389, VOE.edgeDamage: 0.9835, ASD.background: 0.7280, ASD.scratch: 0.7430, ASD.stain: 0.7510, ASD.edgeDamage: 0.7558, MSSD.background: 0.7039, MSSD.scratch: 0.7789, MSSD.stain: 0.8188, MSSD.edgeDamage: 0.8431, Acc.background: 0.9996, Acc.scratch: 0.6616, Acc.stain: 0.4823, Acc.edgeDamage: 0.3729, Fscore.background: 0.9995, Fscore.scratch: 0.7962, Fscore.stain: 0.7075, Fscore.edgeDamage: 0.6025, Precision.background: 0.9993, Precision.scratch: 0.8228, Precision.stain: 0.8791, Precision.edgeDamage: 0.7714, Recall.background: 0.9997, Recall.scratch: 0.7744, Recall.stain: 0.6548, Recall.edgeDamage: 0.5819, Dice.background: 0.9993, Dice.scratch: 0.6943, Dice.stain: 0.5612, Dice.edgeDamage: 0.4038
2023-10-31 21:42:19,129 - mmseg - INFO - Iter [1050/5000]	lr: 8.281e-05, eta: 4:35:53, time: 5.207, data_time: 1.575, memory: 36260, decode.loss_focal: 0.0068, decode.loss_dice: 0.9341, decode.acc_seg: 99.7784, loss: 0.9408
2023-10-31 21:45:43,607 - mmseg - INFO - Iter [1100/5000]	lr: 8.198e-05, eta: 4:32:06, time: 4.090, data_time: 0.446, memory: 36260, decode.loss_focal: 0.0060, decode.loss_dice: 0.8711, decode.acc_seg: 99.7897, loss: 0.8771
2023-10-31 21:49:04,623 - mmseg - INFO - Iter [1150/5000]	lr: 8.115e-05, eta: 4:28:08, time: 4.020, data_time: 0.377, memory: 36260, decode.loss_focal: 0.0055, decode.loss_dice: 0.8360, decode.acc_seg: 99.7908, loss: 0.8415
2023-10-31 21:52:29,519 - mmseg - INFO - Iter [1200/5000]	lr: 8.032e-05, eta: 4:24:27, time: 4.098, data_time: 0.451, memory: 36260, decode.loss_focal: 0.0051, decode.loss_dice: 0.8106, decode.acc_seg: 99.7832, loss: 0.8157
2023-10-31 21:55:50,675 - mmseg - INFO - Iter [1250/5000]	lr: 7.949e-05, eta: 4:20:35, time: 4.023, data_time: 0.365, memory: 36260, decode.loss_focal: 0.0058, decode.loss_dice: 0.9952, decode.acc_seg: 99.6880, loss: 1.0010
2023-10-31 21:59:15,161 - mmseg - INFO - Iter [1300/5000]	lr: 7.865e-05, eta: 4:16:55, time: 4.090, data_time: 0.448, memory: 36260, decode.loss_focal: 0.0054, decode.loss_dice: 0.9011, decode.acc_seg: 99.7203, loss: 0.9065
2023-10-31 22:02:36,049 - mmseg - INFO - Iter [1350/5000]	lr: 7.782e-05, eta: 4:13:07, time: 4.018, data_time: 0.376, memory: 36260, decode.loss_focal: 0.0051, decode.loss_dice: 0.8631, decode.acc_seg: 99.7416, loss: 0.8682
2023-10-31 22:06:00,489 - mmseg - INFO - Iter [1400/5000]	lr: 7.698e-05, eta: 4:09:29, time: 4.089, data_time: 0.447, memory: 36260, decode.loss_focal: 0.0052, decode.loss_dice: 0.8794, decode.acc_seg: 99.7234, loss: 0.8846
2023-10-31 22:09:25,154 - mmseg - INFO - Iter [1450/5000]	lr: 7.614e-05, eta: 4:05:54, time: 4.093, data_time: 0.450, memory: 36260, decode.loss_focal: 0.0051, decode.loss_dice: 0.8489, decode.acc_seg: 99.7365, loss: 0.8540
2023-10-31 22:12:45,832 - mmseg - INFO - Iter [1500/5000]	lr: 7.530e-05, eta: 4:02:09, time: 4.014, data_time: 0.372, memory: 36260, decode.loss_focal: 0.0048, decode.loss_dice: 0.8418, decode.acc_seg: 99.7397, loss: 0.8467
2023-10-31 22:13:45,501 - mmseg - INFO - per class results:
2023-10-31 22:13:45,502 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 |  72.8 | 70.39 | 99.97 | 99.95  |   99.93   | 99.98  | 99.93 |
|  scratch   | 80.75 | 89.62 | 74.47 | 78.74 | 62.35 | 78.61  |    84.1   |  74.9  | 67.92 |
|   stain    | 81.04 | 89.33 | 74.64 | 79.57 | 58.59 | 79.09  |   94.62   | 72.39  | 68.64 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 22:13:45,502 - mmseg - INFO - Summary:
2023-10-31 22:13:45,503 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 83.02 | 87.35 | 73.97 | 76.23 | 63.56 |  85.89  |   92.88    |  75.71  | 67.45 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 22:13:45,506 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8302, mVOE: 0.8735, mASD: 0.7397, mMSSD: 0.7623, mAcc: 0.6356, mFscore: 0.8589, mPrecision: 0.9288, mRecall: 0.7571, mDice: 0.6745, IoU.background: 0.9994, IoU.scratch: 0.8075, IoU.stain: 0.8104, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8962, VOE.stain: 0.8933, VOE.edgeDamage: 1.0000, ASD.background: 0.7280, ASD.scratch: 0.7447, ASD.stain: 0.7464, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7874, MSSD.stain: 0.7957, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.6235, Acc.stain: 0.5859, Acc.edgeDamage: 0.3333, Fscore.background: 0.9995, Fscore.scratch: 0.7861, Fscore.stain: 0.7909, Fscore.edgeDamage: nan, Precision.background: 0.9993, Precision.scratch: 0.8410, Precision.stain: 0.9462, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.7490, Recall.stain: 0.7239, Recall.edgeDamage: 0.5556, Dice.background: 0.9993, Dice.scratch: 0.6792, Dice.stain: 0.6864, Dice.edgeDamage: 0.3333
2023-10-31 22:17:09,240 - mmseg - INFO - Iter [1550/5000]	lr: 7.446e-05, eta: 4:00:46, time: 5.268, data_time: 1.642, memory: 36260, decode.loss_focal: 0.0046, decode.loss_dice: 0.8173, decode.acc_seg: 99.7509, loss: 0.8219
2023-10-31 22:20:30,069 - mmseg - INFO - Iter [1600/5000]	lr: 7.362e-05, eta: 3:56:58, time: 4.017, data_time: 0.378, memory: 36260, decode.loss_focal: 0.0045, decode.loss_dice: 0.8098, decode.acc_seg: 99.7558, loss: 0.8143
2023-10-31 22:23:54,396 - mmseg - INFO - Iter [1650/5000]	lr: 7.278e-05, eta: 3:53:20, time: 4.086, data_time: 0.440, memory: 36260, decode.loss_focal: 0.0045, decode.loss_dice: 0.8051, decode.acc_seg: 99.7577, loss: 0.8096
2023-10-31 22:27:15,274 - mmseg - INFO - Iter [1700/5000]	lr: 7.194e-05, eta: 3:49:35, time: 4.018, data_time: 0.373, memory: 36260, decode.loss_focal: 0.0044, decode.loss_dice: 0.7959, decode.acc_seg: 99.7582, loss: 0.8003
2023-10-31 22:30:39,904 - mmseg - INFO - Iter [1750/5000]	lr: 7.109e-05, eta: 3:45:59, time: 4.093, data_time: 0.453, memory: 36260, decode.loss_focal: 0.0042, decode.loss_dice: 0.7790, decode.acc_seg: 99.7670, loss: 0.7832
2023-10-31 22:34:00,878 - mmseg - INFO - Iter [1800/5000]	lr: 7.025e-05, eta: 3:42:16, time: 4.019, data_time: 0.371, memory: 36260, decode.loss_focal: 0.0042, decode.loss_dice: 0.7779, decode.acc_seg: 99.7685, loss: 0.7821
2023-10-31 22:37:25,630 - mmseg - INFO - Iter [1850/5000]	lr: 6.940e-05, eta: 3:38:42, time: 4.095, data_time: 0.446, memory: 36260, decode.loss_focal: 0.0042, decode.loss_dice: 0.7875, decode.acc_seg: 99.7660, loss: 0.7917
2023-10-31 22:40:50,038 - mmseg - INFO - Iter [1900/5000]	lr: 6.855e-05, eta: 3:35:07, time: 4.088, data_time: 0.443, memory: 36260, decode.loss_focal: 0.0040, decode.loss_dice: 0.7600, decode.acc_seg: 99.7754, loss: 0.7640
2023-10-31 22:44:10,795 - mmseg - INFO - Iter [1950/5000]	lr: 6.770e-05, eta: 3:31:27, time: 4.015, data_time: 0.371, memory: 36260, decode.loss_focal: 0.0039, decode.loss_dice: 0.7535, decode.acc_seg: 99.7804, loss: 0.7574
2023-10-31 22:47:35,193 - mmseg - INFO - Exp name: unet_all.py
2023-10-31 22:47:35,194 - mmseg - INFO - Iter [2000/5000]	lr: 6.685e-05, eta: 3:27:54, time: 4.088, data_time: 0.445, memory: 36260, decode.loss_focal: 0.0037, decode.loss_dice: 0.7538, decode.acc_seg: 99.7849, loss: 0.7575
2023-10-31 22:48:34,693 - mmseg - INFO - per class results:
2023-10-31 22:48:34,694 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 |  72.8 | 70.39 | 99.96 | 99.95  |   99.94   | 99.97  | 99.93 |
|  scratch   | 82.49 | 87.88 | 74.17 | 77.25 | 69.03 | 81.36  |   83.73   | 79.35  | 72.04 |
|   stain    | 82.09 | 88.28 | 74.52 | 78.97 | 61.31 | 80.75  |   94.37   | 74.21  | 71.13 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 22:48:34,694 - mmseg - INFO - Summary:
2023-10-31 22:48:34,694 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 83.72 | 86.65 | 73.83 | 75.54 | 65.91 |  87.35  |   92.68    |  77.27  | 69.11 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 22:48:34,699 - mmseg - INFO - Exp name: unet_all.py
2023-10-31 22:48:34,700 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8372, mVOE: 0.8665, mASD: 0.7383, mMSSD: 0.7554, mAcc: 0.6591, mFscore: 0.8735, mPrecision: 0.9268, mRecall: 0.7727, mDice: 0.6911, IoU.background: 0.9994, IoU.scratch: 0.8249, IoU.stain: 0.8209, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8788, VOE.stain: 0.8828, VOE.edgeDamage: 1.0000, ASD.background: 0.7280, ASD.scratch: 0.7417, ASD.stain: 0.7452, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7725, MSSD.stain: 0.7897, MSSD.edgeDamage: nan, Acc.background: 0.9996, Acc.scratch: 0.6903, Acc.stain: 0.6131, Acc.edgeDamage: 0.3333, Fscore.background: 0.9995, Fscore.scratch: 0.8136, Fscore.stain: 0.8075, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8373, Precision.stain: 0.9437, Precision.edgeDamage: nan, Recall.background: 0.9997, Recall.scratch: 0.7935, Recall.stain: 0.7421, Recall.edgeDamage: 0.5556, Dice.background: 0.9993, Dice.scratch: 0.7204, Dice.stain: 0.7113, Dice.edgeDamage: 0.3333
2023-10-31 22:51:54,797 - mmseg - INFO - Iter [2050/5000]	lr: 6.599e-05, eta: 3:25:40, time: 5.192, data_time: 1.569, memory: 36260, decode.loss_focal: 0.0039, decode.loss_dice: 0.7449, decode.acc_seg: 99.7805, loss: 0.7488
2023-10-31 22:55:20,334 - mmseg - INFO - Iter [2100/5000]	lr: 6.514e-05, eta: 3:22:06, time: 4.111, data_time: 0.451, memory: 36260, decode.loss_focal: 0.0037, decode.loss_dice: 0.7458, decode.acc_seg: 99.7868, loss: 0.7495
2023-10-31 22:58:41,242 - mmseg - INFO - Iter [2150/5000]	lr: 6.428e-05, eta: 3:18:26, time: 4.018, data_time: 0.379, memory: 36260, decode.loss_focal: 0.0038, decode.loss_dice: 0.7360, decode.acc_seg: 99.7790, loss: 0.7398
2023-10-31 23:02:05,598 - mmseg - INFO - Iter [2200/5000]	lr: 6.343e-05, eta: 3:14:52, time: 4.087, data_time: 0.444, memory: 36260, decode.loss_focal: 0.0036, decode.loss_dice: 0.7333, decode.acc_seg: 99.7902, loss: 0.7369
2023-10-31 23:05:26,468 - mmseg - INFO - Iter [2250/5000]	lr: 6.257e-05, eta: 3:11:13, time: 4.017, data_time: 0.380, memory: 36260, decode.loss_focal: 0.0037, decode.loss_dice: 0.7337, decode.acc_seg: 99.7848, loss: 0.7374
2023-10-31 23:08:50,804 - mmseg - INFO - Iter [2300/5000]	lr: 6.171e-05, eta: 3:07:40, time: 4.087, data_time: 0.448, memory: 36260, decode.loss_focal: 0.0036, decode.loss_dice: 0.7264, decode.acc_seg: 99.7858, loss: 0.7300
2023-10-31 23:12:15,528 - mmseg - INFO - Iter [2350/5000]	lr: 6.084e-05, eta: 3:04:07, time: 4.094, data_time: 0.452, memory: 36260, decode.loss_focal: 0.0034, decode.loss_dice: 0.7251, decode.acc_seg: 99.7930, loss: 0.7285
2023-10-31 23:15:36,438 - mmseg - INFO - Iter [2400/5000]	lr: 5.998e-05, eta: 3:00:30, time: 4.018, data_time: 0.366, memory: 36260, decode.loss_focal: 0.0034, decode.loss_dice: 0.7193, decode.acc_seg: 99.8000, loss: 0.7227
2023-10-31 23:19:01,597 - mmseg - INFO - Iter [2450/5000]	lr: 5.911e-05, eta: 2:56:59, time: 4.103, data_time: 0.452, memory: 36260, decode.loss_focal: 0.0035, decode.loss_dice: 0.7072, decode.acc_seg: 99.7942, loss: 0.7107
2023-10-31 23:22:22,750 - mmseg - INFO - Iter [2500/5000]	lr: 5.825e-05, eta: 2:53:23, time: 4.023, data_time: 0.381, memory: 36260, decode.loss_focal: 0.0034, decode.loss_dice: 0.7168, decode.acc_seg: 99.7987, loss: 0.7202
2023-10-31 23:23:20,900 - mmseg - INFO - per class results:
2023-10-31 23:23:20,901 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 |  72.8 | 70.39 | 99.97 | 99.96  |   99.94   | 99.98  | 99.94 |
|  scratch   |  83.1 | 87.27 | 74.19 | 77.36 | 68.57 | 82.27  |   86.53   | 79.04  | 73.41 |
|   stain    | 83.47 |  86.9 | 74.29 | 77.84 | 66.37 |  82.8  |   91.27   | 77.58  |  74.2 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 23:23:20,901 - mmseg - INFO - Summary:
2023-10-31 23:23:20,902 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 84.22 | 86.15 | 73.76 |  75.2 | 67.06 |  88.34  |   92.58    |  78.04  | 70.22 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 23:23:20,906 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8422, mVOE: 0.8615, mASD: 0.7376, mMSSD: 0.7520, mAcc: 0.6706, mFscore: 0.8834, mPrecision: 0.9258, mRecall: 0.7804, mDice: 0.7022, IoU.background: 0.9994, IoU.scratch: 0.8310, IoU.stain: 0.8347, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8727, VOE.stain: 0.8690, VOE.edgeDamage: 1.0000, ASD.background: 0.7280, ASD.scratch: 0.7419, ASD.stain: 0.7429, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7736, MSSD.stain: 0.7784, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.6857, Acc.stain: 0.6637, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8227, Fscore.stain: 0.8280, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8653, Precision.stain: 0.9127, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.7904, Recall.stain: 0.7758, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7341, Dice.stain: 0.7420, Dice.edgeDamage: 0.3333
2023-10-31 23:26:44,710 - mmseg - INFO - Iter [2550/5000]	lr: 5.738e-05, eta: 2:50:47, time: 5.239, data_time: 1.606, memory: 36260, decode.loss_focal: 0.0033, decode.loss_dice: 0.7167, decode.acc_seg: 99.8009, loss: 0.7200
2023-10-31 23:30:05,610 - mmseg - INFO - Iter [2600/5000]	lr: 5.651e-05, eta: 2:47:10, time: 4.018, data_time: 0.376, memory: 36260, decode.loss_focal: 0.0033, decode.loss_dice: 0.7108, decode.acc_seg: 99.8003, loss: 0.7141
2023-10-31 23:33:30,449 - mmseg - INFO - Iter [2650/5000]	lr: 5.563e-05, eta: 2:43:38, time: 4.097, data_time: 0.453, memory: 36260, decode.loss_focal: 0.0032, decode.loss_dice: 0.7064, decode.acc_seg: 99.8019, loss: 0.7097
2023-10-31 23:36:51,569 - mmseg - INFO - Iter [2700/5000]	lr: 5.476e-05, eta: 2:40:02, time: 4.022, data_time: 0.379, memory: 36260, decode.loss_focal: 0.0032, decode.loss_dice: 0.7105, decode.acc_seg: 99.8017, loss: 0.7138
2023-10-31 23:40:16,065 - mmseg - INFO - Iter [2750/5000]	lr: 5.388e-05, eta: 2:36:30, time: 4.090, data_time: 0.451, memory: 36260, decode.loss_focal: 0.0031, decode.loss_dice: 0.7021, decode.acc_seg: 99.8072, loss: 0.7052
2023-10-31 23:43:40,751 - mmseg - INFO - Iter [2800/5000]	lr: 5.301e-05, eta: 2:32:58, time: 4.094, data_time: 0.452, memory: 36260, decode.loss_focal: 0.0032, decode.loss_dice: 0.7001, decode.acc_seg: 99.8010, loss: 0.7033
2023-10-31 23:47:01,647 - mmseg - INFO - Iter [2850/5000]	lr: 5.213e-05, eta: 2:29:24, time: 4.018, data_time: 0.377, memory: 36260, decode.loss_focal: 0.0030, decode.loss_dice: 0.6971, decode.acc_seg: 99.8072, loss: 0.7001
2023-10-31 23:50:26,158 - mmseg - INFO - Iter [2900/5000]	lr: 5.124e-05, eta: 2:25:52, time: 4.090, data_time: 0.450, memory: 36260, decode.loss_focal: 0.0031, decode.loss_dice: 0.6914, decode.acc_seg: 99.8100, loss: 0.6945
2023-10-31 23:53:47,217 - mmseg - INFO - Iter [2950/5000]	lr: 5.036e-05, eta: 2:22:19, time: 4.021, data_time: 0.376, memory: 36260, decode.loss_focal: 0.0031, decode.loss_dice: 0.6924, decode.acc_seg: 99.8050, loss: 0.6956
2023-10-31 23:57:11,934 - mmseg - INFO - Exp name: unet_all.py
2023-10-31 23:57:11,935 - mmseg - INFO - Iter [3000/5000]	lr: 4.947e-05, eta: 2:18:48, time: 4.094, data_time: 0.452, memory: 36260, decode.loss_focal: 0.0030, decode.loss_dice: 0.6969, decode.acc_seg: 99.8101, loss: 0.6999
2023-10-31 23:58:10,913 - mmseg - INFO - per class results:
2023-10-31 23:58:10,914 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.94 | 70.43 |  72.8 | 70.39 | 99.96 | 99.96  |   99.94   | 99.98  | 99.94 |
|  scratch   | 83.84 | 86.53 | 74.04 | 76.59 | 72.01 | 83.34  |   85.67   | 81.34  |  75.0 |
|   stain    | 83.61 | 86.76 | 74.29 | 77.81 |  66.5 |  83.0  |   91.73   | 77.67  |  74.5 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-10-31 23:58:10,914 - mmseg - INFO - Summary:
2023-10-31 23:58:10,914 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.94 | 84.44 | 85.93 | 73.71 | 74.93 | 67.95 |  88.77  |   92.45    |  78.63  | 70.69 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-10-31 23:58:10,919 - mmseg - INFO - Exp name: unet_all.py
2023-10-31 23:58:10,919 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9994, mIoU: 0.8444, mVOE: 0.8593, mASD: 0.7371, mMSSD: 0.7493, mAcc: 0.6795, mFscore: 0.8877, mPrecision: 0.9245, mRecall: 0.7863, mDice: 0.7069, IoU.background: 0.9994, IoU.scratch: 0.8384, IoU.stain: 0.8361, IoU.edgeDamage: 0.7037, VOE.background: 0.7043, VOE.scratch: 0.8653, VOE.stain: 0.8676, VOE.edgeDamage: 1.0000, ASD.background: 0.7280, ASD.scratch: 0.7404, ASD.stain: 0.7429, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7659, MSSD.stain: 0.7781, MSSD.edgeDamage: nan, Acc.background: 0.9996, Acc.scratch: 0.7201, Acc.stain: 0.6650, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8334, Fscore.stain: 0.8300, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8567, Precision.stain: 0.9173, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.8134, Recall.stain: 0.7767, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7500, Dice.stain: 0.7450, Dice.edgeDamage: 0.3333
2023-11-01 00:01:30,875 - mmseg - INFO - Iter [3050/5000]	lr: 4.858e-05, eta: 2:15:52, time: 5.179, data_time: 1.555, memory: 36260, decode.loss_focal: 0.0029, decode.loss_dice: 0.6891, decode.acc_seg: 99.8174, loss: 0.6920
2023-11-01 00:04:55,549 - mmseg - INFO - Iter [3100/5000]	lr: 4.769e-05, eta: 2:12:21, time: 4.093, data_time: 0.452, memory: 36260, decode.loss_focal: 0.0029, decode.loss_dice: 0.6883, decode.acc_seg: 99.8143, loss: 0.6913
2023-11-01 00:08:16,378 - mmseg - INFO - Iter [3150/5000]	lr: 4.680e-05, eta: 2:08:47, time: 4.017, data_time: 0.377, memory: 36260, decode.loss_focal: 0.0030, decode.loss_dice: 0.6819, decode.acc_seg: 99.8113, loss: 0.6849
2023-11-01 00:11:40,801 - mmseg - INFO - Iter [3200/5000]	lr: 4.590e-05, eta: 2:05:16, time: 4.088, data_time: 0.446, memory: 36260, decode.loss_focal: 0.0028, decode.loss_dice: 0.6883, decode.acc_seg: 99.8209, loss: 0.6911
2023-11-01 00:15:05,607 - mmseg - INFO - Iter [3250/5000]	lr: 4.500e-05, eta: 2:01:45, time: 4.096, data_time: 0.448, memory: 36260, decode.loss_focal: 0.0030, decode.loss_dice: 0.6811, decode.acc_seg: 99.8140, loss: 0.6841
2023-11-01 00:18:26,222 - mmseg - INFO - Iter [3300/5000]	lr: 4.410e-05, eta: 1:58:12, time: 4.012, data_time: 0.375, memory: 36260, decode.loss_focal: 0.0029, decode.loss_dice: 0.6781, decode.acc_seg: 99.8166, loss: 0.6811
2023-11-01 00:21:50,892 - mmseg - INFO - Iter [3350/5000]	lr: 4.320e-05, eta: 1:54:41, time: 4.093, data_time: 0.450, memory: 36260, decode.loss_focal: 0.0029, decode.loss_dice: 0.6802, decode.acc_seg: 99.8149, loss: 0.6831
2023-11-01 00:25:11,901 - mmseg - INFO - Iter [3400/5000]	lr: 4.229e-05, eta: 1:51:09, time: 4.020, data_time: 0.380, memory: 36260, decode.loss_focal: 0.0029, decode.loss_dice: 0.6764, decode.acc_seg: 99.8146, loss: 0.6793
2023-11-01 00:28:36,542 - mmseg - INFO - Iter [3450/5000]	lr: 4.138e-05, eta: 1:47:39, time: 4.093, data_time: 0.450, memory: 36260, decode.loss_focal: 0.0028, decode.loss_dice: 0.6766, decode.acc_seg: 99.8203, loss: 0.6794
2023-11-01 00:31:57,817 - mmseg - INFO - Iter [3500/5000]	lr: 4.047e-05, eta: 1:44:08, time: 4.026, data_time: 0.380, memory: 36260, decode.loss_focal: 0.0029, decode.loss_dice: 0.6738, decode.acc_seg: 99.8177, loss: 0.6767
2023-11-01 00:32:56,443 - mmseg - INFO - per class results:
2023-11-01 00:32:56,445 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.95 | 70.42 |  72.8 | 70.39 | 99.97 | 99.96  |   99.94   | 99.98  | 99.94 |
|  scratch   | 83.72 | 86.65 | 74.13 | 77.06 | 69.92 | 83.17  |   87.36   | 79.95  | 74.75 |
|   stain    | 82.99 | 87.38 | 74.35 | 78.13 | 65.06 |  82.1  |   91.19   | 76.71  | 73.16 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 00:32:56,445 - mmseg - INFO - Summary:
2023-11-01 00:32:56,445 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.95 | 84.26 | 86.11 | 73.76 | 75.19 | 67.07 |  88.41  |   92.83    |  78.05  | 70.29 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 00:32:56,451 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9995, mIoU: 0.8426, mVOE: 0.8611, mASD: 0.7376, mMSSD: 0.7519, mAcc: 0.6707, mFscore: 0.8841, mPrecision: 0.9283, mRecall: 0.7805, mDice: 0.7029, IoU.background: 0.9995, IoU.scratch: 0.8372, IoU.stain: 0.8299, IoU.edgeDamage: 0.7037, VOE.background: 0.7042, VOE.scratch: 0.8665, VOE.stain: 0.8738, VOE.edgeDamage: 1.0000, ASD.background: 0.7280, ASD.scratch: 0.7413, ASD.stain: 0.7435, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7706, MSSD.stain: 0.7813, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.6992, Acc.stain: 0.6506, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8317, Fscore.stain: 0.8210, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8736, Precision.stain: 0.9119, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.7995, Recall.stain: 0.7671, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7475, Dice.stain: 0.7316, Dice.edgeDamage: 0.3333
2023-11-01 00:36:20,437 - mmseg - INFO - Iter [3550/5000]	lr: 3.956e-05, eta: 1:41:02, time: 5.252, data_time: 1.623, memory: 36260, decode.loss_focal: 0.0026, decode.loss_dice: 0.6665, decode.acc_seg: 99.8333, loss: 0.6691
2023-11-01 00:39:41,264 - mmseg - INFO - Iter [3600/5000]	lr: 3.864e-05, eta: 1:37:29, time: 4.017, data_time: 0.375, memory: 36260, decode.loss_focal: 0.0027, decode.loss_dice: 0.6707, decode.acc_seg: 99.8247, loss: 0.6734
2023-11-01 00:43:05,641 - mmseg - INFO - Iter [3650/5000]	lr: 3.772e-05, eta: 1:33:59, time: 4.088, data_time: 0.449, memory: 36260, decode.loss_focal: 0.0028, decode.loss_dice: 0.6711, decode.acc_seg: 99.8206, loss: 0.6738
2023-11-01 00:46:29,904 - mmseg - INFO - Iter [3700/5000]	lr: 3.679e-05, eta: 1:30:28, time: 4.085, data_time: 0.447, memory: 36260, decode.loss_focal: 0.0028, decode.loss_dice: 0.6668, decode.acc_seg: 99.8227, loss: 0.6696
2023-11-01 00:49:50,681 - mmseg - INFO - Iter [3750/5000]	lr: 3.586e-05, eta: 1:26:57, time: 4.016, data_time: 0.373, memory: 36260, decode.loss_focal: 0.0028, decode.loss_dice: 0.6638, decode.acc_seg: 99.8240, loss: 0.6666
2023-11-01 00:53:15,394 - mmseg - INFO - Iter [3800/5000]	lr: 3.493e-05, eta: 1:23:27, time: 4.094, data_time: 0.453, memory: 36260, decode.loss_focal: 0.0027, decode.loss_dice: 0.6641, decode.acc_seg: 99.8277, loss: 0.6668
2023-11-01 00:56:36,173 - mmseg - INFO - Iter [3850/5000]	lr: 3.400e-05, eta: 1:19:56, time: 4.015, data_time: 0.377, memory: 36260, decode.loss_focal: 0.0027, decode.loss_dice: 0.6672, decode.acc_seg: 99.8242, loss: 0.6699
2023-11-01 01:00:00,816 - mmseg - INFO - Iter [3900/5000]	lr: 3.306e-05, eta: 1:16:26, time: 4.093, data_time: 0.459, memory: 36260, decode.loss_focal: 0.0026, decode.loss_dice: 0.6677, decode.acc_seg: 99.8277, loss: 0.6703
2023-11-01 01:03:21,841 - mmseg - INFO - Iter [3950/5000]	lr: 3.211e-05, eta: 1:12:56, time: 4.020, data_time: 0.375, memory: 36260, decode.loss_focal: 0.0027, decode.loss_dice: 0.6618, decode.acc_seg: 99.8292, loss: 0.6645
2023-11-01 01:06:46,302 - mmseg - INFO - Exp name: unet_all.py
2023-11-01 01:06:46,302 - mmseg - INFO - Iter [4000/5000]	lr: 3.116e-05, eta: 1:09:26, time: 4.089, data_time: 0.451, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6613, decode.acc_seg: 99.8354, loss: 0.6638
2023-11-01 01:07:46,137 - mmseg - INFO - per class results:
2023-11-01 01:07:46,138 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.95 | 70.42 |  72.8 | 70.39 | 99.97 | 99.96  |   99.94   | 99.98  | 99.94 |
|  scratch   | 83.96 | 86.41 | 74.06 | 76.66 | 71.68 | 83.51  |   86.39   | 81.12  | 75.26 |
|   stain    | 83.15 | 87.22 | 74.35 | 78.13 | 65.07 | 82.35  |   92.08   | 76.71  | 73.52 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 01:07:46,138 - mmseg - INFO - Summary:
2023-11-01 01:07:46,139 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.95 | 84.36 | 86.01 | 73.73 | 75.06 | 67.51 |  88.61  |    92.8    |  78.34  | 70.51 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 01:07:46,144 - mmseg - INFO - Exp name: unet_all.py
2023-11-01 01:07:46,145 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9995, mIoU: 0.8436, mVOE: 0.8601, mASD: 0.7373, mMSSD: 0.7506, mAcc: 0.6751, mFscore: 0.8861, mPrecision: 0.9280, mRecall: 0.7834, mDice: 0.7051, IoU.background: 0.9995, IoU.scratch: 0.8396, IoU.stain: 0.8315, IoU.edgeDamage: 0.7037, VOE.background: 0.7042, VOE.scratch: 0.8641, VOE.stain: 0.8722, VOE.edgeDamage: 1.0000, ASD.background: 0.7280, ASD.scratch: 0.7406, ASD.stain: 0.7435, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7666, MSSD.stain: 0.7813, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.7168, Acc.stain: 0.6507, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8351, Fscore.stain: 0.8235, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8639, Precision.stain: 0.9208, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.8112, Recall.stain: 0.7671, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7526, Dice.stain: 0.7352, Dice.edgeDamage: 0.3333
2023-11-01 01:11:05,926 - mmseg - INFO - Iter [4050/5000]	lr: 3.021e-05, eta: 1:06:10, time: 5.192, data_time: 1.569, memory: 36260, decode.loss_focal: 0.0026, decode.loss_dice: 0.6581, decode.acc_seg: 99.8323, loss: 0.6607
2023-11-01 01:14:30,264 - mmseg - INFO - Iter [4100/5000]	lr: 2.925e-05, eta: 1:02:40, time: 4.087, data_time: 0.448, memory: 36260, decode.loss_focal: 0.0026, decode.loss_dice: 0.6607, decode.acc_seg: 99.8293, loss: 0.6633
2023-11-01 01:17:54,866 - mmseg - INFO - Iter [4150/5000]	lr: 2.829e-05, eta: 0:59:10, time: 4.092, data_time: 0.445, memory: 36260, decode.loss_focal: 0.0027, decode.loss_dice: 0.6573, decode.acc_seg: 99.8227, loss: 0.6600
2023-11-01 01:21:15,558 - mmseg - INFO - Iter [4200/5000]	lr: 2.732e-05, eta: 0:55:40, time: 4.014, data_time: 0.373, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6494, decode.acc_seg: 99.8332, loss: 0.6519
2023-11-01 01:24:39,814 - mmseg - INFO - Iter [4250/5000]	lr: 2.634e-05, eta: 0:52:10, time: 4.085, data_time: 0.447, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6581, decode.acc_seg: 99.8360, loss: 0.6605
2023-11-01 01:28:00,462 - mmseg - INFO - Iter [4300/5000]	lr: 2.536e-05, eta: 0:48:40, time: 4.013, data_time: 0.374, memory: 36260, decode.loss_focal: 0.0026, decode.loss_dice: 0.6526, decode.acc_seg: 99.8296, loss: 0.6552
2023-11-01 01:31:24,876 - mmseg - INFO - Iter [4350/5000]	lr: 2.437e-05, eta: 0:45:11, time: 4.088, data_time: 0.449, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6496, decode.acc_seg: 99.8349, loss: 0.6521
2023-11-01 01:34:45,696 - mmseg - INFO - Iter [4400/5000]	lr: 2.337e-05, eta: 0:41:41, time: 4.016, data_time: 0.375, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6551, decode.acc_seg: 99.8362, loss: 0.6576
2023-11-01 01:38:10,100 - mmseg - INFO - Iter [4450/5000]	lr: 2.237e-05, eta: 0:38:12, time: 4.088, data_time: 0.443, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6507, decode.acc_seg: 99.8375, loss: 0.6531
2023-11-01 01:41:30,933 - mmseg - INFO - Iter [4500/5000]	lr: 2.135e-05, eta: 0:34:43, time: 4.017, data_time: 0.376, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6486, decode.acc_seg: 99.8317, loss: 0.6512
2023-11-01 01:42:30,156 - mmseg - INFO - per class results:
2023-11-01 01:42:30,157 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.95 | 70.42 |  72.8 | 70.39 | 99.97 | 99.96  |   99.94   | 99.98  | 99.94 |
|  scratch   |  84.2 | 86.17 | 74.07 | 76.74 | 71.34 | 83.84  |   87.57   |  80.9  | 75.77 |
|   stain    |  83.0 | 87.37 | 74.38 | 78.29 | 64.37 | 82.13  |   92.69   | 76.24  | 73.19 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 01:42:30,157 - mmseg - INFO - Summary:
2023-11-01 01:42:30,157 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.95 | 84.38 | 85.99 | 73.75 | 75.14 | 67.25 |  88.64  |    93.4    |  78.17  | 70.56 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 01:42:30,162 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9995, mIoU: 0.8438, mVOE: 0.8599, mASD: 0.7375, mMSSD: 0.7514, mAcc: 0.6725, mFscore: 0.8864, mPrecision: 0.9340, mRecall: 0.7817, mDice: 0.7056, IoU.background: 0.9995, IoU.scratch: 0.8420, IoU.stain: 0.8300, IoU.edgeDamage: 0.7037, VOE.background: 0.7042, VOE.scratch: 0.8617, VOE.stain: 0.8737, VOE.edgeDamage: 1.0000, ASD.background: 0.7280, ASD.scratch: 0.7407, ASD.stain: 0.7438, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7674, MSSD.stain: 0.7829, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.7134, Acc.stain: 0.6437, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8384, Fscore.stain: 0.8213, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8757, Precision.stain: 0.9269, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.8090, Recall.stain: 0.7624, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7577, Dice.stain: 0.7319, Dice.edgeDamage: 0.3333
2023-11-01 01:45:53,576 - mmseg - INFO - Iter [4550/5000]	lr: 2.033e-05, eta: 0:31:20, time: 5.253, data_time: 1.629, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6480, decode.acc_seg: 99.8339, loss: 0.6505
2023-11-01 01:49:17,985 - mmseg - INFO - Iter [4600/5000]	lr: 1.929e-05, eta: 0:27:51, time: 4.088, data_time: 0.448, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6471, decode.acc_seg: 99.8374, loss: 0.6495
2023-11-01 01:52:38,898 - mmseg - INFO - Iter [4650/5000]	lr: 1.824e-05, eta: 0:24:21, time: 4.018, data_time: 0.376, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6455, decode.acc_seg: 99.8363, loss: 0.6480
2023-11-01 01:56:03,471 - mmseg - INFO - Iter [4700/5000]	lr: 1.718e-05, eta: 0:20:52, time: 4.091, data_time: 0.446, memory: 36260, decode.loss_focal: 0.0024, decode.loss_dice: 0.6450, decode.acc_seg: 99.8396, loss: 0.6474
2023-11-01 01:59:24,464 - mmseg - INFO - Iter [4750/5000]	lr: 1.609e-05, eta: 0:17:23, time: 4.020, data_time: 0.382, memory: 36260, decode.loss_focal: 0.0025, decode.loss_dice: 0.6459, decode.acc_seg: 99.8366, loss: 0.6484
2023-11-01 02:02:48,701 - mmseg - INFO - Iter [4800/5000]	lr: 1.499e-05, eta: 0:13:54, time: 4.085, data_time: 0.449, memory: 36260, decode.loss_focal: 0.0024, decode.loss_dice: 0.6434, decode.acc_seg: 99.8403, loss: 0.6458
2023-11-01 02:06:09,622 - mmseg - INFO - Iter [4850/5000]	lr: 1.386e-05, eta: 0:10:25, time: 4.018, data_time: 0.371, memory: 36260, decode.loss_focal: 0.0024, decode.loss_dice: 0.6428, decode.acc_seg: 99.8391, loss: 0.6452
2023-11-01 02:09:34,272 - mmseg - INFO - Iter [4900/5000]	lr: 1.269e-05, eta: 0:06:57, time: 4.093, data_time: 0.439, memory: 36260, decode.loss_focal: 0.0024, decode.loss_dice: 0.6430, decode.acc_seg: 99.8431, loss: 0.6453
2023-11-01 02:12:55,953 - mmseg - INFO - Iter [4950/5000]	lr: 1.145e-05, eta: 0:03:28, time: 4.034, data_time: 0.367, memory: 36260, decode.loss_focal: 0.0024, decode.loss_dice: 0.6409, decode.acc_seg: 99.8387, loss: 0.6433
2023-11-01 02:16:20,524 - mmseg - INFO - Saving checkpoint at 5000 iterations
2023-11-01 02:16:21,185 - mmseg - INFO - Exp name: unet_all.py
2023-11-01 02:16:21,185 - mmseg - INFO - Iter [5000/5000]	lr: 1.004e-05, eta: 0:00:00, time: 4.105, data_time: 0.445, memory: 36260, decode.loss_focal: 0.0024, decode.loss_dice: 0.6408, decode.acc_seg: 99.8411, loss: 0.6432
2023-11-01 02:17:20,423 - mmseg - INFO - per class results:
2023-11-01 02:17:20,424 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.95 | 70.42 |  72.8 | 70.39 | 99.97 | 99.96  |   99.94   | 99.98  | 99.94 |
|  scratch   | 84.12 | 86.25 | 74.07 | 76.75 | 71.31 | 83.73  |   87.31   | 80.87  | 75.59 |
|   stain    | 83.18 | 87.19 | 74.34 | 78.11 | 65.18 | 82.38  |   91.99   | 76.79  | 73.58 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 02:17:20,424 - mmseg - INFO - Summary:
2023-11-01 02:17:20,424 - mmseg - INFO - 
+-------+------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc | mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.95 | 84.4 | 85.97 | 73.74 | 75.08 | 67.45 |  88.69  |   93.08    |   78.3  | 70.61 |
+-------+------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 02:17:20,428 - mmseg - INFO - Exp name: unet_all.py
2023-11-01 02:17:20,428 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9995, mIoU: 0.8440, mVOE: 0.8597, mASD: 0.7374, mMSSD: 0.7508, mAcc: 0.6745, mFscore: 0.8869, mPrecision: 0.9308, mRecall: 0.7830, mDice: 0.7061, IoU.background: 0.9995, IoU.scratch: 0.8412, IoU.stain: 0.8318, IoU.edgeDamage: 0.7037, VOE.background: 0.7042, VOE.scratch: 0.8625, VOE.stain: 0.8719, VOE.edgeDamage: 1.0000, ASD.background: 0.7280, ASD.scratch: 0.7407, ASD.stain: 0.7434, ASD.edgeDamage: nan, MSSD.background: 0.7039, MSSD.scratch: 0.7675, MSSD.stain: 0.7811, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.7131, Acc.stain: 0.6518, Acc.edgeDamage: 0.3333, Fscore.background: 0.9996, Fscore.scratch: 0.8373, Fscore.stain: 0.8238, Fscore.edgeDamage: nan, Precision.background: 0.9994, Precision.scratch: 0.8731, Precision.stain: 0.9199, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.8087, Recall.stain: 0.7679, Recall.edgeDamage: 0.5556, Dice.background: 0.9994, Dice.scratch: 0.7559, Dice.stain: 0.7358, Dice.edgeDamage: 0.3333
