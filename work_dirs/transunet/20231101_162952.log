2023-11-01 16:29:52,960 - mmseg - INFO - Multi-processing start method is `None`
2023-11-01 16:29:52,960 - mmseg - INFO - OpenCV num_threads is `112
2023-11-01 16:29:52,960 - mmseg - INFO - OMP num threads is 1
2023-11-01 16:29:53,119 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3: Quadro RTX 8000
CUDA_HOME: /usr/local/cuda-10.1
NVCC: Cuda compilation tools, release 10.1, V10.1.10
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.10.0+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.1+cu102
OpenCV: 4.7.0
MMCV: 1.7.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMSegmentation: 0.29.1+
------------------------------------------------------------

2023-11-01 16:29:53,119 - mmseg - INFO - Distributed training: True
2023-11-01 16:29:53,477 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
backbone_norm_cfg = dict(type='LN', requires_grad=True)
model = dict(
    type='EncoderDecoderFull',
    pretrained=None,
    decode_head=dict(
        type='TransUNet',
        img_dim=512,
        in_channels=3,
        out_channels=128,
        head_num=4,
        mlp_dim=512,
        block_num=8,
        patch_dim=16,
        class_num=4,
        loss_decode=[
            dict(
                type='FocalLoss',
                use_sigmoid=True,
                gamma=2.0,
                alpha=0.25,
                loss_weight=2.0),
            dict(type='DiceLoss', loss_name='loss_dice', loss_weight=2.0)
        ]))
train_cfg = dict()
test_cfg = dict(mode='whole')
dataset_type = 'MyDataset'
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(600, 600)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(512, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=False),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data_root = './datasets/'
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=2,
    train=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='train/images',
        ann_dir='train/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(600, 600)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TensorboardLoggerHook'),
        dict(type='TextLoggerHook', by_epoch=False)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = False
find_unused_parameters = True
optimizer = dict(type='Adam', lr=0.0001, betas=(0.9, 0.999))
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=1e-05, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=5000)
checkpoint_config = dict(by_epoch=False, save_optimizer=False, interval=5000)
evaluation = dict(interval=500, metric=['mIoU', 'mFscore', 'mDice'])
work_dir = './work_dirs/transunet'
gpu_ids = range(0, 4)
auto_resume = False

2023-11-01 16:29:53,477 - mmseg - INFO - Set random seed to 0, deterministic: False
2023-11-01 16:29:54,030 - mmseg - INFO - initialize TransUNet with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

decode_head.conv_seg.weight - torch.Size([2, 64, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([2]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.encoder.conv1.weight - torch.Size([128, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.norm1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.norm1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.downsample.0.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.conv1.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.norm2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.norm2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.conv3.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.norm3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder1.norm3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.conv1.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.norm1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.norm1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.conv3.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.norm3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder2.norm3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.conv1.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.conv2.weight - torch.Size([1024, 1024, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.conv3.weight - torch.Size([1024, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.norm3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.encoder3.norm3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.embedding - torch.Size([1025, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.cls_token - torch.Size([1, 1, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.projection.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.projection.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.multi_head_attention.qkv_layer.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.multi_head_attention.out_attention.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.mlp.mlp_layers.0.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.mlp.mlp_layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.mlp.mlp_layers.3.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.mlp.mlp_layers.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.layer_norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.layer_norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.layer_norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.0.layer_norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.multi_head_attention.qkv_layer.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.multi_head_attention.out_attention.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.mlp.mlp_layers.0.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.mlp.mlp_layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.mlp.mlp_layers.3.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.mlp.mlp_layers.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.layer_norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.layer_norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.layer_norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.1.layer_norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.multi_head_attention.qkv_layer.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.multi_head_attention.out_attention.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.mlp.mlp_layers.0.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.mlp.mlp_layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.mlp.mlp_layers.3.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.mlp.mlp_layers.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.layer_norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.layer_norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.layer_norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.2.layer_norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.multi_head_attention.qkv_layer.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.multi_head_attention.out_attention.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.mlp.mlp_layers.0.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.mlp.mlp_layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.mlp.mlp_layers.3.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.mlp.mlp_layers.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.layer_norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.layer_norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.layer_norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.3.layer_norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.multi_head_attention.qkv_layer.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.multi_head_attention.out_attention.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.mlp.mlp_layers.0.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.mlp.mlp_layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.mlp.mlp_layers.3.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.mlp.mlp_layers.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.layer_norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.layer_norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.layer_norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.4.layer_norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.multi_head_attention.qkv_layer.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.multi_head_attention.out_attention.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.mlp.mlp_layers.0.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.mlp.mlp_layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.mlp.mlp_layers.3.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.mlp.mlp_layers.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.layer_norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.layer_norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.layer_norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.5.layer_norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.multi_head_attention.qkv_layer.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.multi_head_attention.out_attention.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.mlp.mlp_layers.0.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.mlp.mlp_layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.mlp.mlp_layers.3.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.mlp.mlp_layers.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.layer_norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.layer_norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.layer_norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.6.layer_norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.multi_head_attention.qkv_layer.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.multi_head_attention.out_attention.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.mlp.mlp_layers.0.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.mlp.mlp_layers.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.mlp.mlp_layers.3.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.mlp.mlp_layers.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.layer_norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.layer_norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.layer_norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.vit.transformer.layer_blocks.7.layer_norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.conv2.weight - torch.Size([512, 1024, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.conv2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.encoder.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder1.layer.0.weight - torch.Size([256, 1024, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder1.layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder1.layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder1.layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder1.layer.3.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder1.layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder1.layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder1.layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder2.layer.0.weight - torch.Size([128, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder2.layer.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder2.layer.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder2.layer.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder2.layer.3.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder2.layer.3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder2.layer.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder2.layer.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder3.layer.0.weight - torch.Size([64, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder3.layer.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder3.layer.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder3.layer.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder3.layer.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder3.layer.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder3.layer.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder3.layer.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder4.layer.0.weight - torch.Size([16, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder4.layer.0.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder4.layer.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder4.layer.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder4.layer.3.weight - torch.Size([16, 16, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder4.layer.3.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder4.layer.4.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.decoder4.layer.4.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.conv1.weight - torch.Size([4, 16, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoder.conv1.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  
2023-11-01 16:29:54,094 - mmseg - INFO - EncoderDecoderFull(
  (decode_head): TransUNet(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): ModuleList(
      (0): FocalLoss()
      (1): DiceLoss()
    )
    (conv_seg): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (encoder): Encoder(
      (conv1): Conv2d(3, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (encoder1): EncoderBottleneck(
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (encoder2): EncoderBottleneck(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (encoder3): EncoderBottleneck(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (vit): ViT(
        (projection): Linear(in_features=1024, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (transformer): TransformerEncoder(
          (layer_blocks): ModuleList(
            (0): TransformerEncoderBlock(
              (multi_head_attention): MultiHeadAttention(
                (qkv_layer): Linear(in_features=1024, out_features=3072, bias=False)
                (out_attention): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (mlp): MLP(
                (mlp_layers): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=1024, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): TransformerEncoderBlock(
              (multi_head_attention): MultiHeadAttention(
                (qkv_layer): Linear(in_features=1024, out_features=3072, bias=False)
                (out_attention): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (mlp): MLP(
                (mlp_layers): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=1024, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): TransformerEncoderBlock(
              (multi_head_attention): MultiHeadAttention(
                (qkv_layer): Linear(in_features=1024, out_features=3072, bias=False)
                (out_attention): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (mlp): MLP(
                (mlp_layers): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=1024, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (3): TransformerEncoderBlock(
              (multi_head_attention): MultiHeadAttention(
                (qkv_layer): Linear(in_features=1024, out_features=3072, bias=False)
                (out_attention): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (mlp): MLP(
                (mlp_layers): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=1024, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (4): TransformerEncoderBlock(
              (multi_head_attention): MultiHeadAttention(
                (qkv_layer): Linear(in_features=1024, out_features=3072, bias=False)
                (out_attention): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (mlp): MLP(
                (mlp_layers): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=1024, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (5): TransformerEncoderBlock(
              (multi_head_attention): MultiHeadAttention(
                (qkv_layer): Linear(in_features=1024, out_features=3072, bias=False)
                (out_attention): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (mlp): MLP(
                (mlp_layers): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=1024, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (6): TransformerEncoderBlock(
              (multi_head_attention): MultiHeadAttention(
                (qkv_layer): Linear(in_features=1024, out_features=3072, bias=False)
                (out_attention): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (mlp): MLP(
                (mlp_layers): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=1024, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (7): TransformerEncoderBlock(
              (multi_head_attention): MultiHeadAttention(
                (qkv_layer): Linear(in_features=1024, out_features=3072, bias=False)
                (out_attention): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (mlp): MLP(
                (mlp_layers): Sequential(
                  (0): Linear(in_features=1024, out_features=512, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=512, out_features=1024, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (conv2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (decoder): Decoder(
      (decoder1): DecoderBottleneck(
        (upsample): Upsample(scale_factor=2.0, mode=bilinear)
        (layer): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
      (decoder2): DecoderBottleneck(
        (upsample): Upsample(scale_factor=2.0, mode=bilinear)
        (layer): Sequential(
          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
      (decoder3): DecoderBottleneck(
        (upsample): Upsample(scale_factor=2.0, mode=bilinear)
        (layer): Sequential(
          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
      (decoder4): DecoderBottleneck(
        (upsample): Upsample(scale_factor=2.0, mode=bilinear)
        (layer): Sequential(
          (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
      (conv1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2023-11-01 16:29:54,101 - mmseg - INFO - Loaded 308 images
2023-11-01 16:29:59,552 - mmseg - INFO - Loaded 78 images
2023-11-01 16:29:59,574 - mmseg - INFO - Start running, host: zhangzifan@s2, work_dir: /data2/zhangzifan/code_dir/2023-10-25-01/work_dirs/transunet
2023-11-01 16:29:59,575 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-11-01 16:29:59,575 - mmseg - INFO - workflow: [('train', 1)], max: 5000 iters
2023-11-01 16:29:59,575 - mmseg - INFO - Checkpoints will be saved to /data2/zhangzifan/code_dir/2023-10-25-01/work_dirs/transunet by HardDiskBackend.
2023-11-01 16:31:12,337 - mmseg - INFO - Iter [50/5000]	lr: 9.921e-05, eta: 1:56:15, time: 1.409, data_time: 0.510, memory: 13756, decode.loss_focal: 0.2265, decode.loss_dice: 1.7830, decode.acc_seg: 22.1474, loss: 2.0095
2023-11-01 16:32:20,424 - mmseg - INFO - Iter [100/5000]	lr: 9.839e-05, eta: 1:53:09, time: 1.362, data_time: 0.455, memory: 13756, decode.loss_focal: 0.1886, decode.loss_dice: 1.7626, decode.acc_seg: 51.7429, loss: 1.9511
2023-11-01 16:33:24,869 - mmseg - INFO - Iter [150/5000]	lr: 9.758e-05, eta: 1:49:23, time: 1.289, data_time: 0.378, memory: 13756, decode.loss_focal: 0.1777, decode.loss_dice: 1.7535, decode.acc_seg: 51.4776, loss: 1.9311
2023-11-01 16:34:33,178 - mmseg - INFO - Iter [200/5000]	lr: 9.677e-05, eta: 1:48:31, time: 1.366, data_time: 0.453, memory: 13756, decode.loss_focal: 0.1724, decode.loss_dice: 1.7393, decode.acc_seg: 51.3949, loss: 1.9118
2023-11-01 16:35:37,658 - mmseg - INFO - Iter [250/5000]	lr: 9.596e-05, eta: 1:46:20, time: 1.290, data_time: 0.369, memory: 13756, decode.loss_focal: 0.1648, decode.loss_dice: 1.7265, decode.acc_seg: 51.7798, loss: 1.8913
2023-11-01 16:36:46,069 - mmseg - INFO - Iter [300/5000]	lr: 9.514e-05, eta: 1:45:32, time: 1.368, data_time: 0.453, memory: 13756, decode.loss_focal: 0.1565, decode.loss_dice: 1.7156, decode.acc_seg: 52.4933, loss: 1.8721
2023-11-01 16:37:50,637 - mmseg - INFO - Iter [350/5000]	lr: 9.433e-05, eta: 1:43:47, time: 1.291, data_time: 0.371, memory: 13756, decode.loss_focal: 0.1483, decode.loss_dice: 1.7049, decode.acc_seg: 53.1424, loss: 1.8533
2023-11-01 16:38:59,248 - mmseg - INFO - Iter [400/5000]	lr: 9.351e-05, eta: 1:42:59, time: 1.372, data_time: 0.449, memory: 13756, decode.loss_focal: 0.1405, decode.loss_dice: 1.6950, decode.acc_seg: 53.5298, loss: 1.8355
2023-11-01 16:40:04,077 - mmseg - INFO - Iter [450/5000]	lr: 9.269e-05, eta: 1:41:28, time: 1.297, data_time: 0.373, memory: 13756, decode.loss_focal: 0.1330, decode.loss_dice: 1.6850, decode.acc_seg: 54.1462, loss: 1.8180
2023-11-01 16:41:12,398 - mmseg - INFO - Iter [500/5000]	lr: 9.187e-05, eta: 1:40:34, time: 1.366, data_time: 0.446, memory: 13756, decode.loss_focal: 0.1260, decode.loss_dice: 1.6748, decode.acc_seg: 55.0991, loss: 1.8008
2023-11-01 16:42:07,043 - mmseg - INFO - per class results:
2023-11-01 16:42:07,045 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background |  88.2 | 82.17 | 74.09 | 76.27 | 73.46 | 88.95  |   99.98   | 82.31  | 83.42 |
|  scratch   | 71.61 | 98.76 | 75.73 | 84.46 | 48.69 | 59.13  |   57.72   | 65.79  |  38.7 |
|   stain    |  70.4 | 99.97 | 75.87 | 85.17 | 94.07 | 55.64  |    55.6   | 96.04  | 33.46 |
| edgeDamage | 70.37 | 100.0 | 75.87 | 85.19 | 33.33 |  nan   |   55.56   | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 16:42:07,046 - mmseg - INFO - Summary:
2023-11-01 16:42:07,046 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 88.18 | 75.14 | 95.23 | 75.39 | 82.77 | 62.39 |  67.91  |   67.21    |  74.92  | 47.23 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 16:42:07,051 - mmseg - INFO - Iter(val) [20]	aAcc: 0.8818, mIoU: 0.7514, mVOE: 0.9523, mASD: 0.7539, mMSSD: 0.8277, mAcc: 0.6239, mFscore: 0.6791, mPrecision: 0.6721, mRecall: 0.7492, mDice: 0.4723, IoU.background: 0.8820, IoU.scratch: 0.7161, IoU.stain: 0.7040, IoU.edgeDamage: 0.7037, VOE.background: 0.8217, VOE.scratch: 0.9876, VOE.stain: 0.9997, VOE.edgeDamage: 1.0000, ASD.background: 0.7409, ASD.scratch: 0.7573, ASD.stain: 0.7587, ASD.edgeDamage: 0.7587, MSSD.background: 0.7627, MSSD.scratch: 0.8446, MSSD.stain: 0.8517, MSSD.edgeDamage: 0.8519, Acc.background: 0.7346, Acc.scratch: 0.4869, Acc.stain: 0.9407, Acc.edgeDamage: 0.3333, Fscore.background: 0.8895, Fscore.scratch: 0.5913, Fscore.stain: 0.5564, Fscore.edgeDamage: nan, Precision.background: 0.9998, Precision.scratch: 0.5772, Precision.stain: 0.5560, Precision.edgeDamage: 0.5556, Recall.background: 0.8231, Recall.scratch: 0.6579, Recall.stain: 0.9604, Recall.edgeDamage: 0.5556, Dice.background: 0.8342, Dice.scratch: 0.3870, Dice.stain: 0.3346, Dice.edgeDamage: 0.3333
2023-11-01 16:43:15,205 - mmseg - INFO - Iter [550/5000]	lr: 9.106e-05, eta: 1:46:58, time: 2.456, data_time: 1.541, memory: 13756, decode.loss_focal: 0.1199, decode.loss_dice: 1.6642, decode.acc_seg: 66.8642, loss: 1.7841
2023-11-01 16:44:20,061 - mmseg - INFO - Iter [600/5000]	lr: 9.024e-05, eta: 1:44:53, time: 1.297, data_time: 0.376, memory: 13756, decode.loss_focal: 0.1138, decode.loss_dice: 1.6521, decode.acc_seg: 94.1227, loss: 1.7658
2023-11-01 16:45:28,539 - mmseg - INFO - Iter [650/5000]	lr: 8.941e-05, eta: 1:43:21, time: 1.370, data_time: 0.445, memory: 13756, decode.loss_focal: 0.1079, decode.loss_dice: 1.6403, decode.acc_seg: 97.6695, loss: 1.7482
2023-11-01 16:46:33,007 - mmseg - INFO - Iter [700/5000]	lr: 8.859e-05, eta: 1:41:28, time: 1.289, data_time: 0.368, memory: 13756, decode.loss_focal: 0.1022, decode.loss_dice: 1.6300, decode.acc_seg: 98.4416, loss: 1.7322
2023-11-01 16:47:41,112 - mmseg - INFO - Iter [750/5000]	lr: 8.777e-05, eta: 1:40:02, time: 1.362, data_time: 0.444, memory: 13756, decode.loss_focal: 0.0970, decode.loss_dice: 1.6190, decode.acc_seg: 98.7339, loss: 1.7160
2023-11-01 16:48:45,656 - mmseg - INFO - Iter [800/5000]	lr: 8.695e-05, eta: 1:38:19, time: 1.291, data_time: 0.375, memory: 13756, decode.loss_focal: 0.0921, decode.loss_dice: 1.6071, decode.acc_seg: 99.0129, loss: 1.6992
2023-11-01 16:49:53,882 - mmseg - INFO - Iter [850/5000]	lr: 8.612e-05, eta: 1:36:59, time: 1.364, data_time: 0.442, memory: 13756, decode.loss_focal: 0.0872, decode.loss_dice: 1.5965, decode.acc_seg: 99.1084, loss: 1.6837
2023-11-01 16:50:58,203 - mmseg - INFO - Iter [900/5000]	lr: 8.530e-05, eta: 1:35:23, time: 1.286, data_time: 0.370, memory: 13756, decode.loss_focal: 0.0827, decode.loss_dice: 1.5865, decode.acc_seg: 99.1426, loss: 1.6692
2023-11-01 16:52:06,343 - mmseg - INFO - Iter [950/5000]	lr: 8.447e-05, eta: 1:34:06, time: 1.363, data_time: 0.450, memory: 13756, decode.loss_focal: 0.0783, decode.loss_dice: 1.5768, decode.acc_seg: 99.1929, loss: 1.6551
2023-11-01 16:53:14,589 - mmseg - INFO - Exp name: transunet.py
2023-11-01 16:53:14,589 - mmseg - INFO - Iter [1000/5000]	lr: 8.364e-05, eta: 1:32:50, time: 1.365, data_time: 0.446, memory: 13756, decode.loss_focal: 0.0743, decode.loss_dice: 1.5669, decode.acc_seg: 99.2601, loss: 1.6412
2023-11-01 16:54:09,426 - mmseg - INFO - per class results:
2023-11-01 16:54:09,427 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.09 | 71.29 | 72.99 |  70.8 | 98.05 |  99.3  |   99.93   |  98.7  | 98.95 |
|  scratch   | 76.63 | 93.74 | 74.87 | 80.17 | 57.34 | 71.06  |    70.6   | 71.56  | 56.59 |
|   stain    | 71.36 | 99.01 | 75.73 | 84.47 | 39.96 | 58.44  |    57.7   | 59.97  | 37.66 |
| edgeDamage | 70.64 | 99.73 | 75.84 | 85.05 | 80.96 | 56.37  |   55.97   | 87.31  | 34.56 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 16:54:09,427 - mmseg - INFO - Summary:
2023-11-01 16:54:09,427 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.09 | 79.43 | 90.94 | 74.86 | 80.12 | 69.08 |  71.29  |   71.05    |  79.38  | 56.94 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 16:54:09,432 - mmseg - INFO - Exp name: transunet.py
2023-11-01 16:54:09,432 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9909, mIoU: 0.7943, mVOE: 0.9094, mASD: 0.7486, mMSSD: 0.8012, mAcc: 0.6908, mFscore: 0.7129, mPrecision: 0.7105, mRecall: 0.7938, mDice: 0.5694, IoU.background: 0.9909, IoU.scratch: 0.7663, IoU.stain: 0.7136, IoU.edgeDamage: 0.7064, VOE.background: 0.7129, VOE.scratch: 0.9374, VOE.stain: 0.9901, VOE.edgeDamage: 0.9973, ASD.background: 0.7299, ASD.scratch: 0.7487, ASD.stain: 0.7573, ASD.edgeDamage: 0.7584, MSSD.background: 0.7080, MSSD.scratch: 0.8017, MSSD.stain: 0.8447, MSSD.edgeDamage: 0.8505, Acc.background: 0.9805, Acc.scratch: 0.5734, Acc.stain: 0.3996, Acc.edgeDamage: 0.8096, Fscore.background: 0.9930, Fscore.scratch: 0.7106, Fscore.stain: 0.5844, Fscore.edgeDamage: 0.5637, Precision.background: 0.9993, Precision.scratch: 0.7060, Precision.stain: 0.5770, Precision.edgeDamage: 0.5597, Recall.background: 0.9870, Recall.scratch: 0.7156, Recall.stain: 0.5997, Recall.edgeDamage: 0.8731, Dice.background: 0.9895, Dice.scratch: 0.5659, Dice.stain: 0.3766, Dice.edgeDamage: 0.3456
2023-11-01 16:55:13,163 - mmseg - INFO - Iter [1050/5000]	lr: 8.281e-05, eta: 1:34:45, time: 2.371, data_time: 1.470, memory: 13756, decode.loss_focal: 0.0704, decode.loss_dice: 1.5566, decode.acc_seg: 99.2669, loss: 1.6270
2023-11-01 16:56:21,472 - mmseg - INFO - Iter [1100/5000]	lr: 8.198e-05, eta: 1:33:20, time: 1.366, data_time: 0.448, memory: 13756, decode.loss_focal: 0.0668, decode.loss_dice: 1.5467, decode.acc_seg: 99.3132, loss: 1.6135
2023-11-01 16:57:25,482 - mmseg - INFO - Iter [1150/5000]	lr: 8.115e-05, eta: 1:31:42, time: 1.280, data_time: 0.372, memory: 13756, decode.loss_focal: 0.0633, decode.loss_dice: 1.5372, decode.acc_seg: 99.3328, loss: 1.6005
2023-11-01 16:58:33,723 - mmseg - INFO - Iter [1200/5000]	lr: 8.032e-05, eta: 1:30:20, time: 1.365, data_time: 0.446, memory: 13756, decode.loss_focal: 0.0601, decode.loss_dice: 1.5278, decode.acc_seg: 99.3518, loss: 1.5879
2023-11-01 16:59:38,354 - mmseg - INFO - Iter [1250/5000]	lr: 7.949e-05, eta: 1:28:49, time: 1.293, data_time: 0.378, memory: 13756, decode.loss_focal: 0.0571, decode.loss_dice: 1.5182, decode.acc_seg: 99.3817, loss: 1.5753
2023-11-01 17:00:46,606 - mmseg - INFO - Iter [1300/5000]	lr: 7.865e-05, eta: 1:27:30, time: 1.365, data_time: 0.453, memory: 13756, decode.loss_focal: 0.0542, decode.loss_dice: 1.5094, decode.acc_seg: 99.3878, loss: 1.5636
2023-11-01 17:01:50,806 - mmseg - INFO - Iter [1350/5000]	lr: 7.782e-05, eta: 1:26:01, time: 1.284, data_time: 0.377, memory: 13756, decode.loss_focal: 0.0515, decode.loss_dice: 1.4999, decode.acc_seg: 99.4124, loss: 1.5515
2023-11-01 17:02:59,123 - mmseg - INFO - Iter [1400/5000]	lr: 7.698e-05, eta: 1:24:44, time: 1.366, data_time: 0.443, memory: 13756, decode.loss_focal: 0.0489, decode.loss_dice: 1.4906, decode.acc_seg: 99.4359, loss: 1.5396
2023-11-01 17:04:07,161 - mmseg - INFO - Iter [1450/5000]	lr: 7.614e-05, eta: 1:23:27, time: 1.361, data_time: 0.451, memory: 13756, decode.loss_focal: 0.0466, decode.loss_dice: 1.4822, decode.acc_seg: 99.4394, loss: 1.5288
2023-11-01 17:05:11,454 - mmseg - INFO - Iter [1500/5000]	lr: 7.530e-05, eta: 1:22:02, time: 1.286, data_time: 0.374, memory: 13756, decode.loss_focal: 0.0442, decode.loss_dice: 1.4731, decode.acc_seg: 99.4836, loss: 1.5173
2023-11-01 17:06:07,643 - mmseg - INFO - per class results:
2023-11-01 17:06:07,644 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.09 | 71.28 |  73.0 | 70.81 | 98.04 | 99.31  |   99.94   | 98.69  | 98.96 |
|  scratch   | 78.22 | 92.15 | 74.67 | 79.17 | 60.39 | 74.17  |   74.79   |  73.6  | 61.26 |
|   stain    |  74.0 | 96.38 | 75.46 | 83.15 | 68.83 | 65.24  |   61.65   | 79.22  | 47.87 |
| edgeDamage | 70.63 | 99.74 | 75.85 | 85.06 | 78.05 | 56.32  |   55.94   | 85.36  | 34.49 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 17:06:07,644 - mmseg - INFO - Summary:
2023-11-01 17:06:07,644 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.09 | 80.48 | 89.89 | 74.74 | 79.55 | 76.33 |  73.76  |   73.08    |  84.22  | 60.64 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 17:06:07,649 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9909, mIoU: 0.8048, mVOE: 0.8989, mASD: 0.7474, mMSSD: 0.7955, mAcc: 0.7633, mFscore: 0.7376, mPrecision: 0.7308, mRecall: 0.8422, mDice: 0.6064, IoU.background: 0.9909, IoU.scratch: 0.7822, IoU.stain: 0.7400, IoU.edgeDamage: 0.7063, VOE.background: 0.7128, VOE.scratch: 0.9215, VOE.stain: 0.9638, VOE.edgeDamage: 0.9974, ASD.background: 0.7300, ASD.scratch: 0.7467, ASD.stain: 0.7546, ASD.edgeDamage: 0.7585, MSSD.background: 0.7081, MSSD.scratch: 0.7917, MSSD.stain: 0.8315, MSSD.edgeDamage: 0.8506, Acc.background: 0.9804, Acc.scratch: 0.6039, Acc.stain: 0.6883, Acc.edgeDamage: 0.7805, Fscore.background: 0.9931, Fscore.scratch: 0.7417, Fscore.stain: 0.6524, Fscore.edgeDamage: 0.5632, Precision.background: 0.9994, Precision.scratch: 0.7479, Precision.stain: 0.6165, Precision.edgeDamage: 0.5594, Recall.background: 0.9869, Recall.scratch: 0.7360, Recall.stain: 0.7922, Recall.edgeDamage: 0.8536, Dice.background: 0.9896, Dice.scratch: 0.6126, Dice.stain: 0.4787, Dice.edgeDamage: 0.3449
2023-11-01 17:07:15,648 - mmseg - INFO - Iter [1550/5000]	lr: 7.446e-05, eta: 1:22:51, time: 2.484, data_time: 1.570, memory: 13756, decode.loss_focal: 0.0420, decode.loss_dice: 1.4615, decode.acc_seg: 99.5001, loss: 1.5036
2023-11-01 17:08:20,000 - mmseg - INFO - Iter [1600/5000]	lr: 7.362e-05, eta: 1:21:23, time: 1.287, data_time: 0.370, memory: 13756, decode.loss_focal: 0.0399, decode.loss_dice: 1.4532, decode.acc_seg: 99.5103, loss: 1.4931
2023-11-01 17:09:28,237 - mmseg - INFO - Iter [1650/5000]	lr: 7.278e-05, eta: 1:20:04, time: 1.365, data_time: 0.445, memory: 13756, decode.loss_focal: 0.0380, decode.loss_dice: 1.4443, decode.acc_seg: 99.5242, loss: 1.4823
2023-11-01 17:10:32,723 - mmseg - INFO - Iter [1700/5000]	lr: 7.194e-05, eta: 1:18:38, time: 1.290, data_time: 0.378, memory: 13756, decode.loss_focal: 0.0361, decode.loss_dice: 1.4332, decode.acc_seg: 99.5483, loss: 1.4693
2023-11-01 17:11:41,205 - mmseg - INFO - Iter [1750/5000]	lr: 7.109e-05, eta: 1:17:21, time: 1.370, data_time: 0.453, memory: 13756, decode.loss_focal: 0.0343, decode.loss_dice: 1.4226, decode.acc_seg: 99.5738, loss: 1.4569
2023-11-01 17:12:45,648 - mmseg - INFO - Iter [1800/5000]	lr: 7.025e-05, eta: 1:15:57, time: 1.289, data_time: 0.372, memory: 13756, decode.loss_focal: 0.0329, decode.loss_dice: 1.4135, decode.acc_seg: 99.5966, loss: 1.4464
2023-11-01 17:13:53,915 - mmseg - INFO - Iter [1850/5000]	lr: 6.940e-05, eta: 1:14:41, time: 1.365, data_time: 0.451, memory: 13756, decode.loss_focal: 0.0324, decode.loss_dice: 1.4012, decode.acc_seg: 99.6087, loss: 1.4336
2023-11-01 17:15:02,488 - mmseg - INFO - Iter [1900/5000]	lr: 6.855e-05, eta: 1:13:26, time: 1.371, data_time: 0.445, memory: 13756, decode.loss_focal: 0.0313, decode.loss_dice: 1.3883, decode.acc_seg: 99.6332, loss: 1.4196
2023-11-01 17:16:06,861 - mmseg - INFO - Iter [1950/5000]	lr: 6.770e-05, eta: 1:12:04, time: 1.288, data_time: 0.374, memory: 13756, decode.loss_focal: 0.0300, decode.loss_dice: 1.3761, decode.acc_seg: 99.6510, loss: 1.4060
2023-11-01 17:17:14,816 - mmseg - INFO - Exp name: transunet.py
2023-11-01 17:17:14,817 - mmseg - INFO - Iter [2000/5000]	lr: 6.685e-05, eta: 1:10:49, time: 1.359, data_time: 0.446, memory: 13756, decode.loss_focal: 0.0286, decode.loss_dice: 1.3643, decode.acc_seg: 99.6666, loss: 1.3929
2023-11-01 17:18:10,203 - mmseg - INFO - per class results:
2023-11-01 17:18:10,204 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.07 |  71.3 |  73.0 | 70.82 | 97.99 | 99.29  |   99.95   | 98.66  | 98.94 |
|  scratch   | 78.83 | 91.54 |  74.6 | 78.84 | 64.11 |  75.3  |   74.59   | 76.07  | 62.96 |
|   stain    | 76.31 | 94.06 | 75.19 | 81.77 | 73.86 | 70.41  |    65.8   | 82.57  | 55.61 |
| edgeDamage |  70.7 | 99.67 | 75.84 | 85.02 | 93.24 | 56.54  |   56.06   | 95.49  | 34.81 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 17:18:10,204 - mmseg - INFO - Summary:
2023-11-01 17:18:10,204 - mmseg - INFO - 
+-------+-------+-------+-------+-------+------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD | mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+------+---------+------------+---------+-------+
| 99.07 | 81.23 | 89.14 | 74.66 | 79.11 | 82.3 |  75.39  |    74.1    |   88.2  | 63.08 |
+-------+-------+-------+-------+-------+------+---------+------------+---------+-------+
2023-11-01 17:18:10,209 - mmseg - INFO - Exp name: transunet.py
2023-11-01 17:18:10,210 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9907, mIoU: 0.8123, mVOE: 0.8914, mASD: 0.7466, mMSSD: 0.7911, mAcc: 0.8230, mFscore: 0.7539, mPrecision: 0.7410, mRecall: 0.8820, mDice: 0.6308, IoU.background: 0.9907, IoU.scratch: 0.7883, IoU.stain: 0.7631, IoU.edgeDamage: 0.7070, VOE.background: 0.7130, VOE.scratch: 0.9154, VOE.stain: 0.9406, VOE.edgeDamage: 0.9967, ASD.background: 0.7300, ASD.scratch: 0.7460, ASD.stain: 0.7519, ASD.edgeDamage: 0.7584, MSSD.background: 0.7082, MSSD.scratch: 0.7884, MSSD.stain: 0.8177, MSSD.edgeDamage: 0.8502, Acc.background: 0.9799, Acc.scratch: 0.6411, Acc.stain: 0.7386, Acc.edgeDamage: 0.9324, Fscore.background: 0.9929, Fscore.scratch: 0.7530, Fscore.stain: 0.7041, Fscore.edgeDamage: 0.5654, Precision.background: 0.9995, Precision.scratch: 0.7459, Precision.stain: 0.6580, Precision.edgeDamage: 0.5606, Recall.background: 0.9866, Recall.scratch: 0.7607, Recall.stain: 0.8257, Recall.edgeDamage: 0.9549, Dice.background: 0.9894, Dice.scratch: 0.6296, Dice.stain: 0.5561, Dice.edgeDamage: 0.3481
2023-11-01 17:19:14,383 - mmseg - INFO - Iter [2050/5000]	lr: 6.599e-05, eta: 1:10:48, time: 2.391, data_time: 1.481, memory: 13756, decode.loss_focal: 0.0273, decode.loss_dice: 1.3518, decode.acc_seg: 99.6693, loss: 1.3791
2023-11-01 17:20:22,645 - mmseg - INFO - Iter [2100/5000]	lr: 6.514e-05, eta: 1:09:31, time: 1.365, data_time: 0.451, memory: 13756, decode.loss_focal: 0.0260, decode.loss_dice: 1.3407, decode.acc_seg: 99.6757, loss: 1.3668
2023-11-01 17:21:26,957 - mmseg - INFO - Iter [2150/5000]	lr: 6.428e-05, eta: 1:08:09, time: 1.286, data_time: 0.375, memory: 13756, decode.loss_focal: 0.0248, decode.loss_dice: 1.3241, decode.acc_seg: 99.6963, loss: 1.3489
2023-11-01 17:22:35,034 - mmseg - INFO - Iter [2200/5000]	lr: 6.343e-05, eta: 1:06:53, time: 1.361, data_time: 0.445, memory: 13756, decode.loss_focal: 0.0238, decode.loss_dice: 1.3116, decode.acc_seg: 99.7180, loss: 1.3354
2023-11-01 17:23:39,601 - mmseg - INFO - Iter [2250/5000]	lr: 6.257e-05, eta: 1:05:32, time: 1.291, data_time: 0.380, memory: 13756, decode.loss_focal: 0.0235, decode.loss_dice: 1.2964, decode.acc_seg: 99.7116, loss: 1.3200
2023-11-01 17:24:48,072 - mmseg - INFO - Iter [2300/5000]	lr: 6.171e-05, eta: 1:04:17, time: 1.369, data_time: 0.448, memory: 13756, decode.loss_focal: 0.0225, decode.loss_dice: 1.2829, decode.acc_seg: 99.7229, loss: 1.3054
2023-11-01 17:25:56,330 - mmseg - INFO - Iter [2350/5000]	lr: 6.084e-05, eta: 1:03:02, time: 1.365, data_time: 0.454, memory: 13756, decode.loss_focal: 0.0216, decode.loss_dice: 1.2813, decode.acc_seg: 99.6969, loss: 1.3030
2023-11-01 17:27:00,660 - mmseg - INFO - Iter [2400/5000]	lr: 5.998e-05, eta: 1:01:43, time: 1.287, data_time: 0.374, memory: 13756, decode.loss_focal: 0.0206, decode.loss_dice: 1.2625, decode.acc_seg: 99.7278, loss: 1.2831
2023-11-01 17:28:09,056 - mmseg - INFO - Iter [2450/5000]	lr: 5.911e-05, eta: 1:00:29, time: 1.368, data_time: 0.446, memory: 13756, decode.loss_focal: 0.0197, decode.loss_dice: 1.2532, decode.acc_seg: 99.7249, loss: 1.2729
2023-11-01 17:29:13,612 - mmseg - INFO - Iter [2500/5000]	lr: 5.825e-05, eta: 0:59:11, time: 1.291, data_time: 0.373, memory: 13756, decode.loss_focal: 0.0187, decode.loss_dice: 1.2359, decode.acc_seg: 99.7428, loss: 1.2547
2023-11-01 17:30:08,779 - mmseg - INFO - per class results:
2023-11-01 17:30:08,780 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.12 | 71.25 | 72.99 | 70.79 |  98.1 | 99.33  |   99.94   | 98.74  | 98.99 |
|  scratch   | 79.08 | 91.29 | 74.65 | 79.08 | 60.82 | 75.74  |   78.02   | 73.88  | 63.61 |
|   stain    | 77.63 | 92.74 | 74.94 | 80.51 | 68.24 | 73.05  |   69.58   | 78.83  | 59.58 |
| edgeDamage | 70.66 | 99.71 | 75.84 | 85.04 | 83.26 | 56.41  |   55.99   | 88.84  | 34.62 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 17:30:08,780 - mmseg - INFO - Summary:
2023-11-01 17:30:08,781 - mmseg - INFO - 
+-------+-------+-------+------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE | mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+------+-------+-------+---------+------------+---------+-------+
| 99.12 | 81.62 | 88.75 | 74.6 | 78.86 | 77.61 |  76.13  |   75.88    |  85.07  |  64.2 |
+-------+-------+-------+------+-------+-------+---------+------------+---------+-------+
2023-11-01 17:30:08,785 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9912, mIoU: 0.8162, mVOE: 0.8875, mASD: 0.7460, mMSSD: 0.7886, mAcc: 0.7761, mFscore: 0.7613, mPrecision: 0.7588, mRecall: 0.8507, mDice: 0.6420, IoU.background: 0.9912, IoU.scratch: 0.7908, IoU.stain: 0.7763, IoU.edgeDamage: 0.7066, VOE.background: 0.7125, VOE.scratch: 0.9129, VOE.stain: 0.9274, VOE.edgeDamage: 0.9971, ASD.background: 0.7299, ASD.scratch: 0.7465, ASD.stain: 0.7494, ASD.edgeDamage: 0.7584, MSSD.background: 0.7079, MSSD.scratch: 0.7908, MSSD.stain: 0.8051, MSSD.edgeDamage: 0.8504, Acc.background: 0.9810, Acc.scratch: 0.6082, Acc.stain: 0.6824, Acc.edgeDamage: 0.8326, Fscore.background: 0.9933, Fscore.scratch: 0.7574, Fscore.stain: 0.7305, Fscore.edgeDamage: 0.5641, Precision.background: 0.9994, Precision.scratch: 0.7802, Precision.stain: 0.6958, Precision.edgeDamage: 0.5599, Recall.background: 0.9874, Recall.scratch: 0.7388, Recall.stain: 0.7883, Recall.edgeDamage: 0.8884, Dice.background: 0.9899, Dice.scratch: 0.6361, Dice.stain: 0.5958, Dice.edgeDamage: 0.3462
2023-11-01 17:31:16,894 - mmseg - INFO - Iter [2550/5000]	lr: 5.738e-05, eta: 0:58:50, time: 2.466, data_time: 1.550, memory: 13756, decode.loss_focal: 0.0178, decode.loss_dice: 1.2220, decode.acc_seg: 99.7619, loss: 1.2398
2023-11-01 17:32:21,298 - mmseg - INFO - Iter [2600/5000]	lr: 5.651e-05, eta: 0:57:31, time: 1.288, data_time: 0.377, memory: 13756, decode.loss_focal: 0.0170, decode.loss_dice: 1.2099, decode.acc_seg: 99.7650, loss: 1.2269
2023-11-01 17:33:29,510 - mmseg - INFO - Iter [2650/5000]	lr: 5.563e-05, eta: 0:56:16, time: 1.364, data_time: 0.445, memory: 13756, decode.loss_focal: 0.0163, decode.loss_dice: 1.1999, decode.acc_seg: 99.7658, loss: 1.2162
2023-11-01 17:34:33,838 - mmseg - INFO - Iter [2700/5000]	lr: 5.476e-05, eta: 0:54:58, time: 1.287, data_time: 0.373, memory: 13756, decode.loss_focal: 0.0155, decode.loss_dice: 1.1880, decode.acc_seg: 99.7719, loss: 1.2035
2023-11-01 17:35:41,860 - mmseg - INFO - Iter [2750/5000]	lr: 5.388e-05, eta: 0:53:43, time: 1.360, data_time: 0.446, memory: 13756, decode.loss_focal: 0.0155, decode.loss_dice: 1.1718, decode.acc_seg: 99.7851, loss: 1.1873
2023-11-01 17:36:50,234 - mmseg - INFO - Iter [2800/5000]	lr: 5.301e-05, eta: 0:52:29, time: 1.367, data_time: 0.446, memory: 13756, decode.loss_focal: 0.0156, decode.loss_dice: 1.1617, decode.acc_seg: 99.7803, loss: 1.1773
2023-11-01 17:37:54,446 - mmseg - INFO - Iter [2850/5000]	lr: 5.213e-05, eta: 0:51:12, time: 1.284, data_time: 0.374, memory: 13756, decode.loss_focal: 0.0150, decode.loss_dice: 1.1490, decode.acc_seg: 99.7901, loss: 1.1640
2023-11-01 17:39:02,740 - mmseg - INFO - Iter [2900/5000]	lr: 5.124e-05, eta: 0:49:58, time: 1.366, data_time: 0.452, memory: 13756, decode.loss_focal: 0.0144, decode.loss_dice: 1.1379, decode.acc_seg: 99.7998, loss: 1.1523
2023-11-01 17:40:07,157 - mmseg - INFO - Iter [2950/5000]	lr: 5.036e-05, eta: 0:48:42, time: 1.288, data_time: 0.371, memory: 13756, decode.loss_focal: 0.0139, decode.loss_dice: 1.1294, decode.acc_seg: 99.7951, loss: 1.1432
2023-11-01 17:41:15,374 - mmseg - INFO - Exp name: transunet.py
2023-11-01 17:41:15,374 - mmseg - INFO - Iter [3000/5000]	lr: 4.947e-05, eta: 0:47:28, time: 1.364, data_time: 0.454, memory: 13756, decode.loss_focal: 0.0133, decode.loss_dice: 1.1222, decode.acc_seg: 99.8046, loss: 1.1355
2023-11-01 17:42:11,005 - mmseg - INFO - per class results:
2023-11-01 17:42:11,007 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.13 | 71.24 | 72.99 | 70.79 | 98.12 | 99.34  |   99.94   | 98.75  | 99.01 |
|  scratch   |  80.6 | 89.77 | 74.54 | 78.51 | 63.36 | 78.37  |   82.08   | 75.58  | 67.56 |
|   stain    | 80.53 | 89.84 | 74.41 | 77.88 | 68.64 | 78.26  |   77.48   | 79.09  | 67.38 |
| edgeDamage | 70.68 | 99.69 | 75.84 | 85.03 | 87.26 | 56.47  |   56.02   | 91.51  | 34.71 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 17:42:11,007 - mmseg - INFO - Summary:
2023-11-01 17:42:11,007 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.13 | 82.74 | 87.63 | 74.44 | 78.05 | 79.35 |  78.11  |   78.88    |  86.23  | 67.17 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 17:42:11,010 - mmseg - INFO - Exp name: transunet.py
2023-11-01 17:42:11,011 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9913, mIoU: 0.8274, mVOE: 0.8763, mASD: 0.7444, mMSSD: 0.7805, mAcc: 0.7935, mFscore: 0.7811, mPrecision: 0.7888, mRecall: 0.8623, mDice: 0.6717, IoU.background: 0.9913, IoU.scratch: 0.8060, IoU.stain: 0.8053, IoU.edgeDamage: 0.7068, VOE.background: 0.7124, VOE.scratch: 0.8977, VOE.stain: 0.8984, VOE.edgeDamage: 0.9969, ASD.background: 0.7299, ASD.scratch: 0.7454, ASD.stain: 0.7441, ASD.edgeDamage: 0.7584, MSSD.background: 0.7079, MSSD.scratch: 0.7851, MSSD.stain: 0.7788, MSSD.edgeDamage: 0.8503, Acc.background: 0.9812, Acc.scratch: 0.6336, Acc.stain: 0.6864, Acc.edgeDamage: 0.8726, Fscore.background: 0.9934, Fscore.scratch: 0.7837, Fscore.stain: 0.7826, Fscore.edgeDamage: 0.5647, Precision.background: 0.9994, Precision.scratch: 0.8208, Precision.stain: 0.7748, Precision.edgeDamage: 0.5602, Recall.background: 0.9875, Recall.scratch: 0.7558, Recall.stain: 0.7909, Recall.edgeDamage: 0.9151, Dice.background: 0.9901, Dice.scratch: 0.6756, Dice.stain: 0.6738, Dice.edgeDamage: 0.3471
2023-11-01 17:43:15,349 - mmseg - INFO - Iter [3050/5000]	lr: 4.858e-05, eta: 0:46:48, time: 2.400, data_time: 1.484, memory: 13756, decode.loss_focal: 0.0127, decode.loss_dice: 1.1113, decode.acc_seg: 99.8130, loss: 1.1240
2023-11-01 17:44:23,395 - mmseg - INFO - Iter [3100/5000]	lr: 4.769e-05, eta: 0:45:34, time: 1.361, data_time: 0.445, memory: 13756, decode.loss_focal: 0.0122, decode.loss_dice: 1.1098, decode.acc_seg: 99.8077, loss: 1.1221
2023-11-01 17:45:27,706 - mmseg - INFO - Iter [3150/5000]	lr: 4.680e-05, eta: 0:44:18, time: 1.286, data_time: 0.377, memory: 13756, decode.loss_focal: 0.0119, decode.loss_dice: 1.1066, decode.acc_seg: 99.7962, loss: 1.1185
2023-11-01 17:46:35,797 - mmseg - INFO - Iter [3200/5000]	lr: 4.590e-05, eta: 0:43:04, time: 1.362, data_time: 0.445, memory: 13756, decode.loss_focal: 0.0114, decode.loss_dice: 1.0937, decode.acc_seg: 99.8108, loss: 1.1051
2023-11-01 17:47:43,980 - mmseg - INFO - Iter [3250/5000]	lr: 4.500e-05, eta: 0:41:50, time: 1.364, data_time: 0.444, memory: 13756, decode.loss_focal: 0.0110, decode.loss_dice: 1.0832, decode.acc_seg: 99.8146, loss: 1.0941
2023-11-01 17:48:48,174 - mmseg - INFO - Iter [3300/5000]	lr: 4.410e-05, eta: 0:40:34, time: 1.284, data_time: 0.373, memory: 13756, decode.loss_focal: 0.0105, decode.loss_dice: 1.0752, decode.acc_seg: 99.8226, loss: 1.0857
2023-11-01 17:49:56,228 - mmseg - INFO - Iter [3350/5000]	lr: 4.320e-05, eta: 0:39:21, time: 1.361, data_time: 0.444, memory: 13756, decode.loss_focal: 0.0102, decode.loss_dice: 1.0704, decode.acc_seg: 99.8201, loss: 1.0806
2023-11-01 17:51:00,766 - mmseg - INFO - Iter [3400/5000]	lr: 4.229e-05, eta: 0:38:06, time: 1.291, data_time: 0.376, memory: 13756, decode.loss_focal: 0.0099, decode.loss_dice: 1.0628, decode.acc_seg: 99.8230, loss: 1.0727
2023-11-01 17:52:08,881 - mmseg - INFO - Iter [3450/5000]	lr: 4.138e-05, eta: 0:36:53, time: 1.362, data_time: 0.447, memory: 13756, decode.loss_focal: 0.0095, decode.loss_dice: 1.0586, decode.acc_seg: 99.8310, loss: 1.0682
2023-11-01 17:53:13,311 - mmseg - INFO - Iter [3500/5000]	lr: 4.047e-05, eta: 0:35:39, time: 1.289, data_time: 0.375, memory: 13756, decode.loss_focal: 0.0093, decode.loss_dice: 1.0505, decode.acc_seg: 99.8292, loss: 1.0598
2023-11-01 17:54:09,769 - mmseg - INFO - per class results:
2023-11-01 17:54:09,770 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.14 | 71.23 | 72.99 | 70.78 | 98.16 | 99.35  |   99.94   | 98.77  | 99.02 |
|  scratch   | 80.59 | 89.78 | 74.63 | 78.99 | 61.21 | 78.36  |   85.05   | 74.14  | 67.54 |
|   stain    |  80.1 | 90.27 | 74.75 | 79.59 |  58.5 | 77.52  |   87.37   | 72.33  | 66.28 |
| edgeDamage | 70.69 | 99.68 | 75.84 | 85.03 | 88.07 |  56.5  |   56.03   | 92.05  | 34.74 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 17:54:09,770 - mmseg - INFO - Summary:
2023-11-01 17:54:09,770 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.14 | 82.63 | 87.74 | 74.55 |  78.6 | 76.49 |  77.93  |    82.1    |  84.32  |  66.9 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 17:54:09,774 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9914, mIoU: 0.8263, mVOE: 0.8774, mASD: 0.7455, mMSSD: 0.7860, mAcc: 0.7649, mFscore: 0.7793, mPrecision: 0.8210, mRecall: 0.8432, mDice: 0.6690, IoU.background: 0.9914, IoU.scratch: 0.8059, IoU.stain: 0.8010, IoU.edgeDamage: 0.7069, VOE.background: 0.7123, VOE.scratch: 0.8978, VOE.stain: 0.9027, VOE.edgeDamage: 0.9968, ASD.background: 0.7299, ASD.scratch: 0.7463, ASD.stain: 0.7475, ASD.edgeDamage: 0.7584, MSSD.background: 0.7078, MSSD.scratch: 0.7899, MSSD.stain: 0.7959, MSSD.edgeDamage: 0.8503, Acc.background: 0.9816, Acc.scratch: 0.6121, Acc.stain: 0.5850, Acc.edgeDamage: 0.8807, Fscore.background: 0.9935, Fscore.scratch: 0.7836, Fscore.stain: 0.7752, Fscore.edgeDamage: 0.5650, Precision.background: 0.9994, Precision.scratch: 0.8505, Precision.stain: 0.8737, Precision.edgeDamage: 0.5603, Recall.background: 0.9877, Recall.scratch: 0.7414, Recall.stain: 0.7233, Recall.edgeDamage: 0.9205, Dice.background: 0.9902, Dice.scratch: 0.6754, Dice.stain: 0.6628, Dice.edgeDamage: 0.3474
2023-11-01 17:55:17,397 - mmseg - INFO - Iter [3550/5000]	lr: 3.956e-05, eta: 0:34:49, time: 2.482, data_time: 1.575, memory: 13756, decode.loss_focal: 0.0090, decode.loss_dice: 1.0434, decode.acc_seg: 99.8400, loss: 1.0523
2023-11-01 17:56:21,545 - mmseg - INFO - Iter [3600/5000]	lr: 3.864e-05, eta: 0:33:34, time: 1.283, data_time: 0.377, memory: 13756, decode.loss_focal: 0.0087, decode.loss_dice: 1.0393, decode.acc_seg: 99.8347, loss: 1.0480
2023-11-01 17:57:29,794 - mmseg - INFO - Iter [3650/5000]	lr: 3.772e-05, eta: 0:32:20, time: 1.365, data_time: 0.444, memory: 13756, decode.loss_focal: 0.0085, decode.loss_dice: 1.0334, decode.acc_seg: 99.8362, loss: 1.0418
2023-11-01 17:58:38,029 - mmseg - INFO - Iter [3700/5000]	lr: 3.679e-05, eta: 0:31:07, time: 1.365, data_time: 0.444, memory: 13756, decode.loss_focal: 0.0082, decode.loss_dice: 1.0289, decode.acc_seg: 99.8393, loss: 1.0371
2023-11-01 17:59:42,282 - mmseg - INFO - Iter [3750/5000]	lr: 3.586e-05, eta: 0:29:53, time: 1.285, data_time: 0.370, memory: 13756, decode.loss_focal: 0.0080, decode.loss_dice: 1.0239, decode.acc_seg: 99.8439, loss: 1.0319
2023-11-01 18:00:50,413 - mmseg - INFO - Iter [3800/5000]	lr: 3.493e-05, eta: 0:28:40, time: 1.363, data_time: 0.442, memory: 13756, decode.loss_focal: 0.0078, decode.loss_dice: 1.0225, decode.acc_seg: 99.8440, loss: 1.0303
2023-11-01 18:01:54,910 - mmseg - INFO - Iter [3850/5000]	lr: 3.400e-05, eta: 0:27:26, time: 1.290, data_time: 0.373, memory: 13756, decode.loss_focal: 0.0076, decode.loss_dice: 1.0171, decode.acc_seg: 99.8448, loss: 1.0247
2023-11-01 18:03:03,476 - mmseg - INFO - Iter [3900/5000]	lr: 3.306e-05, eta: 0:26:14, time: 1.371, data_time: 0.452, memory: 13756, decode.loss_focal: 0.0074, decode.loss_dice: 1.0108, decode.acc_seg: 99.8473, loss: 1.0182
2023-11-01 18:04:08,301 - mmseg - INFO - Iter [3950/5000]	lr: 3.211e-05, eta: 0:25:00, time: 1.297, data_time: 0.384, memory: 13756, decode.loss_focal: 0.0073, decode.loss_dice: 1.0135, decode.acc_seg: 99.8482, loss: 1.0208
2023-11-01 18:05:16,894 - mmseg - INFO - Exp name: transunet.py
2023-11-01 18:05:16,895 - mmseg - INFO - Iter [4000/5000]	lr: 3.116e-05, eta: 0:23:48, time: 1.372, data_time: 0.454, memory: 13756, decode.loss_focal: 0.0071, decode.loss_dice: 1.0045, decode.acc_seg: 99.8528, loss: 1.0116
2023-11-01 18:06:13,683 - mmseg - INFO - per class results:
2023-11-01 18:06:13,686 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.15 | 71.22 | 72.99 | 70.77 | 98.19 | 99.36  |   99.93   |  98.8  | 99.04 |
|  scratch   | 80.62 | 89.75 | 74.69 |  79.3 | 59.83 |  78.4  |   87.87   | 73.22  |  67.6 |
|   stain    | 79.95 | 90.42 | 74.79 | 79.79 |  57.6 | 77.27  |   88.59   | 71.73  | 65.91 |
| edgeDamage | 70.64 | 99.73 | 75.84 | 85.05 | 79.69 | 56.36  |   55.97   | 86.46  | 34.55 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 18:06:13,686 - mmseg - INFO - Summary:
2023-11-01 18:06:13,686 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.16 | 82.59 | 87.78 | 74.58 | 78.73 | 73.83 |  77.85  |   83.09    |  82.55  | 66.77 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 18:06:13,689 - mmseg - INFO - Exp name: transunet.py
2023-11-01 18:06:13,690 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9916, mIoU: 0.8259, mVOE: 0.8778, mASD: 0.7458, mMSSD: 0.7873, mAcc: 0.7383, mFscore: 0.7785, mPrecision: 0.8309, mRecall: 0.8255, mDice: 0.6677, IoU.background: 0.9915, IoU.scratch: 0.8062, IoU.stain: 0.7995, IoU.edgeDamage: 0.7064, VOE.background: 0.7122, VOE.scratch: 0.8975, VOE.stain: 0.9042, VOE.edgeDamage: 0.9973, ASD.background: 0.7299, ASD.scratch: 0.7469, ASD.stain: 0.7479, ASD.edgeDamage: 0.7584, MSSD.background: 0.7077, MSSD.scratch: 0.7930, MSSD.stain: 0.7979, MSSD.edgeDamage: 0.8505, Acc.background: 0.9819, Acc.scratch: 0.5983, Acc.stain: 0.5760, Acc.edgeDamage: 0.7969, Fscore.background: 0.9936, Fscore.scratch: 0.7840, Fscore.stain: 0.7727, Fscore.edgeDamage: 0.5636, Precision.background: 0.9993, Precision.scratch: 0.8787, Precision.stain: 0.8859, Precision.edgeDamage: 0.5597, Recall.background: 0.9880, Recall.scratch: 0.7322, Recall.stain: 0.7173, Recall.edgeDamage: 0.8646, Dice.background: 0.9904, Dice.scratch: 0.6760, Dice.stain: 0.6591, Dice.edgeDamage: 0.3455
2023-11-01 18:07:18,214 - mmseg - INFO - Iter [4050/5000]	lr: 3.021e-05, eta: 0:22:49, time: 2.426, data_time: 1.513, memory: 13756, decode.loss_focal: 0.0069, decode.loss_dice: 1.0007, decode.acc_seg: 99.8521, loss: 1.0076
2023-11-01 18:08:26,362 - mmseg - INFO - Iter [4100/5000]	lr: 2.925e-05, eta: 0:21:36, time: 1.363, data_time: 0.448, memory: 13756, decode.loss_focal: 0.0068, decode.loss_dice: 0.9999, decode.acc_seg: 99.8543, loss: 1.0067
2023-11-01 18:09:34,662 - mmseg - INFO - Iter [4150/5000]	lr: 2.829e-05, eta: 0:20:23, time: 1.366, data_time: 0.448, memory: 13756, decode.loss_focal: 0.0067, decode.loss_dice: 0.9967, decode.acc_seg: 99.8464, loss: 1.0034
2023-11-01 18:10:39,544 - mmseg - INFO - Iter [4200/5000]	lr: 2.732e-05, eta: 0:19:10, time: 1.298, data_time: 0.378, memory: 13756, decode.loss_focal: 0.0066, decode.loss_dice: 0.9931, decode.acc_seg: 99.8528, loss: 0.9997
2023-11-01 18:11:48,219 - mmseg - INFO - Iter [4250/5000]	lr: 2.634e-05, eta: 0:17:57, time: 1.373, data_time: 0.449, memory: 13756, decode.loss_focal: 0.0064, decode.loss_dice: 0.9898, decode.acc_seg: 99.8563, loss: 0.9963
2023-11-01 18:12:52,936 - mmseg - INFO - Iter [4300/5000]	lr: 2.536e-05, eta: 0:16:44, time: 1.294, data_time: 0.379, memory: 13756, decode.loss_focal: 0.0063, decode.loss_dice: 0.9868, decode.acc_seg: 99.8538, loss: 0.9931
2023-11-01 18:14:01,477 - mmseg - INFO - Iter [4350/5000]	lr: 2.437e-05, eta: 0:15:32, time: 1.371, data_time: 0.454, memory: 13756, decode.loss_focal: 0.0062, decode.loss_dice: 0.9829, decode.acc_seg: 99.8575, loss: 0.9891
2023-11-01 18:15:06,167 - mmseg - INFO - Iter [4400/5000]	lr: 2.337e-05, eta: 0:14:19, time: 1.294, data_time: 0.375, memory: 13756, decode.loss_focal: 0.0061, decode.loss_dice: 0.9839, decode.acc_seg: 99.8604, loss: 0.9900
2023-11-01 18:16:14,567 - mmseg - INFO - Iter [4450/5000]	lr: 2.237e-05, eta: 0:13:07, time: 1.368, data_time: 0.457, memory: 13756, decode.loss_focal: 0.0060, decode.loss_dice: 0.9831, decode.acc_seg: 99.8610, loss: 0.9891
2023-11-01 18:17:18,965 - mmseg - INFO - Iter [4500/5000]	lr: 2.135e-05, eta: 0:11:55, time: 1.288, data_time: 0.370, memory: 13756, decode.loss_focal: 0.0059, decode.loss_dice: 0.9775, decode.acc_seg: 99.8578, loss: 0.9834
2023-11-01 18:18:18,125 - mmseg - INFO - per class results:
2023-11-01 18:18:18,127 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.16 | 71.21 | 72.99 | 70.77 |  98.2 | 99.36  |   99.94   |  98.8  | 99.04 |
|  scratch   | 81.19 | 89.18 | 74.58 | 78.75 | 62.28 | 79.33  |   86.51   | 74.85  | 68.99 |
|   stain    | 80.88 | 89.49 | 74.69 |  79.3 | 59.82 | 78.83  |   89.68   | 73.21  | 68.24 |
| edgeDamage | 70.63 | 99.74 | 75.84 | 85.05 | 78.01 | 56.34  |   55.95   | 85.34  | 34.51 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 18:18:18,127 - mmseg - INFO - Summary:
2023-11-01 18:18:18,127 - mmseg - INFO - 
+-------+-------+------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU | mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+------+-------+-------+-------+---------+------------+---------+-------+
| 99.16 | 82.97 | 87.4 | 74.53 | 78.47 | 74.58 |  78.46  |   83.02    |  83.05  |  67.7 |
+-------+-------+------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 18:18:18,131 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9916, mIoU: 0.8297, mVOE: 0.8740, mASD: 0.7453, mMSSD: 0.7847, mAcc: 0.7458, mFscore: 0.7846, mPrecision: 0.8302, mRecall: 0.8305, mDice: 0.6770, IoU.background: 0.9916, IoU.scratch: 0.8119, IoU.stain: 0.8088, IoU.edgeDamage: 0.7063, VOE.background: 0.7121, VOE.scratch: 0.8918, VOE.stain: 0.8949, VOE.edgeDamage: 0.9974, ASD.background: 0.7299, ASD.scratch: 0.7458, ASD.stain: 0.7469, ASD.edgeDamage: 0.7584, MSSD.background: 0.7077, MSSD.scratch: 0.7875, MSSD.stain: 0.7930, MSSD.edgeDamage: 0.8505, Acc.background: 0.9820, Acc.scratch: 0.6228, Acc.stain: 0.5982, Acc.edgeDamage: 0.7801, Fscore.background: 0.9936, Fscore.scratch: 0.7933, Fscore.stain: 0.7883, Fscore.edgeDamage: 0.5634, Precision.background: 0.9994, Precision.scratch: 0.8651, Precision.stain: 0.8968, Precision.edgeDamage: 0.5595, Recall.background: 0.9880, Recall.scratch: 0.7485, Recall.stain: 0.7321, Recall.edgeDamage: 0.8534, Dice.background: 0.9904, Dice.scratch: 0.6899, Dice.stain: 0.6824, Dice.edgeDamage: 0.3451
2023-11-01 18:19:25,985 - mmseg - INFO - Iter [4550/5000]	lr: 2.033e-05, eta: 0:10:49, time: 2.540, data_time: 1.623, memory: 13756, decode.loss_focal: 0.0059, decode.loss_dice: 0.9748, decode.acc_seg: 99.8609, loss: 0.9806
2023-11-01 18:20:34,355 - mmseg - INFO - Iter [4600/5000]	lr: 1.929e-05, eta: 0:09:36, time: 1.367, data_time: 0.445, memory: 13756, decode.loss_focal: 0.0058, decode.loss_dice: 0.9713, decode.acc_seg: 99.8615, loss: 0.9771
2023-11-01 18:21:38,866 - mmseg - INFO - Iter [4650/5000]	lr: 1.824e-05, eta: 0:08:24, time: 1.290, data_time: 0.380, memory: 13756, decode.loss_focal: 0.0057, decode.loss_dice: 0.9716, decode.acc_seg: 99.8639, loss: 0.9773
2023-11-01 18:22:47,266 - mmseg - INFO - Iter [4700/5000]	lr: 1.718e-05, eta: 0:07:11, time: 1.368, data_time: 0.455, memory: 13756, decode.loss_focal: 0.0056, decode.loss_dice: 0.9672, decode.acc_seg: 99.8661, loss: 0.9728
2023-11-01 18:23:52,049 - mmseg - INFO - Iter [4750/5000]	lr: 1.609e-05, eta: 0:05:59, time: 1.296, data_time: 0.382, memory: 13756, decode.loss_focal: 0.0056, decode.loss_dice: 0.9666, decode.acc_seg: 99.8645, loss: 0.9722
2023-11-01 18:25:00,215 - mmseg - INFO - Iter [4800/5000]	lr: 1.499e-05, eta: 0:04:47, time: 1.363, data_time: 0.447, memory: 13756, decode.loss_focal: 0.0055, decode.loss_dice: 0.9653, decode.acc_seg: 99.8673, loss: 0.9708
2023-11-01 18:26:05,600 - mmseg - INFO - Iter [4850/5000]	lr: 1.386e-05, eta: 0:03:35, time: 1.308, data_time: 0.395, memory: 13756, decode.loss_focal: 0.0054, decode.loss_dice: 0.9624, decode.acc_seg: 99.8677, loss: 0.9679
2023-11-01 18:27:13,685 - mmseg - INFO - Iter [4900/5000]	lr: 1.269e-05, eta: 0:02:23, time: 1.361, data_time: 0.448, memory: 13756, decode.loss_focal: 0.0054, decode.loss_dice: 0.9632, decode.acc_seg: 99.8684, loss: 0.9686
2023-11-01 18:28:18,213 - mmseg - INFO - Iter [4950/5000]	lr: 1.145e-05, eta: 0:01:11, time: 1.291, data_time: 0.372, memory: 13756, decode.loss_focal: 0.0054, decode.loss_dice: 0.9591, decode.acc_seg: 99.8674, loss: 0.9644
2023-11-01 18:29:26,725 - mmseg - INFO - Saving checkpoint at 5000 iterations
2023-11-01 18:29:27,875 - mmseg - INFO - Exp name: transunet.py
2023-11-01 18:29:27,875 - mmseg - INFO - Iter [5000/5000]	lr: 1.004e-05, eta: 0:00:00, time: 1.393, data_time: 0.456, memory: 13756, decode.loss_focal: 0.0053, decode.loss_dice: 0.9577, decode.acc_seg: 99.8711, loss: 0.9630
2023-11-01 18:30:25,812 - mmseg - INFO - per class results:
2023-11-01 18:30:25,815 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.17 |  71.2 | 72.99 | 70.76 | 98.23 | 99.37  |   99.94   | 98.82  | 99.05 |
|  scratch   | 81.06 | 89.31 | 74.61 | 78.89 | 61.65 | 79.13  |   86.93   | 74.43  | 68.69 |
|   stain    | 80.76 | 89.61 | 74.71 | 79.36 | 59.54 | 78.63  |   89.54   | 73.02  | 67.95 |
| edgeDamage | 70.66 | 99.71 | 75.84 | 85.04 | 82.42 | 56.43  |    56.0   | 88.28  | 34.64 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-01 18:30:25,815 - mmseg - INFO - Summary:
2023-11-01 18:30:25,816 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.17 | 82.91 | 87.46 | 74.54 | 78.51 | 75.46 |  78.39  |    83.1    |  83.64  | 67.58 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-01 18:30:25,820 - mmseg - INFO - Exp name: transunet.py
2023-11-01 18:30:25,821 - mmseg - INFO - Iter(val) [20]	aAcc: 0.9917, mIoU: 0.8291, mVOE: 0.8746, mASD: 0.7454, mMSSD: 0.7851, mAcc: 0.7546, mFscore: 0.7839, mPrecision: 0.8310, mRecall: 0.8364, mDice: 0.6758, IoU.background: 0.9917, IoU.scratch: 0.8106, IoU.stain: 0.8076, IoU.edgeDamage: 0.7066, VOE.background: 0.7120, VOE.scratch: 0.8931, VOE.stain: 0.8961, VOE.edgeDamage: 0.9971, ASD.background: 0.7299, ASD.scratch: 0.7461, ASD.stain: 0.7471, ASD.edgeDamage: 0.7584, MSSD.background: 0.7076, MSSD.scratch: 0.7889, MSSD.stain: 0.7936, MSSD.edgeDamage: 0.8504, Acc.background: 0.9823, Acc.scratch: 0.6165, Acc.stain: 0.5954, Acc.edgeDamage: 0.8242, Fscore.background: 0.9937, Fscore.scratch: 0.7913, Fscore.stain: 0.7863, Fscore.edgeDamage: 0.5643, Precision.background: 0.9994, Precision.scratch: 0.8693, Precision.stain: 0.8954, Precision.edgeDamage: 0.5600, Recall.background: 0.9882, Recall.scratch: 0.7443, Recall.stain: 0.7302, Recall.edgeDamage: 0.8828, Dice.background: 0.9905, Dice.scratch: 0.6869, Dice.stain: 0.6795, Dice.edgeDamage: 0.3464
