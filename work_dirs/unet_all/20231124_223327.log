2023-11-24 22:33:27,688 - mmseg - INFO - Multi-processing start method is `None`
2023-11-24 22:33:27,689 - mmseg - INFO - OpenCV num_threads is `6
2023-11-24 22:33:27,739 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: /environment/miniconda3
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.11.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu113
OpenCV: 4.8.1
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.29.1+
------------------------------------------------------------

2023-11-24 22:33:27,740 - mmseg - INFO - Distributed training: False
2023-11-24 22:33:28,075 - mmseg - INFO - Config:
norm_cfg = dict(type='BN', requires_grad=True)
backbone_norm_cfg = dict(type='LN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained=None,
    backbone=dict(
        type='UnetBackbone',
        in_channels=3,
        context_layer='',
        conv_down=True,
        channel_list=[64, 128, 256, 512]),
    decode_head=dict(
        type='UnetHead',
        num_classes=4,
        channels=64,
        threshold=0.2,
        norm_cfg=dict(type='BN', requires_grad=True),
        loss_decode=[
            dict(
                type='FocalLoss',
                use_sigmoid=True,
                gamma=2.0,
                alpha=0.25,
                class_weight=[0.1, 0.5, 0.2, 0.2],
                loss_weight=2.0),
            dict(
                type='DiceLoss',
                loss_name='loss_dice',
                class_weight=[0.1, 0.5, 0.2, 0.2],
                loss_weight=2.0)
        ]))
train_cfg = dict()
test_cfg = dict(mode='whole')
dataset_type = 'MyDataset'
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(600, 600)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(512, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=False),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data_root = './datasets/'
data = dict(
    samples_per_gpu=5,
    workers_per_gpu=2,
    train=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='train/images',
        ann_dir='train/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(600, 600)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TensorboardLoggerHook'),
        dict(type='TextLoggerHook', by_epoch=False)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = False
optimizer = dict(type='Adam', lr=0.0001, betas=(0.9, 0.999))
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=1e-05, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=10000)
checkpoint_config = dict(by_epoch=False, save_optimizer=False, interval=5000)
evaluation = dict(interval=500, metric=['mIoU', 'mFscore', 'mDice'])
work_dir = './work_dirs/unet_all'
gpu_ids = [0]
auto_resume = False

2023-11-24 22:33:28,076 - mmseg - INFO - Set random seed to 1411150861, deterministic: False
2023-11-24 22:33:28,228 - mmseg - INFO - initialize UnetHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

backbone.inc.conv.shortCut.weight - torch.Size([64, 3, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.shortCut.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv1_1.0.weight - torch.Size([64, 3, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv1_1.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv1_1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv1_1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv1_2.0.weight - torch.Size([64, 64, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv1_2.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv1_2.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv1_2.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv2_1.0.weight - torch.Size([64, 3, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv2_1.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv2_1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv2_1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv2_2.0.weight - torch.Size([64, 64, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv2_2.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv2_2.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv2_2.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv3_1.0.weight - torch.Size([64, 3, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv3_1.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv3_1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv3_1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv3_2.0.weight - torch.Size([64, 64, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv3_2.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv3_2.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.inc.conv.conv3_2.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.0.weight - torch.Size([64, 64, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.shortCut.weight - torch.Size([128, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.shortCut.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv1_1.0.weight - torch.Size([128, 64, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv1_1.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv1_1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv1_1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv1_2.0.weight - torch.Size([128, 128, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv1_2.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv1_2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv1_2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv2_1.0.weight - torch.Size([128, 64, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv2_1.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv2_1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv2_1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv2_2.0.weight - torch.Size([128, 128, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv2_2.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv2_2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv2_2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv3_1.0.weight - torch.Size([128, 64, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv3_1.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv3_1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv3_1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv3_2.0.weight - torch.Size([128, 128, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv3_2.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv3_2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down1.down_conv.1.conv3_2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.0.weight - torch.Size([128, 128, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.shortCut.weight - torch.Size([256, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.shortCut.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv1_1.0.weight - torch.Size([256, 128, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv1_1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv1_1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv1_1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv1_2.0.weight - torch.Size([256, 256, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv1_2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv1_2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv1_2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv2_1.0.weight - torch.Size([256, 128, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv2_1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv2_1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv2_1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv2_2.0.weight - torch.Size([256, 256, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv2_2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv2_2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv2_2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv3_1.0.weight - torch.Size([256, 128, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv3_1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv3_1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv3_1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv3_2.0.weight - torch.Size([256, 256, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv3_2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv3_2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down2.down_conv.1.conv3_2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.shortCut.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.shortCut.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv1_1.0.weight - torch.Size([512, 256, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv1_1.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv1_1.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv1_1.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv1_2.0.weight - torch.Size([512, 512, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv1_2.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv1_2.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv1_2.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv2_1.0.weight - torch.Size([512, 256, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv2_1.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv2_1.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv2_1.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv2_2.0.weight - torch.Size([512, 512, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv2_2.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv2_2.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv2_2.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv3_1.0.weight - torch.Size([512, 256, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv3_1.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv3_1.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv3_1.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv3_2.0.weight - torch.Size([512, 512, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv3_2.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv3_2.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down3.down_conv.1.conv3_2.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.0.weight - torch.Size([512, 512, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.shortCut.weight - torch.Size([512, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.shortCut.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv1_1.0.weight - torch.Size([512, 512, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv1_1.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv1_1.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv1_1.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv1_2.0.weight - torch.Size([512, 512, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv1_2.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv1_2.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv1_2.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv2_1.0.weight - torch.Size([512, 512, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv2_1.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv2_1.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv2_1.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv2_2.0.weight - torch.Size([512, 512, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv2_2.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv2_2.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv2_2.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv3_1.0.weight - torch.Size([512, 512, 1, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv3_1.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv3_1.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv3_1.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv3_2.0.weight - torch.Size([512, 512, 3, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv3_2.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv3_2.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.down4.down_conv.1.conv3_2.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_seg.weight - torch.Size([4, 64, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.up1.up.weight - torch.Size([512, 512, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.up.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.coord.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.coord.conv1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.coord.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.coord.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.coord.conv_h.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.coord.conv_h.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.coord.conv_w.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.coord.conv_w.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.0.weight - torch.Size([256, 1024, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.3.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up1.conv.conv.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.up.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.up.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.coord.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.coord.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.coord.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.coord.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.coord.conv_h.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.coord.conv_h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.coord.conv_w.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.coord.conv_w.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.0.weight - torch.Size([128, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.3.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.3.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up2.conv.conv.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.up.weight - torch.Size([128, 128, 2, 2]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.up.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.coord.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.coord.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.coord.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.coord.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.coord.conv_h.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.coord.conv_h.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.coord.conv_w.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.coord.conv_w.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.0.weight - torch.Size([64, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up3.conv.conv.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.coord.conv1.weight - torch.Size([32, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.coord.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.coord.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.coord.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.coord.conv_h.weight - torch.Size([128, 32, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.coord.conv_h.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.coord.conv_w.weight - torch.Size([128, 32, 1, 1]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.coord.conv_w.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.0.weight - torch.Size([64, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.3.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.3.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.4.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.up4.conv.conv.4.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2023-11-24 22:33:28,235 - mmseg - INFO - EncoderDecoder(
  (backbone): UnetBackbone(
    (inc): MACInConv(
      (conv): MAC(
        (shortCut): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv1_1): Sequential(
          (0): Conv2d(3, 64, kernel_size=(1, 3), stride=(1, 1), padding=same)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (conv1_2): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=same)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (conv2_1): Sequential(
          (0): Conv2d(3, 64, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 2))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (conv2_2): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(2, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (conv3_1): Sequential(
          (0): Conv2d(3, 64, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 3))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (conv3_2): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(3, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
    )
    (down1): Conv_Down(
      (down_conv): Sequential(
        (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
        (1): MAC(
          (shortCut): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
          (conv1_1): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 3), stride=(1, 1), padding=same)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv1_2): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=same)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv2_1): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 2))
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv2_2): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(2, 1))
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv3_1): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 3))
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv3_2): Sequential(
            (0): Conv2d(128, 128, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(3, 1))
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (down2): Conv_Down(
      (down_conv): Sequential(
        (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
        (1): MAC(
          (shortCut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
          (conv1_1): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=same)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv1_2): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=same)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv2_1): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 2))
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv2_2): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(2, 1))
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv3_1): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 3))
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv3_2): Sequential(
            (0): Conv2d(256, 256, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(3, 1))
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (down3): Conv_Down(
      (down_conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
        (1): MAC(
          (shortCut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
          (conv1_1): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=same)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv1_2): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=same)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv2_1): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 2))
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv2_2): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(2, 1))
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv3_1): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 3))
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv3_2): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(3, 1))
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (down4): Conv_Down(
      (down_conv): Sequential(
        (0): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
        (1): MAC(
          (shortCut): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (conv1_1): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=same)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv1_2): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=same)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv2_1): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 2))
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv2_2): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(2, 1))
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv3_1): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=same, dilation=(1, 3))
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (conv3_2): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 1), stride=(1, 1), padding=same, dilation=(3, 1))
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
  )
  (decode_head): UnetHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): ModuleList(
      (0): FocalLoss()
      (1): DiceLoss()
    )
    (conv_seg): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (up1): Up(
      (up): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      (coord): CoordAtt(
        (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
        (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): h_swish(
          (sigmoid): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (conv_h): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_w): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up2): Up(
      (up): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (coord): CoordAtt(
        (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
        (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): h_swish(
          (sigmoid): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (conv_h): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_w): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up3): Up(
      (up): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
      (coord): CoordAtt(
        (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
        (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): h_swish(
          (sigmoid): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (conv_h): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_w): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up4): Up(
      (up): Upsample(scale_factor=2.0, mode=bilinear)
      (coord): CoordAtt(
        (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
        (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): h_swish(
          (sigmoid): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (conv_h): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_w): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (conv): DoubleConv(
        (conv): Sequential(
          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2023-11-24 22:33:28,247 - mmseg - INFO - Loaded 474 images
2023-11-24 22:33:34,550 - mmseg - INFO - Loaded 105 images
2023-11-24 22:33:34,550 - mmseg - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/test/work_dirs/unet_all
2023-11-24 22:33:34,551 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-11-24 22:33:34,551 - mmseg - INFO - workflow: [('train', 1)], max: 10000 iters
2023-11-24 22:33:34,551 - mmseg - INFO - Checkpoints will be saved to /home/featurize/work/test/work_dirs/unet_all by HardDiskBackend.
2023-11-24 22:33:57,051 - mmseg - INFO - Iter [50/10000]	lr: 9.960e-05, eta: 1:13:53, time: 0.446, data_time: 0.017, memory: 13650, decode.loss_focal: 0.0516, decode.loss_dice: 0.4675, decode.acc_seg: 97.2972, loss: 0.5190
2023-11-24 22:34:21,251 - mmseg - INFO - Iter [100/10000]	lr: 9.920e-05, eta: 1:16:41, time: 0.484, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0388, decode.loss_dice: 0.4609, decode.acc_seg: 99.5369, loss: 0.4997
2023-11-24 22:34:42,851 - mmseg - INFO - Iter [150/10000]	lr: 9.879e-05, eta: 1:14:30, time: 0.432, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0287, decode.loss_dice: 0.4552, decode.acc_seg: 99.6115, loss: 0.4839
2023-11-24 22:35:07,188 - mmseg - INFO - Iter [200/10000]	lr: 9.839e-05, eta: 1:15:28, time: 0.487, data_time: 0.055, memory: 13650, decode.loss_focal: 0.0214, decode.loss_dice: 0.4504, decode.acc_seg: 99.6231, loss: 0.4717
2023-11-24 22:35:29,107 - mmseg - INFO - Iter [250/10000]	lr: 9.798e-05, eta: 1:14:19, time: 0.438, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0163, decode.loss_dice: 0.4461, decode.acc_seg: 99.6242, loss: 0.4624
2023-11-24 22:35:53,461 - mmseg - INFO - Iter [300/10000]	lr: 9.757e-05, eta: 1:14:44, time: 0.487, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0132, decode.loss_dice: 0.4428, decode.acc_seg: 99.6028, loss: 0.4559
2023-11-24 22:36:15,362 - mmseg - INFO - Iter [350/10000]	lr: 9.717e-05, eta: 1:13:47, time: 0.438, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0107, decode.loss_dice: 0.4386, decode.acc_seg: 99.6231, loss: 0.4492
2023-11-24 22:36:39,789 - mmseg - INFO - Iter [400/10000]	lr: 9.676e-05, eta: 1:14:00, time: 0.489, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0088, decode.loss_dice: 0.4345, decode.acc_seg: 99.6106, loss: 0.4434
2023-11-24 22:37:01,636 - mmseg - INFO - Iter [450/10000]	lr: 9.635e-05, eta: 1:13:10, time: 0.437, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0075, decode.loss_dice: 0.4279, decode.acc_seg: 99.6107, loss: 0.4354
2023-11-24 22:37:26,128 - mmseg - INFO - Iter [500/10000]	lr: 9.595e-05, eta: 1:13:15, time: 0.490, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0061, decode.loss_dice: 0.4223, decode.acc_seg: 99.5115, loss: 0.4284
2023-11-24 22:37:49,289 - mmseg - INFO - per class results:
2023-11-24 22:37:49,290 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.85 | 70.52 | 73.18 | 70.42 | 99.89 | 99.89  |   99.85   | 99.93  | 99.83 |
|  scratch   | 72.94 | 97.43 | 75.68 |  82.9 | 44.41 | 62.66  |    62.4   | 62.94  | 43.99 |
|   stain    | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-24 22:37:49,291 - mmseg - INFO - Summary:
2023-11-24 22:37:49,291 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.85 | 78.38 | 91.99 | 74.43 | 76.66 | 52.74 |  81.27  |   81.12    |  68.49  | 52.62 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-24 22:37:49,309 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9985, mIoU: 0.7838, mVOE: 0.9199, mASD: 0.7443, mMSSD: 0.7666, mAcc: 0.5274, mFscore: 0.8127, mPrecision: 0.8112, mRecall: 0.6849, mDice: 0.5262, IoU.background: 0.9985, IoU.scratch: 0.7294, IoU.stain: 0.7037, IoU.edgeDamage: 0.7037, VOE.background: 0.7052, VOE.scratch: 0.9743, VOE.stain: 1.0000, VOE.edgeDamage: 1.0000, ASD.background: 0.7318, ASD.scratch: 0.7568, ASD.stain: nan, ASD.edgeDamage: nan, MSSD.background: 0.7042, MSSD.scratch: 0.8290, MSSD.stain: nan, MSSD.edgeDamage: nan, Acc.background: 0.9989, Acc.scratch: 0.4441, Acc.stain: 0.3333, Acc.edgeDamage: 0.3333, Fscore.background: 0.9989, Fscore.scratch: 0.6266, Fscore.stain: nan, Fscore.edgeDamage: nan, Precision.background: 0.9985, Precision.scratch: 0.6240, Precision.stain: nan, Precision.edgeDamage: nan, Recall.background: 0.9993, Recall.scratch: 0.6294, Recall.stain: 0.5556, Recall.edgeDamage: 0.5556, Dice.background: 0.9983, Dice.scratch: 0.4399, Dice.stain: 0.3333, Dice.edgeDamage: 0.3333
2023-11-24 22:38:10,968 - mmseg - INFO - Iter [550/10000]	lr: 9.554e-05, eta: 1:19:05, time: 0.897, data_time: 0.467, memory: 13650, decode.loss_focal: 0.0049, decode.loss_dice: 0.4120, decode.acc_seg: 99.1801, loss: 0.4169
2023-11-24 22:38:35,401 - mmseg - INFO - Iter [600/10000]	lr: 9.513e-05, eta: 1:18:29, time: 0.489, data_time: 0.057, memory: 13650, decode.loss_focal: 0.0040, decode.loss_dice: 0.3968, decode.acc_seg: 99.4564, loss: 0.4008
2023-11-24 22:38:57,153 - mmseg - INFO - Iter [650/10000]	lr: 9.473e-05, eta: 1:17:17, time: 0.435, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0033, decode.loss_dice: 0.3860, decode.acc_seg: 99.4349, loss: 0.3893
2023-11-24 22:39:21,514 - mmseg - INFO - Iter [700/10000]	lr: 9.432e-05, eta: 1:16:46, time: 0.487, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0028, decode.loss_dice: 0.3773, decode.acc_seg: 99.4808, loss: 0.3801
2023-11-24 22:39:43,294 - mmseg - INFO - Iter [750/10000]	lr: 9.391e-05, eta: 1:15:45, time: 0.436, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0025, decode.loss_dice: 0.3699, decode.acc_seg: 99.5196, loss: 0.3723
2023-11-24 22:40:07,666 - mmseg - INFO - Iter [800/10000]	lr: 9.350e-05, eta: 1:15:18, time: 0.487, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0022, decode.loss_dice: 0.3597, decode.acc_seg: 99.5356, loss: 0.3619
2023-11-24 22:40:32,096 - mmseg - INFO - Iter [850/10000]	lr: 9.309e-05, eta: 1:14:52, time: 0.489, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0020, decode.loss_dice: 0.3605, decode.acc_seg: 99.5683, loss: 0.3624
2023-11-24 22:40:53,898 - mmseg - INFO - Iter [900/10000]	lr: 9.268e-05, eta: 1:13:59, time: 0.436, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0019, decode.loss_dice: 0.3447, decode.acc_seg: 99.5479, loss: 0.3465
2023-11-24 22:41:18,480 - mmseg - INFO - Iter [950/10000]	lr: 9.228e-05, eta: 1:13:37, time: 0.492, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0017, decode.loss_dice: 0.3495, decode.acc_seg: 99.6107, loss: 0.3512
2023-11-24 22:41:40,377 - mmseg - INFO - Exp name: unet_all.py
2023-11-24 22:41:40,378 - mmseg - INFO - Iter [1000/10000]	lr: 9.187e-05, eta: 1:12:50, time: 0.438, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0016, decode.loss_dice: 0.3315, decode.acc_seg: 99.5697, loss: 0.3331
2023-11-24 22:42:03,979 - mmseg - INFO - per class results:
2023-11-24 22:42:03,981 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background |  99.9 | 70.47 | 73.18 | 70.41 | 99.97 | 99.92  |   99.87   | 99.98  | 99.89 |
|  scratch   | 76.98 | 93.39 | 75.37 | 81.37 | 50.51 | 71.76  |   83.26   | 67.01  | 57.64 |
|   stain    | 85.64 | 84.73 | 74.16 |  75.3 | 77.82 | 85.78  |   86.37   | 85.21  | 78.67 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-24 22:42:03,982 - mmseg - INFO - Summary:
2023-11-24 22:42:03,982 - mmseg - INFO - 
+------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.9 | 83.22 | 87.15 | 74.24 | 75.69 | 65.41 |  85.82  |   89.84    |  76.94  | 67.38 |
+------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-24 22:42:03,996 - mmseg - INFO - Exp name: unet_all.py
2023-11-24 22:42:03,996 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9990, mIoU: 0.8322, mVOE: 0.8715, mASD: 0.7424, mMSSD: 0.7569, mAcc: 0.6541, mFscore: 0.8582, mPrecision: 0.8984, mRecall: 0.7694, mDice: 0.6738, IoU.background: 0.9990, IoU.scratch: 0.7698, IoU.stain: 0.8564, IoU.edgeDamage: 0.7037, VOE.background: 0.7047, VOE.scratch: 0.9339, VOE.stain: 0.8473, VOE.edgeDamage: 1.0000, ASD.background: 0.7318, ASD.scratch: 0.7537, ASD.stain: 0.7416, ASD.edgeDamage: nan, MSSD.background: 0.7041, MSSD.scratch: 0.8137, MSSD.stain: 0.7530, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.5051, Acc.stain: 0.7782, Acc.edgeDamage: 0.3333, Fscore.background: 0.9992, Fscore.scratch: 0.7176, Fscore.stain: 0.8578, Fscore.edgeDamage: nan, Precision.background: 0.9987, Precision.scratch: 0.8326, Precision.stain: 0.8637, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.6701, Recall.stain: 0.8521, Recall.edgeDamage: 0.5556, Dice.background: 0.9989, Dice.scratch: 0.5764, Dice.stain: 0.7867, Dice.edgeDamage: 0.3333
2023-11-24 22:42:28,296 - mmseg - INFO - Iter [1050/10000]	lr: 9.146e-05, eta: 1:15:47, time: 0.958, data_time: 0.528, memory: 13650, decode.loss_focal: 0.0015, decode.loss_dice: 0.3325, decode.acc_seg: 99.5780, loss: 0.3340
2023-11-24 22:42:50,075 - mmseg - INFO - Iter [1100/10000]	lr: 9.105e-05, eta: 1:14:52, time: 0.436, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0014, decode.loss_dice: 0.3283, decode.acc_seg: 99.5645, loss: 0.3297
2023-11-24 22:43:14,443 - mmseg - INFO - Iter [1150/10000]	lr: 9.064e-05, eta: 1:14:20, time: 0.487, data_time: 0.055, memory: 13650, decode.loss_focal: 0.0014, decode.loss_dice: 0.3168, decode.acc_seg: 99.5980, loss: 0.3181
2023-11-24 22:43:36,276 - mmseg - INFO - Iter [1200/10000]	lr: 9.023e-05, eta: 1:13:30, time: 0.437, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0012, decode.loss_dice: 0.3192, decode.acc_seg: 99.6321, loss: 0.3204
2023-11-24 22:44:00,695 - mmseg - INFO - Iter [1250/10000]	lr: 8.982e-05, eta: 1:13:01, time: 0.488, data_time: 0.057, memory: 13650, decode.loss_focal: 0.0012, decode.loss_dice: 0.3097, decode.acc_seg: 99.6219, loss: 0.3109
2023-11-24 22:44:22,615 - mmseg - INFO - Iter [1300/10000]	lr: 8.941e-05, eta: 1:12:15, time: 0.438, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0012, decode.loss_dice: 0.3139, decode.acc_seg: 99.6255, loss: 0.3150
2023-11-24 22:44:47,067 - mmseg - INFO - Iter [1350/10000]	lr: 8.900e-05, eta: 1:11:47, time: 0.489, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.3013, decode.acc_seg: 99.6593, loss: 0.3024
2023-11-24 22:45:08,874 - mmseg - INFO - Iter [1400/10000]	lr: 8.858e-05, eta: 1:11:03, time: 0.436, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.2954, decode.acc_seg: 99.6389, loss: 0.2965
2023-11-24 22:45:33,283 - mmseg - INFO - Iter [1450/10000]	lr: 8.817e-05, eta: 1:10:36, time: 0.488, data_time: 0.057, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.2893, decode.acc_seg: 99.6718, loss: 0.2904
2023-11-24 22:45:55,188 - mmseg - INFO - Iter [1500/10000]	lr: 8.776e-05, eta: 1:09:55, time: 0.438, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0012, decode.loss_dice: 0.2927, decode.acc_seg: 99.6253, loss: 0.2938
2023-11-24 22:46:18,956 - mmseg - INFO - per class results:
2023-11-24 22:46:18,957 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background |  99.9 | 70.47 | 73.18 | 70.41 | 99.97 | 99.93  |   99.87   | 99.98  | 99.89 |
|  scratch   | 77.45 | 92.92 | 75.33 | 81.15 | 51.48 | 72.69  |   84.91   | 67.65  | 59.04 |
|   stain    | 86.78 | 83.59 | 74.04 | 74.72 | 80.44 | 87.24  |   87.52   | 86.96  | 80.86 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-24 22:46:18,957 - mmseg - INFO - Summary:
2023-11-24 22:46:18,957 - mmseg - INFO - 
+------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.9 | 83.62 | 86.75 | 74.18 | 75.43 | 66.31 |  86.62  |   90.77    |  77.54  | 68.28 |
+------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-24 22:46:18,973 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9990, mIoU: 0.8362, mVOE: 0.8675, mASD: 0.7418, mMSSD: 0.7543, mAcc: 0.6631, mFscore: 0.8662, mPrecision: 0.9077, mRecall: 0.7754, mDice: 0.6828, IoU.background: 0.9990, IoU.scratch: 0.7745, IoU.stain: 0.8678, IoU.edgeDamage: 0.7037, VOE.background: 0.7047, VOE.scratch: 0.9292, VOE.stain: 0.8359, VOE.edgeDamage: 1.0000, ASD.background: 0.7318, ASD.scratch: 0.7533, ASD.stain: 0.7404, ASD.edgeDamage: nan, MSSD.background: 0.7041, MSSD.scratch: 0.8115, MSSD.stain: 0.7472, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.5148, Acc.stain: 0.8044, Acc.edgeDamage: 0.3333, Fscore.background: 0.9993, Fscore.scratch: 0.7269, Fscore.stain: 0.8724, Fscore.edgeDamage: nan, Precision.background: 0.9987, Precision.scratch: 0.8491, Precision.stain: 0.8752, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.6765, Recall.stain: 0.8696, Recall.edgeDamage: 0.5556, Dice.background: 0.9989, Dice.scratch: 0.5904, Dice.stain: 0.8086, Dice.edgeDamage: 0.3333
2023-11-24 22:46:43,403 - mmseg - INFO - Iter [1550/10000]	lr: 8.735e-05, eta: 1:11:39, time: 0.964, data_time: 0.532, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.2882, decode.acc_seg: 99.6484, loss: 0.2893
2023-11-24 22:47:07,830 - mmseg - INFO - Iter [1600/10000]	lr: 8.694e-05, eta: 1:11:08, time: 0.489, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.2817, decode.acc_seg: 99.6608, loss: 0.2827
2023-11-24 22:47:29,617 - mmseg - INFO - Iter [1650/10000]	lr: 8.653e-05, eta: 1:10:24, time: 0.436, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0012, decode.loss_dice: 0.2804, decode.acc_seg: 99.6265, loss: 0.2816
2023-11-24 22:47:54,012 - mmseg - INFO - Iter [1700/10000]	lr: 8.611e-05, eta: 1:09:55, time: 0.488, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.2738, decode.acc_seg: 99.6761, loss: 0.2748
2023-11-24 22:48:15,809 - mmseg - INFO - Iter [1750/10000]	lr: 8.570e-05, eta: 1:09:13, time: 0.436, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0012, decode.loss_dice: 0.2849, decode.acc_seg: 99.6513, loss: 0.2861
2023-11-24 22:48:40,192 - mmseg - INFO - Iter [1800/10000]	lr: 8.529e-05, eta: 1:08:44, time: 0.488, data_time: 0.056, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.2727, decode.acc_seg: 99.6573, loss: 0.2738
2023-11-24 22:49:02,000 - mmseg - INFO - Iter [1850/10000]	lr: 8.487e-05, eta: 1:08:04, time: 0.436, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.2653, decode.acc_seg: 99.6666, loss: 0.2664
2023-11-24 22:49:26,502 - mmseg - INFO - Iter [1900/10000]	lr: 8.446e-05, eta: 1:07:37, time: 0.490, data_time: 0.058, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.2804, decode.acc_seg: 99.6674, loss: 0.2815
2023-11-24 22:49:48,388 - mmseg - INFO - Iter [1950/10000]	lr: 8.405e-05, eta: 1:06:59, time: 0.438, data_time: 0.004, memory: 13650, decode.loss_focal: 0.0012, decode.loss_dice: 0.2738, decode.acc_seg: 99.6466, loss: 0.2749
2023-11-24 22:50:13,030 - mmseg - INFO - Exp name: unet_all.py
2023-11-24 22:50:13,031 - mmseg - INFO - Iter [2000/10000]	lr: 8.363e-05, eta: 1:06:32, time: 0.493, data_time: 0.058, memory: 13650, decode.loss_focal: 0.0011, decode.loss_dice: 0.2656, decode.acc_seg: 99.6740, loss: 0.2667
2023-11-24 22:50:36,734 - mmseg - INFO - per class results:
2023-11-24 22:50:36,735 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.91 | 70.46 | 73.18 | 70.41 | 99.97 | 99.93  |   99.88   | 99.98  | 99.89 |
|  scratch   |  79.7 | 90.67 | 75.01 | 79.56 | 58.64 | 76.84  |   84.36   | 72.43  | 65.25 |
|   stain    | 88.07 |  82.3 | 74.14 | 75.23 | 78.15 | 88.79  |   93.01   | 85.43  | 83.19 |
| edgeDamage | 70.37 | 100.0 |  nan  |  nan  | 33.33 |  nan   |    nan    | 55.56  | 33.33 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-24 22:50:36,735 - mmseg - INFO - Summary:
2023-11-24 22:50:36,736 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.91 | 84.51 | 85.86 | 74.11 | 75.07 | 67.52 |  88.52  |   92.42    |  78.35  | 70.42 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-24 22:50:36,749 - mmseg - INFO - Exp name: unet_all.py
2023-11-24 22:50:36,749 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9991, mIoU: 0.8451, mVOE: 0.8586, mASD: 0.7411, mMSSD: 0.7507, mAcc: 0.6752, mFscore: 0.8852, mPrecision: 0.9242, mRecall: 0.7835, mDice: 0.7042, IoU.background: 0.9991, IoU.scratch: 0.7970, IoU.stain: 0.8807, IoU.edgeDamage: 0.7037, VOE.background: 0.7046, VOE.scratch: 0.9067, VOE.stain: 0.8230, VOE.edgeDamage: 1.0000, ASD.background: 0.7318, ASD.scratch: 0.7501, ASD.stain: 0.7414, ASD.edgeDamage: nan, MSSD.background: 0.7041, MSSD.scratch: 0.7956, MSSD.stain: 0.7523, MSSD.edgeDamage: nan, Acc.background: 0.9997, Acc.scratch: 0.5864, Acc.stain: 0.7815, Acc.edgeDamage: 0.3333, Fscore.background: 0.9993, Fscore.scratch: 0.7684, Fscore.stain: 0.8879, Fscore.edgeDamage: nan, Precision.background: 0.9988, Precision.scratch: 0.8436, Precision.stain: 0.9301, Precision.edgeDamage: nan, Recall.background: 0.9998, Recall.scratch: 0.7243, Recall.stain: 0.8543, Recall.edgeDamage: 0.5556, Dice.background: 0.9989, Dice.scratch: 0.6525, Dice.stain: 0.8319, Dice.edgeDamage: 0.3333
