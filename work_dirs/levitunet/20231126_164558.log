2023-11-26 16:45:58,253 - mmseg - INFO - Multi-processing start method is `None`
2023-11-26 16:45:58,254 - mmseg - INFO - OpenCV num_threads is `6
2023-11-26 16:45:58,301 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA RTX A4000
CUDA_HOME: /environment/miniconda3
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.11.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu113
OpenCV: 4.8.1
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.29.1+
------------------------------------------------------------

2023-11-26 16:45:58,301 - mmseg - INFO - Distributed training: False
2023-11-26 16:45:58,520 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
backbone_norm_cfg = dict(type='LN', requires_grad=True)
model = dict(
    type='EncoderDecoderFull',
    decode_head=dict(
        type='LevitUnet',
        loss_decode=[
            dict(
                type='FocalLoss',
                use_sigmoid=True,
                gamma=2.0,
                alpha=0.25,
                class_weight=[0.1, 0.5, 0.2, 0.2],
                loss_weight=2.0),
            dict(
                type='DiceLoss',
                loss_name='loss_dice',
                class_weight=[0.1, 0.5, 0.2, 0.2],
                loss_weight=2.0)
        ]))
train_cfg = dict()
test_cfg = dict(mode='whole')
dataset_type = 'MyDataset'
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(600, 600)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(512, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=False),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data_root = './datasets/'
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=2,
    train=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='train/images',
        ann_dir='train/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(600, 600)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(type='Normalize', mean=[0, 0, 0], std=[1, 1, 1], to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MyDataset',
        data_root='./datasets/',
        img_dir='test/images',
        ann_dir='test/labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TensorboardLoggerHook'),
        dict(type='TextLoggerHook', by_epoch=False)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = False
find_unused_parameters = True
optimizer = dict(type='Adam', lr=0.0001, betas=(0.9, 0.999))
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=1e-05, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=5000)
checkpoint_config = dict(by_epoch=False, save_optimizer=False, interval=5000)
evaluation = dict(interval=500, metric=['mIoU', 'mFscore', 'mDice'])
work_dir = './work_dirs/levitunet'
gpu_ids = [0]
auto_resume = False

2023-11-26 16:45:58,521 - mmseg - INFO - Set random seed to 152280384, deterministic: False
2023-11-26 16:46:00,807 - mmseg - INFO - initialize LevitUnet with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

decode_head.conv_seg.weight - torch.Size([2, 64, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([2]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.cnn_b1.0.c.weight - torch.Size([48, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b1.0.bn.weight - torch.Size([48]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b1.0.bn.bias - torch.Size([48]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b2.0.c.weight - torch.Size([96, 48, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b2.0.bn.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b2.0.bn.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b3.0.c.weight - torch.Size([192, 96, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b3.0.bn.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b3.0.bn.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b4.0.c.weight - torch.Size([384, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b4.0.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.cnn_b4.0.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_1.conv1.0.weight - torch.Size([512, 2048, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_1.conv1.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_1.conv1.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_1.conv2.0.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_1.conv2.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_1.conv2.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_2.conv1.0.weight - torch.Size([256, 704, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_2.conv1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_2.conv1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_2.conv2.0.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_2.conv2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_2.conv2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_3.conv1.0.weight - torch.Size([128, 352, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_3.conv1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_3.conv1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_3.conv2.0.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_3.conv2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.decoderBlock_3.conv2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.segmentation_head.0.weight - torch.Size([4, 176, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.segmentation_head.0.bias - torch.Size([4]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.0.m.attention_biases - torch.Size([6, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.0.m.qkv.c.weight - torch.Size([768, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.0.m.qkv.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.0.m.qkv.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.0.m.proj.1.c.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.0.m.proj.1.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.0.m.proj.1.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.1.m.0.c.weight - torch.Size([768, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.1.m.0.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.1.m.0.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.1.m.2.c.weight - torch.Size([384, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.1.m.2.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.1.m.2.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.2.m.attention_biases - torch.Size([6, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.2.m.qkv.c.weight - torch.Size([768, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.2.m.qkv.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.2.m.qkv.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.2.m.proj.1.c.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.2.m.proj.1.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.2.m.proj.1.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.3.m.0.c.weight - torch.Size([768, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.3.m.0.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.3.m.0.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.3.m.2.c.weight - torch.Size([384, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.3.m.2.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.3.m.2.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.4.m.attention_biases - torch.Size([6, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.4.m.qkv.c.weight - torch.Size([768, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.4.m.qkv.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.4.m.qkv.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.4.m.proj.1.c.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.4.m.proj.1.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.4.m.proj.1.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.5.m.0.c.weight - torch.Size([768, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.5.m.0.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.5.m.0.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.5.m.2.c.weight - torch.Size([384, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.5.m.2.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.5.m.2.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.6.m.attention_biases - torch.Size([6, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.6.m.qkv.c.weight - torch.Size([768, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.6.m.qkv.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.6.m.qkv.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.6.m.proj.1.c.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.6.m.proj.1.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.6.m.proj.1.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.7.m.0.c.weight - torch.Size([768, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.7.m.0.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.7.m.0.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.7.m.2.c.weight - torch.Size([384, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.7.m.2.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_1.7.m.2.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.attention_biases - torch.Size([12, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.kv.c.weight - torch.Size([1920, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.kv.bn.weight - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.kv.bn.bias - torch.Size([1920]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.q.1.c.weight - torch.Size([384, 384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.q.1.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.q.1.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.proj.1.c.weight - torch.Size([512, 1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.proj.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.8.proj.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.9.m.0.c.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.9.m.0.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.9.m.0.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.9.m.2.c.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.9.m.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.9.m.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.10.m.attention_biases - torch.Size([9, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.10.m.qkv.c.weight - torch.Size([1152, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.10.m.qkv.bn.weight - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.10.m.qkv.bn.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.10.m.proj.1.c.weight - torch.Size([512, 576]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.10.m.proj.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.10.m.proj.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.11.m.0.c.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.11.m.0.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.11.m.0.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.11.m.2.c.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.11.m.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.11.m.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.12.m.attention_biases - torch.Size([9, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.12.m.qkv.c.weight - torch.Size([1152, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.12.m.qkv.bn.weight - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.12.m.qkv.bn.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.12.m.proj.1.c.weight - torch.Size([512, 576]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.12.m.proj.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.12.m.proj.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.13.m.0.c.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.13.m.0.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.13.m.0.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.13.m.2.c.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.13.m.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.13.m.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.14.m.attention_biases - torch.Size([9, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.14.m.qkv.c.weight - torch.Size([1152, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.14.m.qkv.bn.weight - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.14.m.qkv.bn.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.14.m.proj.1.c.weight - torch.Size([512, 576]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.14.m.proj.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.14.m.proj.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.15.m.0.c.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.15.m.0.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.15.m.0.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.15.m.2.c.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.15.m.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.15.m.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.16.m.attention_biases - torch.Size([9, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.16.m.qkv.c.weight - torch.Size([1152, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.16.m.qkv.bn.weight - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.16.m.qkv.bn.bias - torch.Size([1152]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.16.m.proj.1.c.weight - torch.Size([512, 576]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.16.m.proj.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.16.m.proj.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.17.m.0.c.weight - torch.Size([1024, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.17.m.0.bn.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.17.m.0.bn.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.17.m.2.c.weight - torch.Size([512, 1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.17.m.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_2.17.m.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.attention_biases - torch.Size([16, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.kv.c.weight - torch.Size([2560, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.kv.bn.weight - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.kv.bn.bias - torch.Size([2560]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.q.1.c.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.q.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.q.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.proj.1.c.weight - torch.Size([768, 2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.proj.1.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.18.proj.1.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.19.m.0.c.weight - torch.Size([1536, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.19.m.0.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.19.m.0.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.19.m.2.c.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.19.m.2.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.19.m.2.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.20.m.attention_biases - torch.Size([12, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.20.m.qkv.c.weight - torch.Size([1536, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.20.m.qkv.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.20.m.qkv.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.20.m.proj.1.c.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.20.m.proj.1.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.20.m.proj.1.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.21.m.0.c.weight - torch.Size([1536, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.21.m.0.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.21.m.0.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.21.m.2.c.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.21.m.2.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.21.m.2.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.22.m.attention_biases - torch.Size([12, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.22.m.qkv.c.weight - torch.Size([1536, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.22.m.qkv.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.22.m.qkv.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.22.m.proj.1.c.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.22.m.proj.1.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.22.m.proj.1.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.23.m.0.c.weight - torch.Size([1536, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.23.m.0.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.23.m.0.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.23.m.2.c.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.23.m.2.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.23.m.2.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.24.m.attention_biases - torch.Size([12, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.24.m.qkv.c.weight - torch.Size([1536, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.24.m.qkv.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.24.m.qkv.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.24.m.proj.1.c.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.24.m.proj.1.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.24.m.proj.1.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.25.m.0.c.weight - torch.Size([1536, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.25.m.0.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.25.m.0.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.25.m.2.c.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.25.m.2.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.25.m.2.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.26.m.attention_biases - torch.Size([12, 64]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.26.m.qkv.c.weight - torch.Size([1536, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.26.m.qkv.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.26.m.qkv.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.26.m.proj.1.c.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.26.m.proj.1.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.26.m.proj.1.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.27.m.0.c.weight - torch.Size([1536, 768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.27.m.0.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.27.m.0.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.27.m.2.c.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.27.m.2.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  

decode_head.block_3.27.m.2.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoderFull  
2023-11-26 16:46:00,875 - mmseg - INFO - EncoderDecoderFull(
  (decode_head): LevitUnet(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): ModuleList(
      (0): FocalLoss()
      (1): DiceLoss()
    )
    (conv_seg): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (cnn_b1): Sequential(
      (0): Conv2d_BN(
        (c): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Hardswish()
    )
    (cnn_b2): Sequential(
      (0): Conv2d_BN(
        (c): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Hardswish()
    )
    (cnn_b3): Sequential(
      (0): Conv2d_BN(
        (c): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Hardswish()
    )
    (cnn_b4): Sequential(
      (0): Conv2d_BN(
        (c): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (decoderBlock_1): DecoderBlock(
      (conv1): Conv2dReLU(
        (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv2): Conv2dReLU(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)
    )
    (decoderBlock_2): DecoderBlock(
      (conv1): Conv2dReLU(
        (0): Conv2d(704, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv2): Conv2dReLU(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)
    )
    (decoderBlock_3): DecoderBlock(
      (conv1): Conv2dReLU(
        (0): Conv2d(352, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv2): Conv2dReLU(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)
    )
    (segmentation_head): SegmentationHead(
      (0): Conv2d(176, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)
    )
    (block_1): Sequential(
      (0): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=384, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=384, out_features=384, bias=False)
              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (1): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=384, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=768, out_features=384, bias=False)
            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (2): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=384, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=384, out_features=384, bias=False)
              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (3): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=384, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=768, out_features=384, bias=False)
            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (4): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=384, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=384, out_features=384, bias=False)
              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (5): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=384, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=768, out_features=384, bias=False)
            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (6): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=384, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=384, out_features=384, bias=False)
              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (7): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=384, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=768, out_features=384, bias=False)
            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (block_2): Sequential(
      (8): AttentionSubsample(
        (kv): Linear_BN(
          (c): Linear(in_features=384, out_features=1920, bias=False)
          (bn): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (q): Sequential(
          (0): Subsample()
          (1): Linear_BN(
            (c): Linear(in_features=384, out_features=384, bias=False)
            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (proj): Sequential(
          (0): Hardswish()
          (1): Linear_BN(
            (c): Linear(in_features=1536, out_features=512, bias=False)
            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (9): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=512, out_features=1024, bias=False)
            (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1024, out_features=512, bias=False)
            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (10): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=512, out_features=1152, bias=False)
            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=576, out_features=512, bias=False)
              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (11): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=512, out_features=1024, bias=False)
            (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1024, out_features=512, bias=False)
            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (12): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=512, out_features=1152, bias=False)
            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=576, out_features=512, bias=False)
              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (13): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=512, out_features=1024, bias=False)
            (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1024, out_features=512, bias=False)
            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (14): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=512, out_features=1152, bias=False)
            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=576, out_features=512, bias=False)
              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (15): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=512, out_features=1024, bias=False)
            (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1024, out_features=512, bias=False)
            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (16): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=512, out_features=1152, bias=False)
            (bn): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=576, out_features=512, bias=False)
              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (17): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=512, out_features=1024, bias=False)
            (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1024, out_features=512, bias=False)
            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (block_3): Sequential(
      (18): AttentionSubsample(
        (kv): Linear_BN(
          (c): Linear(in_features=512, out_features=2560, bias=False)
          (bn): BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (q): Sequential(
          (0): Subsample()
          (1): Linear_BN(
            (c): Linear(in_features=512, out_features=512, bias=False)
            (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (proj): Sequential(
          (0): Hardswish()
          (1): Linear_BN(
            (c): Linear(in_features=2048, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (19): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=768, out_features=1536, bias=False)
            (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1536, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (20): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=768, out_features=1536, bias=False)
            (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=768, out_features=768, bias=False)
              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (21): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=768, out_features=1536, bias=False)
            (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1536, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (22): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=768, out_features=1536, bias=False)
            (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=768, out_features=768, bias=False)
              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (23): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=768, out_features=1536, bias=False)
            (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1536, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (24): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=768, out_features=1536, bias=False)
            (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=768, out_features=768, bias=False)
              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (25): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=768, out_features=1536, bias=False)
            (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1536, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (26): Residual(
        (m): Attention(
          (qkv): Linear_BN(
            (c): Linear(in_features=768, out_features=1536, bias=False)
            (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (proj): Sequential(
            (0): Hardswish()
            (1): Linear_BN(
              (c): Linear(in_features=768, out_features=768, bias=False)
              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (27): Residual(
        (m): Sequential(
          (0): Linear_BN(
            (c): Linear(in_features=768, out_features=1536, bias=False)
            (bn): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): Hardswish()
          (2): Linear_BN(
            (c): Linear(in_features=1536, out_features=768, bias=False)
            (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2023-11-26 16:46:01,857 - mmseg - INFO - Loaded 474 images
2023-11-26 16:47:25,316 - mmseg - INFO - Loaded 105 images
2023-11-26 16:47:25,317 - mmseg - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/test/work_dirs/levitunet
2023-11-26 16:47:25,318 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardLoggerHook              
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-11-26 16:47:25,329 - mmseg - INFO - workflow: [('train', 1)], max: 5000 iters
2023-11-26 16:47:25,329 - mmseg - INFO - Checkpoints will be saved to /home/featurize/work/test/work_dirs/levitunet by HardDiskBackend.
2023-11-26 16:48:18,284 - mmseg - INFO - Iter [50/5000]	lr: 9.921e-05, eta: 1:20:19, time: 0.974, data_time: 0.348, memory: 8488, decode.loss_focal: 0.0102, decode.loss_dice: 0.4339, decode.acc_seg: 97.0774, loss: 0.4441
2023-11-26 16:48:51,119 - mmseg - INFO - Iter [100/5000]	lr: 9.839e-05, eta: 1:06:34, time: 0.657, data_time: 0.106, memory: 8488, decode.loss_focal: 0.0019, decode.loss_dice: 0.4012, decode.acc_seg: 98.8360, loss: 0.4031
2023-11-26 16:49:22,612 - mmseg - INFO - Iter [150/5000]	lr: 9.758e-05, eta: 1:00:54, time: 0.630, data_time: 0.075, memory: 8488, decode.loss_focal: 0.0016, decode.loss_dice: 0.3753, decode.acc_seg: 98.9847, loss: 0.3770
2023-11-26 16:49:53,927 - mmseg - INFO - Iter [200/5000]	lr: 9.677e-05, eta: 0:57:43, time: 0.626, data_time: 0.069, memory: 8488, decode.loss_focal: 0.0016, decode.loss_dice: 0.3547, decode.acc_seg: 98.9905, loss: 0.3563
2023-11-26 16:50:25,185 - mmseg - INFO - Iter [250/5000]	lr: 9.596e-05, eta: 0:55:36, time: 0.625, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0014, decode.loss_dice: 0.3525, decode.acc_seg: 98.9853, loss: 0.3539
2023-11-26 16:50:56,356 - mmseg - INFO - Iter [300/5000]	lr: 9.514e-05, eta: 0:53:59, time: 0.623, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0011, decode.loss_dice: 0.3386, decode.acc_seg: 99.0622, loss: 0.3398
2023-11-26 16:51:24,722 - mmseg - INFO - Iter [350/5000]	lr: 9.433e-05, eta: 0:52:03, time: 0.567, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0011, decode.loss_dice: 0.3348, decode.acc_seg: 99.0737, loss: 0.3359
2023-11-26 16:51:55,934 - mmseg - INFO - Iter [400/5000]	lr: 9.351e-05, eta: 0:51:02, time: 0.624, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0010, decode.loss_dice: 0.3287, decode.acc_seg: 99.0894, loss: 0.3297
2023-11-26 16:52:27,435 - mmseg - INFO - Iter [450/5000]	lr: 9.269e-05, eta: 0:50:11, time: 0.630, data_time: 0.070, memory: 8488, decode.loss_focal: 0.0010, decode.loss_dice: 0.3246, decode.acc_seg: 99.1654, loss: 0.3256
2023-11-26 16:52:58,856 - mmseg - INFO - Iter [500/5000]	lr: 9.187e-05, eta: 0:49:23, time: 0.628, data_time: 0.068, memory: 8488, decode.loss_focal: 0.0011, decode.loss_dice: 0.3283, decode.acc_seg: 99.0760, loss: 0.3294
2023-11-26 16:53:24,075 - mmseg - INFO - per class results:
2023-11-26 16:53:24,077 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.77 |  70.6 | 71.01 | 70.45 | 99.64 | 99.83  |    99.9   | 99.76  | 99.75 |
|  scratch   | 73.24 | 97.13 |  73.6 | 83.37 | 54.46 |  63.4  |   60.99   | 69.64  |  45.1 |
|   stain    | 82.79 | 87.58 | 72.59 | 78.34 | 64.12 |  81.8  |   91.97   | 76.08  | 72.71 |
| edgeDamage | 79.75 | 90.62 | 72.75 | 79.11 | 72.09 | 76.93  |   73.79   | 81.39  |  65.4 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 16:53:24,077 - mmseg - INFO - Summary:
2023-11-26 16:53:24,077 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.77 | 83.89 | 86.48 | 72.49 | 77.82 | 72.58 |  80.49  |   81.66    |  81.72  | 70.74 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-26 16:53:24,101 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9977, mIoU: 0.8389, mVOE: 0.8648, mASD: 0.7249, mMSSD: 0.7782, mAcc: 0.7258, mFscore: 0.8049, mPrecision: 0.8166, mRecall: 0.8172, mDice: 0.7074, IoU.background: 0.9977, IoU.scratch: 0.7324, IoU.stain: 0.8279, IoU.edgeDamage: 0.7975, VOE.background: 0.7060, VOE.scratch: 0.9713, VOE.stain: 0.8758, VOE.edgeDamage: 0.9062, ASD.background: 0.7101, ASD.scratch: 0.7360, ASD.stain: 0.7259, ASD.edgeDamage: 0.7275, MSSD.background: 0.7045, MSSD.scratch: 0.8337, MSSD.stain: 0.7834, MSSD.edgeDamage: 0.7911, Acc.background: 0.9964, Acc.scratch: 0.5446, Acc.stain: 0.6412, Acc.edgeDamage: 0.7209, Fscore.background: 0.9983, Fscore.scratch: 0.6340, Fscore.stain: 0.8180, Fscore.edgeDamage: 0.7693, Precision.background: 0.9990, Precision.scratch: 0.6099, Precision.stain: 0.9197, Precision.edgeDamage: 0.7379, Recall.background: 0.9976, Recall.scratch: 0.6964, Recall.stain: 0.7608, Recall.edgeDamage: 0.8139, Dice.background: 0.9975, Dice.scratch: 0.4510, Dice.stain: 0.7271, Dice.edgeDamage: 0.6540
2023-11-26 16:53:55,030 - mmseg - INFO - Iter [550/5000]	lr: 9.106e-05, eta: 0:51:58, time: 1.123, data_time: 0.570, memory: 8488, decode.loss_focal: 0.0010, decode.loss_dice: 0.3116, decode.acc_seg: 99.1525, loss: 0.3126
2023-11-26 16:54:26,390 - mmseg - INFO - Iter [600/5000]	lr: 9.024e-05, eta: 0:50:56, time: 0.627, data_time: 0.069, memory: 8488, decode.loss_focal: 0.0010, decode.loss_dice: 0.3205, decode.acc_seg: 99.1422, loss: 0.3214
2023-11-26 16:54:57,847 - mmseg - INFO - Iter [650/5000]	lr: 8.941e-05, eta: 0:49:59, time: 0.629, data_time: 0.068, memory: 8488, decode.loss_focal: 0.0010, decode.loss_dice: 0.3141, decode.acc_seg: 99.1375, loss: 0.3151
2023-11-26 16:55:26,249 - mmseg - INFO - Iter [700/5000]	lr: 8.859e-05, eta: 0:48:47, time: 0.568, data_time: 0.006, memory: 8488, decode.loss_focal: 0.0010, decode.loss_dice: 0.3091, decode.acc_seg: 99.1748, loss: 0.3101
2023-11-26 16:55:57,771 - mmseg - INFO - Iter [750/5000]	lr: 8.777e-05, eta: 0:47:59, time: 0.630, data_time: 0.069, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.3114, decode.acc_seg: 99.2373, loss: 0.3124
2023-11-26 16:56:29,185 - mmseg - INFO - Iter [800/5000]	lr: 8.695e-05, eta: 0:47:12, time: 0.628, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.3057, decode.acc_seg: 99.1659, loss: 0.3066
2023-11-26 16:57:00,575 - mmseg - INFO - Iter [850/5000]	lr: 8.612e-05, eta: 0:46:27, time: 0.628, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2954, decode.acc_seg: 99.2413, loss: 0.2963
2023-11-26 16:57:31,980 - mmseg - INFO - Iter [900/5000]	lr: 8.530e-05, eta: 0:45:44, time: 0.628, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2957, decode.acc_seg: 99.1918, loss: 0.2966
2023-11-26 16:58:03,335 - mmseg - INFO - Iter [950/5000]	lr: 8.447e-05, eta: 0:45:01, time: 0.627, data_time: 0.064, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.3016, decode.acc_seg: 99.2623, loss: 0.3026
2023-11-26 16:58:31,830 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 16:58:31,830 - mmseg - INFO - Iter [1000/5000]	lr: 8.364e-05, eta: 0:44:08, time: 0.570, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2979, decode.acc_seg: 99.2564, loss: 0.2988
2023-11-26 16:58:55,683 - mmseg - INFO - per class results:
2023-11-26 16:58:55,684 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.44 | 70.93 | 71.05 | 70.62 | 98.86 | 99.57  |   99.91   | 99.24  | 99.36 |
|  scratch   | 75.03 | 95.34 | 73.45 | 82.63 | 44.82 | 67.62  |   84.05   | 63.21  | 51.44 |
|   stain    | 85.36 | 85.01 | 72.04 | 75.57 | 76.59 | 85.42  |   86.52   | 84.39  | 78.12 |
| edgeDamage | 72.22 | 98.15 | 73.77 | 84.25 | 90.92 | 60.79  |   58.36   | 93.95  | 41.18 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 16:58:55,684 - mmseg - INFO - Summary:
2023-11-26 16:58:55,685 - mmseg - INFO - 
+-------+-------+-------+-------+-------+------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD | mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+------+---------+------------+---------+-------+
| 99.44 | 83.01 | 87.36 | 72.58 | 78.27 | 77.8 |  78.35  |   82.21    |   85.2  | 67.53 |
+-------+-------+-------+-------+-------+------+---------+------------+---------+-------+
2023-11-26 16:58:55,700 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 16:58:55,700 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9944, mIoU: 0.8301, mVOE: 0.8736, mASD: 0.7258, mMSSD: 0.7827, mAcc: 0.7780, mFscore: 0.7835, mPrecision: 0.8221, mRecall: 0.8520, mDice: 0.6753, IoU.background: 0.9944, IoU.scratch: 0.7503, IoU.stain: 0.8536, IoU.edgeDamage: 0.7222, VOE.background: 0.7093, VOE.scratch: 0.9534, VOE.stain: 0.8501, VOE.edgeDamage: 0.9815, ASD.background: 0.7105, ASD.scratch: 0.7345, ASD.stain: 0.7204, ASD.edgeDamage: 0.7377, MSSD.background: 0.7062, MSSD.scratch: 0.8263, MSSD.stain: 0.7557, MSSD.edgeDamage: 0.8425, Acc.background: 0.9886, Acc.scratch: 0.4482, Acc.stain: 0.7659, Acc.edgeDamage: 0.9092, Fscore.background: 0.9957, Fscore.scratch: 0.6762, Fscore.stain: 0.8542, Fscore.edgeDamage: 0.6079, Precision.background: 0.9991, Precision.scratch: 0.8405, Precision.stain: 0.8652, Precision.edgeDamage: 0.5836, Recall.background: 0.9924, Recall.scratch: 0.6321, Recall.stain: 0.8439, Recall.edgeDamage: 0.9395, Dice.background: 0.9936, Dice.scratch: 0.5144, Dice.stain: 0.7812, Dice.edgeDamage: 0.4118
2023-11-26 16:59:26,680 - mmseg - INFO - Iter [1050/5000]	lr: 8.281e-05, eta: 0:44:57, time: 1.097, data_time: 0.543, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2923, decode.acc_seg: 99.1467, loss: 0.2932
2023-11-26 16:59:58,049 - mmseg - INFO - Iter [1100/5000]	lr: 8.198e-05, eta: 0:44:13, time: 0.627, data_time: 0.068, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2924, decode.acc_seg: 99.3505, loss: 0.2932
2023-11-26 17:00:29,337 - mmseg - INFO - Iter [1150/5000]	lr: 8.115e-05, eta: 0:43:30, time: 0.626, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2938, decode.acc_seg: 99.2668, loss: 0.2948
2023-11-26 17:01:00,665 - mmseg - INFO - Iter [1200/5000]	lr: 8.032e-05, eta: 0:42:48, time: 0.627, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2888, decode.acc_seg: 99.2810, loss: 0.2896
2023-11-26 17:01:31,980 - mmseg - INFO - Iter [1250/5000]	lr: 7.949e-05, eta: 0:42:07, time: 0.626, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2874, decode.acc_seg: 99.3260, loss: 0.2883
2023-11-26 17:02:03,326 - mmseg - INFO - Iter [1300/5000]	lr: 7.865e-05, eta: 0:41:26, time: 0.627, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2805, decode.acc_seg: 99.2416, loss: 0.2813
2023-11-26 17:02:31,718 - mmseg - INFO - Iter [1350/5000]	lr: 7.782e-05, eta: 0:40:39, time: 0.568, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2840, decode.acc_seg: 99.2794, loss: 0.2849
2023-11-26 17:03:03,029 - mmseg - INFO - Iter [1400/5000]	lr: 7.698e-05, eta: 0:40:00, time: 0.626, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2807, decode.acc_seg: 99.2833, loss: 0.2815
2023-11-26 17:03:34,170 - mmseg - INFO - Iter [1450/5000]	lr: 7.614e-05, eta: 0:39:21, time: 0.623, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2820, decode.acc_seg: 99.3075, loss: 0.2829
2023-11-26 17:04:05,495 - mmseg - INFO - Iter [1500/5000]	lr: 7.530e-05, eta: 0:38:43, time: 0.627, data_time: 0.068, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2815, decode.acc_seg: 99.2694, loss: 0.2824
2023-11-26 17:04:28,817 - mmseg - INFO - per class results:
2023-11-26 17:04:28,818 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.79 | 70.58 | 71.01 | 70.45 | 99.63 | 99.84  |   99.93   | 99.75  | 99.76 |
|  scratch   | 76.86 | 93.51 | 72.96 |  80.2 | 55.75 | 71.53  |   72.72   |  70.5  |  57.3 |
|   stain    | 86.97 |  83.4 | 72.17 | 76.21 | 73.74 | 87.47  |   94.71   | 82.49  | 81.21 |
| edgeDamage | 76.14 | 94.23 | 73.37 | 82.21 | 91.21 | 70.04  |   64.47   | 94.14  | 55.06 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 17:04:28,818 - mmseg - INFO - Summary:
2023-11-26 17:04:28,819 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.79 | 84.94 | 85.43 | 72.38 | 77.27 | 80.08 |  82.22  |   82.96    |  86.72  | 73.33 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-26 17:04:28,833 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9979, mIoU: 0.8494, mVOE: 0.8543, mASD: 0.7238, mMSSD: 0.7727, mAcc: 0.8008, mFscore: 0.8222, mPrecision: 0.8296, mRecall: 0.8672, mDice: 0.7333, IoU.background: 0.9979, IoU.scratch: 0.7686, IoU.stain: 0.8697, IoU.edgeDamage: 0.7614, VOE.background: 0.7058, VOE.scratch: 0.9351, VOE.stain: 0.8340, VOE.edgeDamage: 0.9423, ASD.background: 0.7101, ASD.scratch: 0.7296, ASD.stain: 0.7217, ASD.edgeDamage: 0.7337, MSSD.background: 0.7045, MSSD.scratch: 0.8020, MSSD.stain: 0.7621, MSSD.edgeDamage: 0.8221, Acc.background: 0.9963, Acc.scratch: 0.5575, Acc.stain: 0.7374, Acc.edgeDamage: 0.9121, Fscore.background: 0.9984, Fscore.scratch: 0.7153, Fscore.stain: 0.8747, Fscore.edgeDamage: 0.7004, Precision.background: 0.9993, Precision.scratch: 0.7272, Precision.stain: 0.9471, Precision.edgeDamage: 0.6447, Recall.background: 0.9975, Recall.scratch: 0.7050, Recall.stain: 0.8249, Recall.edgeDamage: 0.9414, Dice.background: 0.9976, Dice.scratch: 0.5730, Dice.stain: 0.8121, Dice.edgeDamage: 0.5506
2023-11-26 17:04:59,768 - mmseg - INFO - Iter [1550/5000]	lr: 7.446e-05, eta: 0:38:57, time: 1.085, data_time: 0.535, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2738, decode.acc_seg: 99.3701, loss: 0.2746
2023-11-26 17:05:30,799 - mmseg - INFO - Iter [1600/5000]	lr: 7.362e-05, eta: 0:38:17, time: 0.621, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2754, decode.acc_seg: 99.3623, loss: 0.2762
2023-11-26 17:05:59,040 - mmseg - INFO - Iter [1650/5000]	lr: 7.278e-05, eta: 0:37:32, time: 0.565, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2782, decode.acc_seg: 99.3099, loss: 0.2791
2023-11-26 17:06:30,207 - mmseg - INFO - Iter [1700/5000]	lr: 7.194e-05, eta: 0:36:54, time: 0.623, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2705, decode.acc_seg: 99.3779, loss: 0.2713
2023-11-26 17:07:01,417 - mmseg - INFO - Iter [1750/5000]	lr: 7.109e-05, eta: 0:36:16, time: 0.624, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2710, decode.acc_seg: 99.3345, loss: 0.2718
2023-11-26 17:07:32,580 - mmseg - INFO - Iter [1800/5000]	lr: 7.025e-05, eta: 0:35:38, time: 0.623, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2706, decode.acc_seg: 99.3464, loss: 0.2714
2023-11-26 17:08:03,910 - mmseg - INFO - Iter [1850/5000]	lr: 6.940e-05, eta: 0:35:01, time: 0.627, data_time: 0.068, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2699, decode.acc_seg: 99.3281, loss: 0.2707
2023-11-26 17:08:35,214 - mmseg - INFO - Iter [1900/5000]	lr: 6.855e-05, eta: 0:34:24, time: 0.626, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2637, decode.acc_seg: 99.3795, loss: 0.2644
2023-11-26 17:09:06,653 - mmseg - INFO - Iter [1950/5000]	lr: 6.770e-05, eta: 0:33:48, time: 0.629, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2651, decode.acc_seg: 99.3838, loss: 0.2659
2023-11-26 17:09:35,050 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 17:09:35,050 - mmseg - INFO - Iter [2000/5000]	lr: 6.685e-05, eta: 0:33:08, time: 0.568, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2711, decode.acc_seg: 99.3469, loss: 0.2719
2023-11-26 17:09:59,464 - mmseg - INFO - per class results:
2023-11-26 17:09:59,465 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.83 | 70.54 | 71.01 | 70.43 | 99.74 | 99.87  |   99.92   | 99.83  | 99.81 |
|  scratch   |  77.3 | 93.07 | 73.14 | 81.09 | 51.77 |  72.4  |   82.34   | 67.85  |  58.6 |
|   stain    | 87.04 | 83.33 | 71.96 | 75.19 | 78.31 | 87.56  |   89.86   | 85.54  | 81.34 |
| edgeDamage | 77.18 | 93.19 | 73.25 | 81.63 | 89.42 | 72.17  |   66.24   | 92.95  | 58.25 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 17:09:59,465 - mmseg - INFO - Summary:
2023-11-26 17:09:59,466 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.83 | 85.34 | 85.03 | 72.34 | 77.08 | 79.81 |   83.0  |   84.59    |  86.54  |  74.5 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-26 17:09:59,483 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 17:09:59,483 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9983, mIoU: 0.8534, mVOE: 0.8503, mASD: 0.7234, mMSSD: 0.7708, mAcc: 0.7981, mFscore: 0.8300, mPrecision: 0.8459, mRecall: 0.8654, mDice: 0.7450, IoU.background: 0.9983, IoU.scratch: 0.7730, IoU.stain: 0.8704, IoU.edgeDamage: 0.7718, VOE.background: 0.7054, VOE.scratch: 0.9307, VOE.stain: 0.8333, VOE.edgeDamage: 0.9319, ASD.background: 0.7101, ASD.scratch: 0.7314, ASD.stain: 0.7196, ASD.edgeDamage: 0.7325, MSSD.background: 0.7043, MSSD.scratch: 0.8109, MSSD.stain: 0.7519, MSSD.edgeDamage: 0.8163, Acc.background: 0.9974, Acc.scratch: 0.5177, Acc.stain: 0.7831, Acc.edgeDamage: 0.8942, Fscore.background: 0.9987, Fscore.scratch: 0.7240, Fscore.stain: 0.8756, Fscore.edgeDamage: 0.7217, Precision.background: 0.9992, Precision.scratch: 0.8234, Precision.stain: 0.8986, Precision.edgeDamage: 0.6624, Recall.background: 0.9983, Recall.scratch: 0.6785, Recall.stain: 0.8554, Recall.edgeDamage: 0.9295, Dice.background: 0.9981, Dice.scratch: 0.5860, Dice.stain: 0.8134, Dice.edgeDamage: 0.5825
2023-11-26 17:10:30,487 - mmseg - INFO - Iter [2050/5000]	lr: 6.599e-05, eta: 0:33:07, time: 1.109, data_time: 0.556, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2642, decode.acc_seg: 99.3607, loss: 0.2650
2023-11-26 17:11:01,684 - mmseg - INFO - Iter [2100/5000]	lr: 6.514e-05, eta: 0:32:29, time: 0.624, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0009, decode.loss_dice: 0.2636, decode.acc_seg: 99.3405, loss: 0.2645
2023-11-26 17:11:32,993 - mmseg - INFO - Iter [2150/5000]	lr: 6.428e-05, eta: 0:31:53, time: 0.626, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2668, decode.acc_seg: 99.4706, loss: 0.2675
2023-11-26 17:12:04,659 - mmseg - INFO - Iter [2200/5000]	lr: 6.343e-05, eta: 0:31:17, time: 0.633, data_time: 0.070, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2560, decode.acc_seg: 99.3836, loss: 0.2567
2023-11-26 17:12:36,144 - mmseg - INFO - Iter [2250/5000]	lr: 6.257e-05, eta: 0:30:41, time: 0.630, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2590, decode.acc_seg: 99.3548, loss: 0.2598
2023-11-26 17:13:04,666 - mmseg - INFO - Iter [2300/5000]	lr: 6.171e-05, eta: 0:30:01, time: 0.570, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2642, decode.acc_seg: 99.4205, loss: 0.2650
2023-11-26 17:13:36,090 - mmseg - INFO - Iter [2350/5000]	lr: 6.084e-05, eta: 0:29:26, time: 0.628, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2570, decode.acc_seg: 99.3948, loss: 0.2578
2023-11-26 17:14:07,575 - mmseg - INFO - Iter [2400/5000]	lr: 5.998e-05, eta: 0:28:51, time: 0.630, data_time: 0.068, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2551, decode.acc_seg: 99.4335, loss: 0.2559
2023-11-26 17:14:39,039 - mmseg - INFO - Iter [2450/5000]	lr: 5.911e-05, eta: 0:28:15, time: 0.629, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2542, decode.acc_seg: 99.4022, loss: 0.2550
2023-11-26 17:15:10,551 - mmseg - INFO - Iter [2500/5000]	lr: 5.825e-05, eta: 0:27:40, time: 0.630, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2419, decode.acc_seg: 99.4283, loss: 0.2426
2023-11-26 17:15:34,043 - mmseg - INFO - per class results:
2023-11-26 17:15:34,044 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background |  99.8 | 70.57 | 71.01 | 70.45 | 99.66 | 99.85  |   99.93   | 99.77  | 99.78 |
|  scratch   |  78.0 | 92.37 | 72.98 | 80.28 | 55.42 | 73.75  |   79.38   | 70.28  | 60.63 |
|   stain    | 85.23 | 85.14 |  72.0 |  75.4 | 78.36 | 85.24  |   84.92   | 85.57  | 77.86 |
| edgeDamage | 76.29 | 94.08 | 73.35 | 82.13 | 90.77 | 70.35  |   64.73   | 93.84  | 55.53 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 17:15:34,044 - mmseg - INFO - Summary:
2023-11-26 17:15:34,044 - mmseg - INFO - 
+------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.8 | 84.83 | 85.54 | 72.34 | 77.06 | 81.05 |   82.3  |   82.24    |  87.37  | 73.45 |
+------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-26 17:15:34,060 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9980, mIoU: 0.8483, mVOE: 0.8554, mASD: 0.7234, mMSSD: 0.7706, mAcc: 0.8105, mFscore: 0.8230, mPrecision: 0.8224, mRecall: 0.8737, mDice: 0.7345, IoU.background: 0.9980, IoU.scratch: 0.7800, IoU.stain: 0.8523, IoU.edgeDamage: 0.7629, VOE.background: 0.7057, VOE.scratch: 0.9237, VOE.stain: 0.8514, VOE.edgeDamage: 0.9408, ASD.background: 0.7101, ASD.scratch: 0.7298, ASD.stain: 0.7200, ASD.edgeDamage: 0.7335, MSSD.background: 0.7045, MSSD.scratch: 0.8028, MSSD.stain: 0.7540, MSSD.edgeDamage: 0.8213, Acc.background: 0.9966, Acc.scratch: 0.5542, Acc.stain: 0.7836, Acc.edgeDamage: 0.9077, Fscore.background: 0.9985, Fscore.scratch: 0.7375, Fscore.stain: 0.8524, Fscore.edgeDamage: 0.7035, Precision.background: 0.9993, Precision.scratch: 0.7938, Precision.stain: 0.8492, Precision.edgeDamage: 0.6473, Recall.background: 0.9977, Recall.scratch: 0.7028, Recall.stain: 0.8557, Recall.edgeDamage: 0.9384, Dice.background: 0.9978, Dice.scratch: 0.6063, Dice.stain: 0.7786, Dice.edgeDamage: 0.5553
2023-11-26 17:16:05,116 - mmseg - INFO - Iter [2550/5000]	lr: 5.738e-05, eta: 0:27:28, time: 1.091, data_time: 0.538, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2529, decode.acc_seg: 99.4196, loss: 0.2537
2023-11-26 17:16:36,365 - mmseg - INFO - Iter [2600/5000]	lr: 5.651e-05, eta: 0:26:52, time: 0.625, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2510, decode.acc_seg: 99.4596, loss: 0.2517
2023-11-26 17:17:04,745 - mmseg - INFO - Iter [2650/5000]	lr: 5.563e-05, eta: 0:26:14, time: 0.568, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2564, decode.acc_seg: 99.3842, loss: 0.2572
2023-11-26 17:17:36,077 - mmseg - INFO - Iter [2700/5000]	lr: 5.476e-05, eta: 0:25:38, time: 0.627, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2500, decode.acc_seg: 99.4431, loss: 0.2508
2023-11-26 17:18:07,454 - mmseg - INFO - Iter [2750/5000]	lr: 5.388e-05, eta: 0:25:03, time: 0.628, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2512, decode.acc_seg: 99.4427, loss: 0.2519
2023-11-26 17:18:38,702 - mmseg - INFO - Iter [2800/5000]	lr: 5.301e-05, eta: 0:24:28, time: 0.625, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2433, decode.acc_seg: 99.4615, loss: 0.2440
2023-11-26 17:19:09,878 - mmseg - INFO - Iter [2850/5000]	lr: 5.213e-05, eta: 0:23:53, time: 0.624, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2423, decode.acc_seg: 99.5117, loss: 0.2430
2023-11-26 17:19:41,126 - mmseg - INFO - Iter [2900/5000]	lr: 5.124e-05, eta: 0:23:18, time: 0.625, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2478, decode.acc_seg: 99.4145, loss: 0.2486
2023-11-26 17:20:09,395 - mmseg - INFO - Iter [2950/5000]	lr: 5.036e-05, eta: 0:22:41, time: 0.565, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2420, decode.acc_seg: 99.4636, loss: 0.2427
2023-11-26 17:20:40,593 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 17:20:40,593 - mmseg - INFO - Iter [3000/5000]	lr: 4.947e-05, eta: 0:22:07, time: 0.624, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2432, decode.acc_seg: 99.4578, loss: 0.2440
2023-11-26 17:21:03,102 - mmseg - INFO - per class results:
2023-11-26 17:21:03,103 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.87 |  70.5 | 71.01 | 70.41 | 99.82 |  99.9  |   99.93   | 99.88  | 99.85 |
|  scratch   | 78.46 | 91.91 | 72.98 | 80.26 | 55.49 | 74.61  |    82.4   | 70.33  | 61.92 |
|   stain    |  84.0 | 86.37 | 72.32 | 76.96 |  81.9 | 83.57  |   80.24   | 87.93  | 75.35 |
| edgeDamage | 80.16 | 90.21 | 72.91 | 79.95 | 89.02 | 77.64  |   71.27   | 92.68  | 66.45 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 17:21:03,103 - mmseg - INFO - Summary:
2023-11-26 17:21:03,104 - mmseg - INFO - 
+-------+-------+-------+------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE | mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+------+-------+-------+---------+------------+---------+-------+
| 99.87 | 85.62 | 84.75 | 72.3 | 76.89 | 81.56 |  83.93  |   83.46    |   87.7  | 75.89 |
+-------+-------+-------+------+-------+-------+---------+------------+---------+-------+
2023-11-26 17:21:03,118 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 17:21:03,119 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9987, mIoU: 0.8562, mVOE: 0.8475, mASD: 0.7230, mMSSD: 0.7689, mAcc: 0.8156, mFscore: 0.8393, mPrecision: 0.8346, mRecall: 0.8770, mDice: 0.7589, IoU.background: 0.9987, IoU.scratch: 0.7846, IoU.stain: 0.8400, IoU.edgeDamage: 0.8016, VOE.background: 0.7050, VOE.scratch: 0.9191, VOE.stain: 0.8637, VOE.edgeDamage: 0.9021, ASD.background: 0.7101, ASD.scratch: 0.7298, ASD.stain: 0.7232, ASD.edgeDamage: 0.7291, MSSD.background: 0.7041, MSSD.scratch: 0.8026, MSSD.stain: 0.7696, MSSD.edgeDamage: 0.7995, Acc.background: 0.9982, Acc.scratch: 0.5549, Acc.stain: 0.8190, Acc.edgeDamage: 0.8902, Fscore.background: 0.9990, Fscore.scratch: 0.7461, Fscore.stain: 0.8357, Fscore.edgeDamage: 0.7764, Precision.background: 0.9993, Precision.scratch: 0.8240, Precision.stain: 0.8024, Precision.edgeDamage: 0.7127, Recall.background: 0.9988, Recall.scratch: 0.7033, Recall.stain: 0.8793, Recall.edgeDamage: 0.9268, Dice.background: 0.9985, Dice.scratch: 0.6192, Dice.stain: 0.7535, Dice.edgeDamage: 0.6645
2023-11-26 17:21:34,087 - mmseg - INFO - Iter [3050/5000]	lr: 4.858e-05, eta: 0:21:47, time: 1.070, data_time: 0.519, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2411, decode.acc_seg: 99.4668, loss: 0.2418
2023-11-26 17:22:05,156 - mmseg - INFO - Iter [3100/5000]	lr: 4.769e-05, eta: 0:21:12, time: 0.621, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2393, decode.acc_seg: 99.4619, loss: 0.2400
2023-11-26 17:22:36,407 - mmseg - INFO - Iter [3150/5000]	lr: 4.680e-05, eta: 0:20:37, time: 0.625, data_time: 0.068, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2354, decode.acc_seg: 99.4886, loss: 0.2360
2023-11-26 17:23:07,786 - mmseg - INFO - Iter [3200/5000]	lr: 4.590e-05, eta: 0:20:02, time: 0.628, data_time: 0.068, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2355, decode.acc_seg: 99.4958, loss: 0.2362
2023-11-26 17:23:39,085 - mmseg - INFO - Iter [3250/5000]	lr: 4.500e-05, eta: 0:19:28, time: 0.626, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2376, decode.acc_seg: 99.4870, loss: 0.2383
2023-11-26 17:24:07,410 - mmseg - INFO - Iter [3300/5000]	lr: 4.410e-05, eta: 0:18:52, time: 0.566, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2334, decode.acc_seg: 99.5318, loss: 0.2341
2023-11-26 17:24:38,635 - mmseg - INFO - Iter [3350/5000]	lr: 4.320e-05, eta: 0:18:17, time: 0.625, data_time: 0.064, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2388, decode.acc_seg: 99.4970, loss: 0.2395
2023-11-26 17:25:09,985 - mmseg - INFO - Iter [3400/5000]	lr: 4.229e-05, eta: 0:17:43, time: 0.627, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2374, decode.acc_seg: 99.5081, loss: 0.2381
2023-11-26 17:25:41,312 - mmseg - INFO - Iter [3450/5000]	lr: 4.138e-05, eta: 0:17:09, time: 0.627, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0008, decode.loss_dice: 0.2337, decode.acc_seg: 99.4696, loss: 0.2345
2023-11-26 17:26:12,686 - mmseg - INFO - Iter [3500/5000]	lr: 4.047e-05, eta: 0:16:35, time: 0.627, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2372, decode.acc_seg: 99.4708, loss: 0.2379
2023-11-26 17:26:36,798 - mmseg - INFO - per class results:
2023-11-26 17:26:36,799 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.83 | 70.54 | 71.01 | 70.43 | 99.73 | 99.87  |   99.93   | 99.82  | 99.81 |
|  scratch   | 78.55 | 91.82 | 72.94 | 80.08 |  56.3 | 74.79  |   81.42   | 70.87  | 62.19 |
|   stain    |  87.8 | 82.57 | 71.85 | 74.63 | 80.85 | 88.48  |   89.83   | 87.23  | 82.72 |
| edgeDamage | 77.06 | 93.31 | 73.26 | 81.68 | 88.39 | 71.92  |   66.09   | 92.26  | 57.88 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 17:26:36,799 - mmseg - INFO - Summary:
2023-11-26 17:26:36,799 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.83 | 85.81 | 84.56 | 72.26 |  76.7 | 81.32 |  83.77  |   84.32    |  87.55  | 75.65 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-26 17:26:36,816 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9983, mIoU: 0.8581, mVOE: 0.8456, mASD: 0.7226, mMSSD: 0.7670, mAcc: 0.8132, mFscore: 0.8377, mPrecision: 0.8432, mRecall: 0.8755, mDice: 0.7565, IoU.background: 0.9983, IoU.scratch: 0.7855, IoU.stain: 0.8780, IoU.edgeDamage: 0.7706, VOE.background: 0.7054, VOE.scratch: 0.9182, VOE.stain: 0.8257, VOE.edgeDamage: 0.9331, ASD.background: 0.7101, ASD.scratch: 0.7294, ASD.stain: 0.7185, ASD.edgeDamage: 0.7326, MSSD.background: 0.7043, MSSD.scratch: 0.8008, MSSD.stain: 0.7463, MSSD.edgeDamage: 0.8168, Acc.background: 0.9973, Acc.scratch: 0.5630, Acc.stain: 0.8085, Acc.edgeDamage: 0.8839, Fscore.background: 0.9987, Fscore.scratch: 0.7479, Fscore.stain: 0.8848, Fscore.edgeDamage: 0.7192, Precision.background: 0.9993, Precision.scratch: 0.8142, Precision.stain: 0.8983, Precision.edgeDamage: 0.6609, Recall.background: 0.9982, Recall.scratch: 0.7087, Recall.stain: 0.8723, Recall.edgeDamage: 0.9226, Dice.background: 0.9981, Dice.scratch: 0.6219, Dice.stain: 0.8272, Dice.edgeDamage: 0.5788
2023-11-26 17:27:07,773 - mmseg - INFO - Iter [3550/5000]	lr: 3.956e-05, eta: 0:16:11, time: 1.102, data_time: 0.548, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2374, decode.acc_seg: 99.4844, loss: 0.2381
2023-11-26 17:27:39,045 - mmseg - INFO - Iter [3600/5000]	lr: 3.864e-05, eta: 0:15:36, time: 0.625, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2314, decode.acc_seg: 99.4646, loss: 0.2322
2023-11-26 17:28:07,498 - mmseg - INFO - Iter [3650/5000]	lr: 3.772e-05, eta: 0:15:01, time: 0.569, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2254, decode.acc_seg: 99.5283, loss: 0.2260
2023-11-26 17:28:38,989 - mmseg - INFO - Iter [3700/5000]	lr: 3.679e-05, eta: 0:14:27, time: 0.630, data_time: 0.067, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2300, decode.acc_seg: 99.5099, loss: 0.2306
2023-11-26 17:29:10,371 - mmseg - INFO - Iter [3750/5000]	lr: 3.586e-05, eta: 0:13:53, time: 0.628, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2293, decode.acc_seg: 99.5090, loss: 0.2300
2023-11-26 17:29:41,720 - mmseg - INFO - Iter [3800/5000]	lr: 3.493e-05, eta: 0:13:19, time: 0.627, data_time: 0.064, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2331, decode.acc_seg: 99.5259, loss: 0.2338
2023-11-26 17:30:13,109 - mmseg - INFO - Iter [3850/5000]	lr: 3.400e-05, eta: 0:12:45, time: 0.628, data_time: 0.064, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2239, decode.acc_seg: 99.5535, loss: 0.2246
2023-11-26 17:30:44,528 - mmseg - INFO - Iter [3900/5000]	lr: 3.306e-05, eta: 0:12:11, time: 0.628, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2222, decode.acc_seg: 99.5360, loss: 0.2229
2023-11-26 17:31:12,995 - mmseg - INFO - Iter [3950/5000]	lr: 3.211e-05, eta: 0:11:37, time: 0.569, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0007, decode.loss_dice: 0.2244, decode.acc_seg: 99.5160, loss: 0.2250
2023-11-26 17:31:44,322 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 17:31:44,323 - mmseg - INFO - Iter [4000/5000]	lr: 3.116e-05, eta: 0:11:03, time: 0.627, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2238, decode.acc_seg: 99.5642, loss: 0.2244
2023-11-26 17:32:07,575 - mmseg - INFO - per class results:
2023-11-26 17:32:07,576 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.86 | 70.51 | 71.01 | 70.42 | 99.79 |  99.9  |   99.93   | 99.86  | 99.85 |
|  scratch   | 79.02 | 91.35 | 72.89 | 79.81 | 57.53 | 75.63  |   82.13   | 71.69  | 63.45 |
|   stain    | 89.19 | 81.18 |  71.8 |  74.4 | 81.87 | 90.09  |   92.58   | 87.91  | 85.13 |
| edgeDamage | 79.05 | 91.32 | 73.05 | 80.65 | 91.29 | 75.69  |   69.17   |  94.2  | 63.54 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 17:32:07,577 - mmseg - INFO - Summary:
2023-11-26 17:32:07,577 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.86 | 86.78 | 83.59 | 72.19 | 76.32 | 82.62 |  85.33  |   85.95    |  88.41  | 77.99 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-26 17:32:07,593 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 17:32:07,594 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9986, mIoU: 0.8678, mVOE: 0.8359, mASD: 0.7219, mMSSD: 0.7632, mAcc: 0.8262, mFscore: 0.8533, mPrecision: 0.8595, mRecall: 0.8841, mDice: 0.7799, IoU.background: 0.9986, IoU.scratch: 0.7902, IoU.stain: 0.8919, IoU.edgeDamage: 0.7905, VOE.background: 0.7051, VOE.scratch: 0.9135, VOE.stain: 0.8118, VOE.edgeDamage: 0.9132, ASD.background: 0.7101, ASD.scratch: 0.7289, ASD.stain: 0.7180, ASD.edgeDamage: 0.7305, MSSD.background: 0.7042, MSSD.scratch: 0.7981, MSSD.stain: 0.7440, MSSD.edgeDamage: 0.8065, Acc.background: 0.9979, Acc.scratch: 0.5753, Acc.stain: 0.8187, Acc.edgeDamage: 0.9129, Fscore.background: 0.9990, Fscore.scratch: 0.7563, Fscore.stain: 0.9009, Fscore.edgeDamage: 0.7569, Precision.background: 0.9993, Precision.scratch: 0.8213, Precision.stain: 0.9258, Precision.edgeDamage: 0.6917, Recall.background: 0.9986, Recall.scratch: 0.7169, Recall.stain: 0.8791, Recall.edgeDamage: 0.9420, Dice.background: 0.9985, Dice.scratch: 0.6345, Dice.stain: 0.8513, Dice.edgeDamage: 0.6354
2023-11-26 17:32:38,442 - mmseg - INFO - Iter [4050/5000]	lr: 3.021e-05, eta: 0:10:35, time: 1.082, data_time: 0.530, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2212, decode.acc_seg: 99.5106, loss: 0.2218
2023-11-26 17:33:09,588 - mmseg - INFO - Iter [4100/5000]	lr: 2.925e-05, eta: 0:10:01, time: 0.623, data_time: 0.064, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2226, decode.acc_seg: 99.5210, loss: 0.2232
2023-11-26 17:33:40,840 - mmseg - INFO - Iter [4150/5000]	lr: 2.829e-05, eta: 0:09:27, time: 0.625, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2247, decode.acc_seg: 99.5262, loss: 0.2253
2023-11-26 17:34:12,116 - mmseg - INFO - Iter [4200/5000]	lr: 2.732e-05, eta: 0:08:53, time: 0.626, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2156, decode.acc_seg: 99.5181, loss: 0.2163
2023-11-26 17:34:43,313 - mmseg - INFO - Iter [4250/5000]	lr: 2.634e-05, eta: 0:08:20, time: 0.624, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2209, decode.acc_seg: 99.5020, loss: 0.2215
2023-11-26 17:35:11,554 - mmseg - INFO - Iter [4300/5000]	lr: 2.536e-05, eta: 0:07:45, time: 0.565, data_time: 0.006, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2204, decode.acc_seg: 99.5296, loss: 0.2210
2023-11-26 17:35:42,711 - mmseg - INFO - Iter [4350/5000]	lr: 2.437e-05, eta: 0:07:12, time: 0.623, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2186, decode.acc_seg: 99.5275, loss: 0.2192
2023-11-26 17:36:13,870 - mmseg - INFO - Iter [4400/5000]	lr: 2.337e-05, eta: 0:06:38, time: 0.623, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2253, decode.acc_seg: 99.5484, loss: 0.2259
2023-11-26 17:36:45,051 - mmseg - INFO - Iter [4450/5000]	lr: 2.237e-05, eta: 0:06:05, time: 0.624, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2178, decode.acc_seg: 99.5526, loss: 0.2184
2023-11-26 17:37:16,124 - mmseg - INFO - Iter [4500/5000]	lr: 2.135e-05, eta: 0:05:31, time: 0.621, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2182, decode.acc_seg: 99.5484, loss: 0.2188
2023-11-26 17:37:40,022 - mmseg - INFO - per class results:
2023-11-26 17:37:40,023 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.88 | 70.49 | 71.01 | 70.41 | 99.82 | 99.91  |   99.93   | 99.88  | 99.86 |
|  scratch   | 79.51 | 90.86 | 72.84 | 79.58 | 58.54 | 76.52  |   83.41   | 72.36  | 64.77 |
|   stain    | 89.88 | 80.49 | 71.76 | 74.18 | 82.84 | 90.85  |   93.47   | 88.56  | 86.27 |
| edgeDamage | 79.94 | 90.43 | 72.95 | 80.13 | 90.52 | 77.26  |   70.72   | 93.68  | 65.89 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 17:37:40,023 - mmseg - INFO - Summary:
2023-11-26 17:37:40,024 - mmseg - INFO - 
+-------+------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc | mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.88 | 87.3 | 83.07 | 72.14 | 76.08 | 82.93 |  86.13  |   86.88    |  88.62  |  79.2 |
+-------+------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-26 17:37:40,039 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9988, mIoU: 0.8730, mVOE: 0.8307, mASD: 0.7214, mMSSD: 0.7608, mAcc: 0.8293, mFscore: 0.8613, mPrecision: 0.8688, mRecall: 0.8862, mDice: 0.7920, IoU.background: 0.9988, IoU.scratch: 0.7951, IoU.stain: 0.8988, IoU.edgeDamage: 0.7994, VOE.background: 0.7049, VOE.scratch: 0.9086, VOE.stain: 0.8049, VOE.edgeDamage: 0.9043, ASD.background: 0.7101, ASD.scratch: 0.7284, ASD.stain: 0.7176, ASD.edgeDamage: 0.7295, MSSD.background: 0.7041, MSSD.scratch: 0.7958, MSSD.stain: 0.7418, MSSD.edgeDamage: 0.8013, Acc.background: 0.9982, Acc.scratch: 0.5854, Acc.stain: 0.8284, Acc.edgeDamage: 0.9052, Fscore.background: 0.9991, Fscore.scratch: 0.7652, Fscore.stain: 0.9085, Fscore.edgeDamage: 0.7726, Precision.background: 0.9993, Precision.scratch: 0.8341, Precision.stain: 0.9347, Precision.edgeDamage: 0.7072, Recall.background: 0.9988, Recall.scratch: 0.7236, Recall.stain: 0.8856, Recall.edgeDamage: 0.9368, Dice.background: 0.9986, Dice.scratch: 0.6477, Dice.stain: 0.8627, Dice.edgeDamage: 0.6589
2023-11-26 17:38:10,877 - mmseg - INFO - Iter [4550/5000]	lr: 2.033e-05, eta: 0:05:00, time: 1.095, data_time: 0.544, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2147, decode.acc_seg: 99.5544, loss: 0.2153
2023-11-26 17:38:39,076 - mmseg - INFO - Iter [4600/5000]	lr: 1.929e-05, eta: 0:04:26, time: 0.564, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2144, decode.acc_seg: 99.5557, loss: 0.2150
2023-11-26 17:39:10,504 - mmseg - INFO - Iter [4650/5000]	lr: 1.824e-05, eta: 0:03:53, time: 0.629, data_time: 0.069, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2133, decode.acc_seg: 99.5553, loss: 0.2139
2023-11-26 17:39:41,801 - mmseg - INFO - Iter [4700/5000]	lr: 1.718e-05, eta: 0:03:19, time: 0.626, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2172, decode.acc_seg: 99.5509, loss: 0.2178
2023-11-26 17:40:13,237 - mmseg - INFO - Iter [4750/5000]	lr: 1.609e-05, eta: 0:02:46, time: 0.629, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2128, decode.acc_seg: 99.5623, loss: 0.2134
2023-11-26 17:40:44,601 - mmseg - INFO - Iter [4800/5000]	lr: 1.499e-05, eta: 0:02:13, time: 0.627, data_time: 0.065, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2110, decode.acc_seg: 99.5484, loss: 0.2116
2023-11-26 17:41:16,130 - mmseg - INFO - Iter [4850/5000]	lr: 1.386e-05, eta: 0:01:39, time: 0.631, data_time: 0.066, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2082, decode.acc_seg: 99.5523, loss: 0.2088
2023-11-26 17:41:47,718 - mmseg - INFO - Iter [4900/5000]	lr: 1.269e-05, eta: 0:01:06, time: 0.632, data_time: 0.068, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2148, decode.acc_seg: 99.5660, loss: 0.2154
2023-11-26 17:42:16,261 - mmseg - INFO - Iter [4950/5000]	lr: 1.145e-05, eta: 0:00:33, time: 0.571, data_time: 0.007, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2086, decode.acc_seg: 99.5812, loss: 0.2092
2023-11-26 17:42:47,856 - mmseg - INFO - Saving checkpoint at 5000 iterations
2023-11-26 17:42:49,154 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 17:42:49,154 - mmseg - INFO - Iter [5000/5000]	lr: 1.004e-05, eta: 0:00:00, time: 0.658, data_time: 0.070, memory: 8488, decode.loss_focal: 0.0006, decode.loss_dice: 0.2148, decode.acc_seg: 99.5372, loss: 0.2155
2023-11-26 17:43:13,302 - mmseg - INFO - per class results:
2023-11-26 17:43:13,303 - mmseg - INFO - 
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
|   Class    |  IoU  |  VOE  |  ASD  |  MSSD |  Acc  | Fscore | Precision | Recall |  Dice |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
| background | 99.89 | 70.48 | 71.01 | 70.41 | 99.84 | 99.91  |   99.93   | 99.89  | 99.87 |
|  scratch   | 79.68 | 90.69 | 72.79 | 79.31 | 59.75 |  76.8  |   82.33   | 73.17  | 65.21 |
|   stain    | 90.11 | 80.26 | 71.71 | 73.93 | 83.99 |  91.1  |   93.07   | 89.33  | 86.65 |
| edgeDamage | 80.87 |  89.5 | 72.84 | 79.57 | 89.78 | 78.82  |   72.39   | 93.19  | 68.23 |
+------------+-------+-------+-------+-------+-------+--------+-----------+--------+-------+
2023-11-26 17:43:13,303 - mmseg - INFO - Summary:
2023-11-26 17:43:13,304 - mmseg - INFO - 
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
|  aAcc |  mIoU |  mVOE |  mASD | mMSSD |  mAcc | mFscore | mPrecision | mRecall | mDice |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
| 99.89 | 87.64 | 82.73 | 72.09 | 75.81 | 83.34 |  86.66  |   86.93    |  88.89  | 79.99 |
+-------+-------+-------+-------+-------+-------+---------+------------+---------+-------+
2023-11-26 17:43:13,319 - mmseg - INFO - Exp name: levitunet.py
2023-11-26 17:43:13,320 - mmseg - INFO - Iter(val) [105]	aAcc: 0.9989, mIoU: 0.8764, mVOE: 0.8273, mASD: 0.7209, mMSSD: 0.7581, mAcc: 0.8334, mFscore: 0.8666, mPrecision: 0.8693, mRecall: 0.8889, mDice: 0.7999, IoU.background: 0.9989, IoU.scratch: 0.7968, IoU.stain: 0.9011, IoU.edgeDamage: 0.8087, VOE.background: 0.7048, VOE.scratch: 0.9069, VOE.stain: 0.8026, VOE.edgeDamage: 0.8950, ASD.background: 0.7101, ASD.scratch: 0.7279, ASD.stain: 0.7171, ASD.edgeDamage: 0.7284, MSSD.background: 0.7041, MSSD.scratch: 0.7931, MSSD.stain: 0.7393, MSSD.edgeDamage: 0.7957, Acc.background: 0.9984, Acc.scratch: 0.5975, Acc.stain: 0.8399, Acc.edgeDamage: 0.8978, Fscore.background: 0.9991, Fscore.scratch: 0.7680, Fscore.stain: 0.9110, Fscore.edgeDamage: 0.7882, Precision.background: 0.9993, Precision.scratch: 0.8233, Precision.stain: 0.9307, Precision.edgeDamage: 0.7239, Recall.background: 0.9989, Recall.scratch: 0.7317, Recall.stain: 0.8933, Recall.edgeDamage: 0.9319, Dice.background: 0.9987, Dice.scratch: 0.6521, Dice.stain: 0.8665, Dice.edgeDamage: 0.6823
